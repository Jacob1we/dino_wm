# ğŸ§  DINO World Model - VollstÃ¤ndige Training-Dokumentation

> Eine detaillierte, chronologische Dokumentation des gesamten Trainingsprozesses fÃ¼r das DINO World Model mit dem Franka Cube Stacking Datensatz.

---

## ğŸ“‘ Inhaltsverzeichnis

1. [Ãœberblick und Konzept](#1-Ã¼berblick-und-konzept)
2. [Datensatz-Struktur](#2-datensatz-struktur)
3. [Konfiguration und Parameter](#3-konfiguration-und-parameter)
4. [Training-Pipeline (Chronologisch)](#4-training-pipeline-chronologisch)
5. [Modell-Architektur](#5-modell-architektur)
6. [Loss-Funktionen](#6-loss-funktionen)
7. [Training starten](#7-training-starten)
8. [Glossar](#8-glossar)
9. [Troubleshooting](#9-troubleshooting)

---

## 1. Ãœberblick und Konzept

### Was ist das DINO World Model?

Das **DINO World Model** ist ein visuelles Weltmodell, das lernt, zukÃ¼nftige visuelle ZustÃ¤nde eines Roboters vorherzusagen. Es kombiniert:

- **DINO v2 Encoder**: Vortrainiertes Vision-Modell von Meta zur BildreprÃ¤sentation
- **ViT Predictor**: Vision Transformer zur Vorhersage im Latent-Space
- **VQ-VAE Decoder**: Rekonstruktion von Bildern aus dem Latent-Space

### Konzept der Vorhersage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WORLD MODEL KONZEPT                                   â”‚
â”‚                                                                             â”‚
â”‚    Gegeben:  [Bild_t-2, Bild_t-1, Bild_t] + [Aktionen]                     â”‚
â”‚    Ziel:     Vorhersage von Bild_t+1 im Latent-Space                       â”‚
â”‚                                                                             â”‚
â”‚    Das Modell lernt die DYNAMIK der Welt:                                  â”‚
â”‚    "Wenn ich diese Bilder sehe und diese Aktion ausfÃ¼hre,                  â”‚
â”‚     wie wird die Welt danach aussehen?"                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Datensatz-Struktur

### 2.1 Dein Datensatz: `2026_01_13_1152_fcs_dset`

```
2026_01_13_1152_fcs_dset/
â”œâ”€â”€ states.pth          # Roboter-ZustÃ¤nde: (10, 932, 22)
â”œâ”€â”€ actions.pth         # Aktionen: (10, 932, 9)
â”œâ”€â”€ metadata.pkl        # Metadaten
â”œâ”€â”€ seq_lengths.pkl     # SequenzlÃ¤ngen
â”œâ”€â”€ cameras/            # Kamera-Konfiguration
â””â”€â”€ 000000/ ... 000009/ # 10 Episoden
    â”œâ”€â”€ obses.pth       # RGB-Bilder: (932, 256, 256, 3)
    â”œâ”€â”€ images/         # PNG-Bilder (optional)
    â””â”€â”€ property_params.pkl
```

### 2.2 Datensatz-Dimensionen

| Komponente | Form | Beschreibung |
|------------|------|--------------|
| **States** | `(10, 932, 22)` | 10 Episoden, 932 Timesteps, 22 State-Dimensionen |
| **Actions** | `(10, 932, 9)` | 10 Episoden, 932 Timesteps, 9 Action-Dimensionen |
| **Images** | `(932, 256, 256, 3)` | 932 RGB-Bilder pro Episode |

### 2.3 State-Vektor Aufbau (22 Dimensionen)

State = [ee_pos(3), ee_quat(4), gripper(1), joints(7), joint_vel(7)]
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€ Proprio: Nur EE-Position (erste 3 Dimensionen) wird als
             "Proprioceptive Input" fÃ¼r das Modell verwendet

### 2.4 Action-Vektor Aufbau (9 Dimensionen)
Der Action-Vektor enthÃ¤lt die Roboter-Kommandos und setzt sich wie folgt zusammen:
Action = [joint_cmd(7), gripper_cmd(2)]
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â””â”€â”€ Gripper-Fingerposition (links, rechts)
          â””â”€â”€ 7 Joint-Positionen (Soll-Werte fÃ¼r Gelenke 0-6)

Index	Dimension	Beschreibung	Typischer Wertebereich
0-6	joint_cmd[0:7]	Joint-Positionen (Radiant)	ca. -3.0 bis +3.0
7-8	gripper_cmd[0:2]	Gripper-Finger (links/rechts)	0.0 (geschlossen) bis 0.04 (offen)

Beispiel-Action aus deinem Datensatz:
[-0.095, -0.521, 0.047, -2.841, 0.031, 2.886, 0.842, 0.0, 0.0]
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜
  â”‚                                                         Gripper
  â””â”€â”€ 7 Joint-Sollpositionen                               (geschlossen)

Hinweis: Bei frameskip > 1 werden mehrere aufeinanderfolgende Actions konkateniert:
Mit frameskip=5: Effektive Action-Dimension = 9 Ã— 5 = 45
Format: [action_t, action_t+1, action_t+2, action_t+3, action_t+4]
---

## 3. Konfiguration und Parameter

### 3.1 Haupt-Konfigurationsdatei: `conf/train.yaml`

```yaml
# KRITISCHE PARAMETER
frameskip: 5       # Temporales Subsampling
num_hist: 3        # Anzahl Kontext-Frames
num_pred: 1        # Anzahl Vorhersage-Frames (nur 1 unterstÃ¼tzt)
img_size: 224      # BildgrÃ¶ÃŸe fÃ¼r Encoder

### Temporales Subsampling (frameskip)
Temporales Subsampling bedeutet, dass nur jeder n-te Frame aus der Originalsequenz verwendet wird, anstatt alle Frames.

# TRAINING
training:
  epochs: 100
  batch_size: 12
  seed: 0
  save_every_x_epoch: 1
  encoder_lr: 1e-6      # DINO Encoder (meist eingefroren)
  decoder_lr: 3e-4      # VQ-VAE Decoder
  predictor_lr: 5e-4    # ViT Predictor
  action_encoder_lr: 5e-4

# EMBEDDING DIMENSIONEN
action_emb_dim: 10      # Action Embedding Dimension
proprio_emb_dim: 10     # Proprio Embedding Dimension
concat_dim: 1           # Wie Embeddings kombiniert werden (0 oder 1)

# MODELL-KOMPONENTEN
model:
  train_encoder: False   # DINO wird NICHT trainiert (vortrainiert)
  train_predictor: True  # Predictor wird trainiert
  train_decoder: True    # Decoder wird trainiert
```

### 3.2 Parameter-ErklÃ¤rung: `frameskip`

**Frameskip** definiert das temporale Subsampling der Daten:

Temporales Subsampling bedeutet, dass nur jeder n-te Frame aus der Originalsequenz verwendet wird, anstatt alle Frames.

Original-Aufnahme (30 FPS, 932 Frames):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚ 9 â”‚10 â”‚11 â”‚12 â”‚13 â”‚14 â”‚15 â”‚16 â”‚17 â”‚18 â”‚19 â”‚...
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Mit frameskip=5 (jeder 5. Frame):
â”Œâ”€â”€â”€â”           â”Œâ”€â”€â”€â”           â”Œâ”€â”€â”€â”           â”Œâ”€â”€â”€â”
â”‚ 0 â”‚           â”‚ 5 â”‚           â”‚10 â”‚           â”‚15 â”‚  ...
â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜
  â†“               â†“               â†“               â†“
Frame 0 â”€â”€â”€â”€â”€â”€â–º Frame 1 â”€â”€â”€â”€â”€â”€â–º Frame 2 â”€â”€â”€â”€â”€â”€â–º Frame 3  (fÃ¼r das Modell verwendet)

# Warum Subsampling?
Vorteil	                ErklÃ¤rung
GrÃ¶ÃŸere Bewegung	      Zwischen Frame 0 und Frame 5 passiert mehr als zwischen Frame 0 und Frame 1 â†’ leichter zu lernen
Weniger Redundanz	      Aufeinanderfolgende Frames sind oft fast identisch
Effektivere Aktionen	  5 Aktionen werden zu einer kombiniert â†’ reichhaltigere Information
LÃ¤ngere Zeitspannen	    Mit gleicher Anzahl Frames kann mehr Zeit abgedeckt werden

**Auswirkungen:**
- **GrÃ¶ÃŸere visuelle Differenzen** zwischen Frames â†’ einfacher zu lernen
- **Mehr Bewegung pro Schritt** â†’ Modell muss grÃ¶ÃŸere Dynamik erfassen
- **Aktionen werden konkateniert**: 5 Aktionen â†’ 1 kombinierte Aktion
  - `action_dim_effektiv = action_dim Ã— frameskip = 9 Ã— 5 = 45`

### 3.3 Parameter-ErklÃ¤rung: `num_hist`
Kontext-Frames (num_hist)
Kontext-Frames sind die Anzahl der vergangenen Bilder, die dem Modell als Input gegeben werden, um die Zukunft vorherzusagen.

Beispiel mit num_hist=3, num_pred=1, frameskip=5:
Input:      [Frame0, Frame5, Frame10] + [Actions 0-14]
                    â†“ Predictor
Output:     Vorhersage fÃ¼r Frame15

Zeitlinie:      t=0     t=5     t=10    t=15
                 â”‚       â”‚        â”‚       â”‚
                 â–¼       â–¼        â–¼       â–¼
              â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
              â”‚Bild â”‚ â”‚Bild â”‚ â”‚Bild â”‚ â”‚Bild â”‚
              â”‚  0  â”‚ â”‚  1  â”‚ â”‚  2  â”‚ â”‚  3  â”‚
              â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
                 â”‚       â”‚        â”‚       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                         â”‚                â”‚
                    KONTEXT (3)      VORHERSAGE (1)
                    (num_hist)        (num_pred)
                         â”‚                â”‚
                         â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ INPUT fÃ¼r Modell â”‚  â”‚ ZIEL/TARGET  â”‚
              â”‚ [Bild0,Bild1,    â”‚  â”‚   [Bild3]    â”‚
              â”‚  Bild2,Actions]  â”‚  â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Warum mehrere Kontext-Frames?
Grund	              ErklÃ¤rung
Geschwindigkeit	    Aus 2+ Frames kann Bewegungsrichtung inferiert werden
Beschleunigung	    Aus 3+ Frames kann Beschleunigung erkannt werden
Verdeckungen	      Objekte, die in einem Frame verdeckt sind, kÃ¶nnen in anderen sichtbar sein
AmbiguitÃ¤t	        Ein einzelnes Bild kann mehrdeutig sein (steht still? bewegt sich?)

**Warum wichtig:**
- Mehr Historie = besseres VerstÃ¤ndnis der Dynamik
- Geschwindigkeit/Beschleunigung kÃ¶nnen inferiert werden
- Trade-off: Mehr Speicher, aber bessere Vorhersagen


### Zusammenspiel beider Parameter frameskip und num_hist
Gesamte SequenzlÃ¤nge pro Sample = (num_hist + num_pred) Ã— frameskip
                                = (3 + 1) Ã— 5 = 20 Original-Frames

Aus deinen 932 Frames pro Episode:
â”œâ”€â”€ Sample 1: Frames [0, 5, 10, 15]  â†’ Input: [0,5,10], Target: [15]
â”œâ”€â”€ Sample 2: Frames [1, 6, 11, 16]  â†’ Input: [1,6,11], Target: [16]
â”œâ”€â”€ Sample 3: Frames [2, 7, 12, 17]  â†’ Input: [2,7,12], Target: [17]
...
â””â”€â”€ Sample 913: Frames [912, 917, 922, 927]

= 913 Trainingssamples pro Episode

### 3.4 Action & Proprio Embedding Prozess

Die `action_emb_dim: 10` und `proprio_emb_dim: 10` entsprechen **nicht** den Rohdimensionen deiner Daten (Action: 9, Proprio: 3). Stattdessen werden die Rohdaten durch einen **lernbaren Encoder** in diese Embedding-Dimensionen transformiert.

#### Schritt 1: Frameskip-Konkatenation (nur fÃ¼r Actions)

Bevor die Aktionen eingebettet werden, werden sie durch den `frameskip` konkateniert:

```
Deine Original-Aktionen:     9 Dimensionen pro Frame

Mit frameskip=5:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Action t â”‚Action t+1â”‚Action t+2â”‚Action t+3â”‚Action t+4â”‚
â”‚  (9)    â”‚   (9)   â”‚   (9)   â”‚   (9)   â”‚   (9)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼ Konkatenation
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Kombinierte Action    â”‚
              â”‚      (9 Ã— 5 = 45)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Schritt 2: Embedding durch Conv1d

Der `ProprioceptiveEmbedding`-Encoder transformiert die Rohdaten in kompakte Embeddings:

```
ACTION ENCODER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  (Batch, Time, 45)   â† 45 = action_dim Ã— frameskip = 9 Ã— 5
              â”‚
              â–¼
        Conv1d(45 â†’ 10)     â† Lernbare Projektion
              â”‚
              â–¼
Output: (Batch, Time, 10)   â† action_emb_dim


PROPRIO ENCODER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  (Batch, Time, 3)    â† EE-Position (x, y, z)
              â”‚
              â–¼
        Conv1d(3 â†’ 10)      â† Lernbare Projektion
              â”‚
              â–¼
Output: (Batch, Time, 10)   â† proprio_emb_dim
```

#### Warum diese Transformation?

| Aspekt | ErklÃ¤rung |
|--------|-----------|
| **Dimensionsreduktion** | 45 â†’ 10 komprimiert die Action-Information |
| **Lernbare Features** | Netzwerk lernt, welche Action-Kombinationen wichtig sind |
| **KompatibilitÃ¤t** | Kleinere Embedding-Dimension passt besser zu DINO (384 dim) |
| **Regularisierung** | Verhindert Overfitting auf hochdimensionale Inputs |

#### Finale Embedding-Zusammensetzung (concat_dim=1)

Pro Patch im Latent-Space werden alle Embeddings konkateniert:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DINO Visual (384) â”‚ Proprio Emb (10) â”‚ Action Emb (10) â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚       384         â”‚        10        â”‚       10        â”‚= 404â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Code-Referenz

```python
# Aus models/proprio.py - ProprioceptiveEmbedding:
self.patch_embed = nn.Conv1d(
    in_chans,      # 45 fÃ¼r Actions (9Ã—5), 3 fÃ¼r Proprio
    emb_dim,       # 10 (action_emb_dim / proprio_emb_dim)
    kernel_size=1,
    stride=1
)
```

**Zusammenfassung des Datenflusses:**
```
Actions:  (B, T, 9) â”€â”€frameskipâ”€â”€â–º (B, T, 45) â”€â”€Conv1dâ”€â”€â–º (B, T, 10)
Proprio:  (B, T, 3) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Conv1dâ”€â”€â–º (B, T, 10)
```

### 3.5 Umgebungs-Konfiguration: `conf/env/franka_cube_stack.yaml`

```yaml
name: franka_cube_stack
dataset:
  _target_: "datasets.franka_cube_stack_dset.load_franka_cube_stack_slice_train_val"
  data_path: /pfad/zu/deinem/datensatz
  n_rollout: null        # null = alle Rollouts laden
  normalize_action: true # Aktionen werden z-normalisiert
  split_ratio: 0.9       # 90% Train, 10% Validation
  transform:
    _target_: "datasets.img_transforms.default_transform"
    img_size: 224        # Resize auf 224x224

num_workers: 4           # Dataloader Workers
decoder_path: null       # Optional: Vortrainierter Decoder
```

---

## 4. Training-Pipeline (Chronologisch)

### Phase 1: Initialisierung

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 1: Konfiguration laden                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Alle Hyperparameter und Pfade aus YAML-Dateien einlesen.            â”‚
â”‚  Hydra ermÃ¶glicht hierarchische Konfiguration und Command-Line-Overrides.   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  python train.py env=franka_cube_stack                                      â”‚
â”‚                                                                             â”‚
â”‚  â†’ Hydra lÃ¤dt: conf/train.yaml + conf/env/franka_cube_stack.yaml           â”‚
â”‚  â†’ Parameter werden zusammengefÃ¼hrt                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 2: Trainer-Objekt erstellen                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Zentrale Klasse, die Training, Validation und Logging koordiniert.  â”‚
â”‚  Accelerator abstrahiert GPU/Multi-GPU und integriert Weights & Biases.     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  class Trainer:                                                             â”‚
â”‚      def __init__(self, cfg):                                               â”‚
â”‚          self.cfg = cfg                                                     â”‚
â”‚          self.accelerator = Accelerator(log_with="wandb")                   â”‚
â”‚          self.device = self.accelerator.device  # GPU                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Datensatz laden und vorbereiten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 3: FrankaCubeStackDataset laden                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Rohdaten (Bilder, States, Actions) von der Festplatte laden.        â”‚
â”‚  Bilder werden in RAM gecacht fÃ¼r schnellen Zugriff wÃ¤hrend des Trainings.  â”‚
â”‚  Z-Normalisierung stabilisiert das Training durch einheitliche Wertebereicheâ”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  class FrankaCubeStackDataset:                                              â”‚
â”‚      def __init__(self, data_path, ...):                                    â”‚
â”‚          # 1. States und Actions laden                                      â”‚
â”‚          self.states = torch.load("states.pth")    # (10, 932, 22)         â”‚
â”‚          self.actions = torch.load("actions.pth")  # (10, 932, 9)          â”‚
â”‚                                                                             â”‚
â”‚          # 2. Episoden-LÃ¤ngen aus metadata.pkl                              â”‚
â”‚          self.seq_lengths = [932, 932, ..., 932]   # 10 Ã— 932              â”‚
â”‚                                                                             â”‚
â”‚          # 3. Proprio extrahieren (EE-Position)                             â”‚
â”‚          self.proprios = self.states[..., :3]      # (10, 932, 3)          â”‚
â”‚                                                                             â”‚
â”‚          # 4. Z-Normalisierung (wenn normalize_action=True)                 â”‚
â”‚          self.actions = (self.actions - mean) / std                         â”‚
â”‚          self.proprios = (self.proprios - mean) / std                       â”‚
â”‚                                                                             â”‚
â”‚          # 5. Alle Bilder in RAM laden (preload_images=True)                â”‚
â”‚          self.images_cache = [                                              â”‚
â”‚              torch.load("000000/obses.pth"),  # (932, 256, 256, 3)         â”‚
â”‚              torch.load("000001/obses.pth"),                                â”‚
â”‚              ...                                                            â”‚
â”‚          ]                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 4: Train/Validation Split                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Episoden in Training und Validation aufteilen zur Overfitting-Kontrolle.â”‚
â”‚  Validation-Daten werden NIE zum Training verwendet, nur zur Evaluation.    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Mit split_ratio=0.9 und 10 Episoden:                                       â”‚
â”‚  - Training: 9 Episoden (zufÃ¤llig ausgewÃ¤hlt)                               â”‚
â”‚  - Validation: 1 Episode                                                    â”‚
â”‚                                                                             â”‚
â”‚  split_traj_datasets(dataset, train_fraction=0.9)                           â”‚
â”‚  â†’ train_set = TrajSubset(dataset, [0,1,2,3,4,5,6,7,8])  # Beispiel        â”‚
â”‚  â†’ val_set = TrajSubset(dataset, [9])                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 5: TrajSlicerDataset erstellen                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Lange Episoden in kurze, Ã¼berlappende Trainings-Samples schneiden.  â”‚
â”‚  Frameskip und num_hist bestimmen die LÃ¤nge und AuflÃ¶sung jedes Samples.    â”‚
â”‚  Shuffling der Slices verhindert, dass das Modell Sequenz-Reihenfolge lernt.â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Parameter:                                                                 â”‚
â”‚  - num_frames = num_hist + num_pred = 3 + 1 = 4                            â”‚
â”‚  - frameskip = 5                                                            â”‚
â”‚                                                                             â”‚
â”‚  FÃ¼r jede Episode (T=932 Frames):                                           â”‚
â”‚  - BenÃ¶tigte Frames pro Sample: 4 Ã— 5 = 20                                 â”‚
â”‚  - MÃ¶gliche Start-Positionen: 932 - 20 + 1 = 913                           â”‚
â”‚                                                                             â”‚
â”‚  Slices pro Episode:                                                        â”‚
â”‚  [                                                                          â”‚
â”‚    (episode_idx, 0, 20),    # Frames 0,5,10,15                              â”‚
â”‚    (episode_idx, 1, 21),    # Frames 1,6,11,16                              â”‚
â”‚    (episode_idx, 2, 22),    # Frames 2,7,12,17                              â”‚
â”‚    ...                                                                      â”‚
â”‚    (episode_idx, 912, 932), # Frames 912,917,922,927                        â”‚
â”‚  ]                                                                          â”‚
â”‚                                                                             â”‚
â”‚  GESAMT: 9 Episoden Ã— 913 Slices = ~8.217 Training-Samples                 â”‚
â”‚          1 Episode Ã— 913 Slices = ~913 Validation-Samples                   â”‚
â”‚                                                                             â”‚
â”‚  â†’ Slices werden zufÃ¤llig gemischt (shuffle)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 3: Modelle initialisieren

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 6: DINO v2 Encoder laden                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Vortrainiertes Vision-Modell extrahiert semantische Bild-Features.  â”‚
â”‚  DINO wurde auf Millionen Bildern trainiert und ist hier EINGEFROREN.       â”‚
â”‚  Output: 256 Patch-Tokens Ã  384 Dimensionen pro Bild.                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  self.encoder = DinoV2Encoder(                                              â”‚
â”‚      name="dinov2_vits14",          # ViT-Small, Patch 14                   â”‚
â”‚      feature_key="x_norm_patchtokens"                                       â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  # LÃ¤dt vortrainiertes Modell von Facebook                                  â”‚
â”‚  torch.hub.load("facebookresearch/dinov2", "dinov2_vits14")                 â”‚
â”‚                                                                             â”‚
â”‚  Eigenschaften:                                                             â”‚
â”‚  - emb_dim: 384 (Feature-Dimension)                                         â”‚
â”‚  - patch_size: 14 (jeder 14Ã—14 Pixel-Block = 1 Token)                      â”‚
â”‚  - latent_ndim: 2 (Patches sind 2D angeordnet)                              â”‚
â”‚                                                                             â”‚
â”‚  FÃ¼r 224Ã—224 Bilder:                                                        â”‚
â”‚  - num_patches = (224/14)Â² = 16Â² = 256 Patches                              â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: train_encoder=False â†’ Parameter sind eingefroren!                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 7: Action & Proprio Encoder laden                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Roboter-Aktionen und Propriozeption in kompakte Embeddings wandeln. â”‚
â”‚  Conv1d lernt, welche Action-Kombinationen fÃ¼r Vorhersagen relevant sind.   â”‚
â”‚  Diese Encoder werden MIT trainiert (im Gegensatz zu DINO).                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  # Action Encoder                                                           â”‚
â”‚  self.action_encoder = ProprioceptiveEmbedding(                             â”‚
â”‚      in_chans=45,      # action_dim Ã— frameskip = 9 Ã— 5                    â”‚
â”‚      emb_dim=10        # action_emb_dim aus config                          â”‚
â”‚  )                                                                          â”‚
â”‚  # Verwendet 1D Convolution: Conv1d(45 â†’ 10)                                â”‚
â”‚                                                                             â”‚
â”‚  # Proprio Encoder                                                          â”‚
â”‚  self.proprio_encoder = ProprioceptiveEmbedding(                            â”‚
â”‚      in_chans=3,       # proprio_dim (EE-Position x,y,z)                    â”‚
â”‚      emb_dim=10        # proprio_emb_dim aus config                         â”‚
â”‚  )                                                                          â”‚
â”‚  # Verwendet 1D Convolution: Conv1d(3 â†’ 10)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 8: ViT Predictor laden                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Kernkomponente - lernt zukÃ¼nftige ZustÃ¤nde im Latent-Space vorherzusagen.â”‚
â”‚  Kausale Maske verhindert, dass das Modell in die Zukunft "schaut".         â”‚
â”‚  6 Transformer-BlÃ¶cke mit 16 Attention-Heads fÃ¼r komplexe Dynamik-Modellierung.â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  self.predictor = ViTPredictor(                                             â”‚
â”‚      num_patches=198,  # 196 visual + 2 (proprio + action bei concat_dim=0) â”‚
â”‚                        # oder 196 bei concat_dim=1                          â”‚
â”‚      num_frames=3,     # num_hist                                           â”‚
â”‚      dim=404,          # 384 (DINO) + 10 (action) + 10 (proprio)           â”‚
â”‚      depth=6,          # 6 Transformer-BlÃ¶cke                               â”‚
â”‚      heads=16,         # 16 Attention-Heads                                 â”‚
â”‚      mlp_dim=2048,     # Feed-Forward Dimension                             â”‚
â”‚      dropout=0.1                                                            â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  # Verwendet KAUSALE ATTENTION MASK                                         â”‚
â”‚  # â†’ Kann nur vergangene Frames sehen, nicht zukÃ¼nftige                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 9: VQ-VAE Decoder laden                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Rekonstruiert Bilder aus dem Latent-Space zur Visualisierung.       â”‚
â”‚  Upsampling von 16Ã—16 auf 224Ã—224 durch transponierte Convolutions.         â”‚
â”‚  Quantisierung ist deaktiviert fÃ¼r kontinuierlichen, glatten Latent-Space.  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  self.decoder = VQVAE(                                                      â”‚
â”‚      channel=384,       # Entspricht DINO emb_dim                           â”‚
â”‚      n_embed=2048,      # Codebook-GrÃ¶ÃŸe (nicht verwendet wenn quantize=F)  â”‚
â”‚      n_res_block=4,     # Residual Blocks                                   â”‚
â”‚      n_res_channel=128,                                                     â”‚
â”‚      quantize=False     # KEINE Quantisierung (kontinuierlicher Latent)    â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  # Architektur:                                                             â”‚
â”‚  # Latent (14Ã—14Ã—384) â†’ Upsample (4Ã—) â†’ 56Ã—56 â†’ Upsample (4Ã—) â†’ 224Ã—224Ã—3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 10: VWorldModel zusammensetzen                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Alle Komponenten zu einem einheitlichen World Model verbinden.      â”‚
â”‚  Definiert den Datenfluss: Encode â†’ Concatenate â†’ Predict â†’ Decode.         â”‚
â”‚  concat_dim=1 bedeutet, dass Embeddings entlang der Feature-Dimension kombiniert werden.â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  self.model = VWorldModel(                                                  â”‚
â”‚      encoder=self.encoder,                                                  â”‚
â”‚      proprio_encoder=self.proprio_encoder,                                  â”‚
â”‚      action_encoder=self.action_encoder,                                    â”‚
â”‚      predictor=self.predictor,                                              â”‚
â”‚      decoder=self.decoder,                                                  â”‚
â”‚      num_hist=3,                                                            â”‚
â”‚      num_pred=1,                                                            â”‚
â”‚      concat_dim=1  # Embeddings werden entlang Feature-Dimension konkateniertâ”‚
â”‚  )                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 4: Training-Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 11: Optimizer initialisieren                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Separate Optimizer fÃ¼r jede Komponente mit unterschiedlichen Lernraten.â”‚
â”‚  AdamW fÃ¼r Predictor (mit Weight Decay), Adam fÃ¼r Encoder/Decoder.          â”‚
â”‚  Niedrige Encoder-LR (1e-6) da DINO-Weights meist eingefroren bleiben.      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  self.encoder_optimizer = Adam(encoder.parameters(), lr=1e-6)               â”‚
â”‚  self.predictor_optimizer = AdamW(predictor.parameters(), lr=5e-4)          â”‚
â”‚  self.decoder_optimizer = Adam(decoder.parameters(), lr=3e-4)               â”‚
â”‚  self.action_encoder_optimizer = AdamW(                                     â”‚
â”‚      [action_encoder.params, proprio_encoder.params], lr=5e-4               â”‚
â”‚  )                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 12: Training-Epoch                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Hauptschleife - iteriert Ã¼ber alle Batches und aktualisiert Gewichte.â”‚
â”‚  Forward Pass berechnet Vorhersagen, Backward Pass berechnet Gradienten.    â”‚
â”‚  Nur trainierbare Komponenten (Predictor, Decoder, Action-Encoder) werden aktualisiert.â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  for epoch in range(1, 101):  # 100 Epochen                                â”‚
â”‚      for batch in dataloader:                                               â”‚
â”‚          obs, act, state = batch                                            â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â”‚  obs['visual']: (B, 4, 3, 224, 224) - 4 Bilder                  â”‚
â”‚          â”‚  obs['proprio']: (B, 4, 3) - 4 EE-Positionen                    â”‚
â”‚          â”‚  act: (B, 4, 45) - 4 Ã— (9Ã—5) konkatenierte Aktionen             â”‚
â”‚          â”‚  state: (B, 4, 22) - 4 vollstÃ¤ndige States                      â”‚
â”‚          â–¼                                                                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚      â”‚  FORWARD PASS (siehe nÃ¤chstes Diagramm)                         â”‚   â”‚
â”‚      â”‚  z_pred, visual_pred, visual_recon, loss = model(obs, act)      â”‚   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â–¼                                                                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚      â”‚  BACKWARD PASS                                                   â”‚   â”‚
â”‚      â”‚  1. encoder_optimizer.zero_grad()                               â”‚   â”‚
â”‚      â”‚  2. predictor_optimizer.zero_grad()                             â”‚   â”‚
â”‚      â”‚  3. decoder_optimizer.zero_grad()                               â”‚   â”‚
â”‚      â”‚  4. action_encoder_optimizer.zero_grad()                        â”‚   â”‚
â”‚      â”‚                                                                  â”‚   â”‚
â”‚      â”‚  accelerator.backward(loss)  # Gradient berechnen               â”‚   â”‚
â”‚      â”‚                                                                  â”‚   â”‚
â”‚      â”‚  # NUR trainierbare Komponenten updaten:                        â”‚   â”‚
â”‚      â”‚  predictor_optimizer.step()      # âœ“ train_predictor=True       â”‚   â”‚
â”‚      â”‚  decoder_optimizer.step()         # âœ“ train_decoder=True        â”‚   â”‚
â”‚      â”‚  action_encoder_optimizer.step()  # âœ“ immer                     â”‚   â”‚
â”‚      â”‚  # encoder_optimizer.step()      # âœ— train_encoder=False       â”‚   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 5: Forward Pass im Detail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 13: Forward Pass - Encoding                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Rohdaten (Bilder, Proprio, Actions) in einheitliche Embeddings wandeln.â”‚
â”‚  DINO transformiert Bilder in 256 semantische Patch-Tokens.                 â”‚
â”‚  Action/Proprio-Encoder komprimieren Sensordaten in 10-dimensionale Vektoren.â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  INPUT:                                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                                    â”‚
â”‚  obs['visual']: (B, 4, 3, 224, 224)   # 4 RGB Bilder                       â”‚
â”‚  obs['proprio']: (B, 4, 3)            # 4 EE-Positionen (x,y,z)            â”‚
â”‚  act: (B, 4, 45)                      # 4 konkatenierte Aktionen           â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  VISUAL ENCODING (DINO v2):                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  1. Bilder reshapen: (B, 4, 3, 224, 224) â†’ (BÃ—4, 3, 224, 224)              â”‚
â”‚  2. Optional resize auf encoder_image_size (fÃ¼r DINO patch alignment)       â”‚
â”‚  3. DINO forward: (BÃ—4, 3, 224, 224) â†’ (BÃ—4, 256, 384)                     â”‚
â”‚                   [batchÃ—time, num_patches, emb_dim]                        â”‚
â”‚  4. Reshape zurÃ¼ck: (BÃ—4, 256, 384) â†’ (B, 4, 256, 384)                     â”‚
â”‚                                                                             â”‚
â”‚  z_visual: (B, 4, 256, 384)                                                 â”‚
â”‚            â†‘    â†‘    â†‘                                                      â”‚
â”‚         batch time patches                                                  â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  PROPRIO ENCODING:                                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  proprio_encoder(obs['proprio'])                                            â”‚
â”‚  (B, 4, 3) â†’ Conv1d â†’ (B, 4, 10)                                           â”‚
â”‚                                                                             â”‚
â”‚  z_proprio: (B, 4, 10)                                                      â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  ACTION ENCODING:                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  action_encoder(act)                                                        â”‚
â”‚  (B, 4, 45) â†’ Conv1d â†’ (B, 4, 10)                                          â”‚
â”‚                                                                             â”‚
â”‚  z_action: (B, 4, 10)                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 14: Forward Pass - Concatenation (concat_dim=1)                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Visuelle, propriozeptive und Action-Information zu einem Vektor vereinen.â”‚
â”‚  Tiling repliziert Proprio/Action auf alle 256 Patches fÃ¼r einheitliche Dim.â”‚
â”‚  Ergebnis: Jeder Patch enthÃ¤lt visuelle + Roboter-Information (404 dim).    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Mit concat_dim=1: Embeddings werden entlang Feature-Dimension konkateniert â”‚
â”‚                                                                             â”‚
â”‚  z_visual:  (B, 4, 256, 384)  # 256 Patches Ã— 384 dim                      â”‚
â”‚  z_proprio: (B, 4, 10)        # â†’ tile auf (B, 4, 256, 10)                 â”‚
â”‚  z_action:  (B, 4, 10)        # â†’ tile auf (B, 4, 256, 10)                 â”‚
â”‚                                                                             â”‚
â”‚  Konkatenation:                                                             â”‚
â”‚  z = concat([z_visual, z_proprio_tiled, z_action_tiled], dim=-1)           â”‚
â”‚                                                                             â”‚
â”‚  z: (B, 4, 256, 384+10+10) = (B, 4, 256, 404)                              â”‚
â”‚                                                                             â”‚
â”‚  Visualisierung eines Patches:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ DINO Features (384) â”‚ Proprio (10) â”‚ Action (10) â”‚ = 404 dim â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 15: Forward Pass - Prediction                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Transformer sagt basierend auf Historie den nÃ¤chsten Zustand vorher.â”‚
â”‚  Kausale Maske: Frame 2 kann Frame 0,1,2 sehen, aber nicht Frame 3.         â”‚
â”‚  Target ist um 1 Zeitschritt verschoben - Modell lernt "was kommt als nÃ¤chstes".â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Source (Input fÃ¼r Predictor):                                              â”‚
â”‚  z_src = z[:, :num_hist]   = z[:, :3]   # Erste 3 Zeitschritte             â”‚
â”‚  z_src: (B, 3, 256, 404)                                                    â”‚
â”‚                                                                             â”‚
â”‚  Target (Ground Truth):                                                     â”‚
â”‚  z_tgt = z[:, num_pred:]   = z[:, 1:]   # Letzte 3 Zeitschritte            â”‚
â”‚  z_tgt: (B, 3, 256, 404)   # Zeitlich um 1 verschoben                      â”‚
â”‚                                                                             â”‚
â”‚  ViT Predictor:                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚  1. Reshape: (B, 3, 256, 404) â†’ (B, 768, 404)  # 3Ã—256 = 768 Tokens        â”‚
â”‚  2. Position Embedding addieren                                             â”‚
â”‚  3. 6Ã— Transformer Blocks mit KAUSALER MASKE                               â”‚
â”‚  4. Output: (B, 768, 404) â†’ (B, 3, 256, 404)                               â”‚
â”‚                                                                             â”‚
â”‚  z_pred: (B, 3, 256, 404)                                                   â”‚
â”‚                                                                             â”‚
â”‚  Kausale Maske Visualisierung:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚     Frame 0   Frame 1   Frame 2         â”‚                               â”‚
â”‚  â”‚  F0   âœ“         âœ—         âœ—             â”‚  â† Kann nur sich selbst sehen â”‚
â”‚  â”‚  F1   âœ“         âœ“         âœ—             â”‚  â† Kann F0 und sich sehen     â”‚
â”‚  â”‚  F2   âœ“         âœ“         âœ“             â”‚  â† Kann alle sehen            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 16: Forward Pass - Decoding                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Latent-Space Vorhersagen zurÃ¼ck in Pixel-Bilder wandeln.            â”‚
â”‚  ErmÃ¶glicht visuelle Inspektion der VorhersagequalitÃ¤t.                     â”‚
â”‚  Decoder trainiert auf Rekonstruktion, nicht auf Vorhersage (detached).     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Separate Embeddings:                                                       â”‚
â”‚  z_pred: (B, 3, 256, 404)                                                   â”‚
â”‚      â†“                                                                      â”‚
â”‚  z_visual_pred: (B, 3, 256, 384)  # Nur DINO Features                      â”‚
â”‚  z_proprio_pred: (B, 3, 10)        # Proprio (nicht decodiert)             â”‚
â”‚  z_action_pred: (B, 3, 10)         # Action (nicht decodiert)              â”‚
â”‚                                                                             â”‚
â”‚  VQ-VAE Decoder:                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  1. Reshape: (B, 3, 256, 384) â†’ (BÃ—3, 16, 16, 384)  # âˆš256 = 16           â”‚
â”‚  2. Permute: (BÃ—3, 16, 16, 384) â†’ (BÃ—3, 384, 16, 16)                       â”‚
â”‚  3. Upsample 4Ã—: (BÃ—3, 384, 16, 16) â†’ (BÃ—3, 384, 64, 64)                   â”‚
â”‚  4. Decode: (BÃ—3, 384, 64, 64) â†’ (BÃ—3, 3, 224, 224)                        â”‚
â”‚  5. Reshape: (BÃ—3, 3, 224, 224) â†’ (B, 3, 3, 224, 224)                      â”‚
â”‚                                                                             â”‚
â”‚  visual_pred: (B, 3, 3, 224, 224)  # Vorhersagte Bilder                    â”‚
â”‚                                                                             â”‚
â”‚  ZusÃ¤tzlich: Rekonstruktion der Originalen                                  â”‚
â”‚  visual_recon: (B, 4, 3, 224, 224)  # Alle 4 Frames rekonstruiert          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 6: Loss-Berechnung

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 17: Loss-Berechnung                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Fehler zwischen Vorhersage und Ground Truth quantifizieren.         â”‚
â”‚  z_loss trainiert den Predictor im kompakten Latent-Space (robust).         â”‚
â”‚  decoder_loss trainiert den Decoder fÃ¼r gute Bildrekonstruktion.            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LATENT SPACE LOSS (z_loss)                                          â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚   â”‚
â”‚  â”‚  z_pred: (B, 3, 256, 404) - Vorhersage                              â”‚   â”‚
â”‚  â”‚  z_tgt:  (B, 3, 256, 404) - Ground Truth (detached)                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  z_visual_loss = MSE(z_pred[..., :384], z_tgt[..., :384])           â”‚   â”‚
â”‚  â”‚  z_proprio_loss = MSE(z_pred[..., 384:394], z_tgt[..., 384:394])    â”‚   â”‚
â”‚  â”‚  z_loss = MSE(z_pred[..., :394], z_tgt[..., :394])  # ohne Action   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Gewichtung: 1.0                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    +                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DECODER LOSS (Reconstruction)                                       â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚   â”‚
â”‚  â”‚  visual_recon: (B, 4, 3, 224, 224) - Rekonstruierte Bilder          â”‚   â”‚
â”‚  â”‚  obs['visual']: (B, 4, 3, 224, 224) - Original Bilder               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  recon_loss = MSE(visual_recon, obs['visual'])                      â”‚   â”‚
â”‚  â”‚  vq_loss = 0 (da quantize=False)                                    â”‚   â”‚
â”‚  â”‚  decoder_loss = recon_loss + 0.25 Ã— vq_loss                         â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Gewichtung: 1.0                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    +                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DECODER PREDICTION LOSS (Optional)                                  â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚   â”‚
â”‚  â”‚  visual_pred: (B, 3, 3, 224, 224) - Vorhersagte Bilder              â”‚   â”‚
â”‚  â”‚  obs['visual'][:, 1:]: (B, 3, 3, 224, 224) - Ground Truth           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  pred_recon_loss = MSE(visual_pred, obs['visual'][:, 1:])           â”‚   â”‚
â”‚  â”‚  (Dieser Loss wird geloggt aber nicht zum Training verwendet)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    =                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TOTAL LOSS                                                          â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚   â”‚
â”‚  â”‚  loss = z_loss + decoder_loss                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 7: Validation und Logging

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 18: Validation                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Modell auf ungesehenen Daten evaluieren zur Overfitting-Erkennung.  â”‚
â”‚  Open-Loop Rollout testet autoregressive Vorhersage Ã¼ber mehrere Schritte.  â”‚
â”‚  model.eval() deaktiviert Dropout und Batch-Normalisierung fÃ¼r Konsistenz.  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  model.eval()  # Keine Gradientenberechnung                                â”‚
â”‚                                                                             â”‚
â”‚  1. Standard Validation (wie Training, aber ohne Optimizer-Steps)          â”‚
â”‚  2. Open-Loop Rollout:                                                      â”‚
â”‚     - Nimm erste num_hist Frames                                           â”‚
â”‚     - Sage iterativ zukÃ¼nftige Frames vorher                               â”‚
â”‚     - Vergleiche mit Ground Truth                                          â”‚
â”‚                                                                             â”‚
â”‚  Rollout-Visualisierung:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  t=0   t=1   t=2   t=3   t=4   t=5   ...                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  GT:   [F0]  [F1]  [F2]  [F3]  [F4]  [F5]  ...                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Pred: [F0]  [F1]  [F2]  [P3]  [P4]  [P5]  ...                      â”‚   â”‚
â”‚  â”‚         â†‘     â†‘     â†‘     â†‘                                         â”‚   â”‚
â”‚  â”‚       Input Input Input Vorhersage (autoregressiv)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 19: Logging zu Weights & Biases                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Training-Fortschritt visualisieren und Experimente vergleichen.     â”‚
â”‚  Loss-Kurven zeigen Konvergenz, Bild-Metriken (PSNR/SSIM) zeigen QualitÃ¤t.  â”‚
â”‚  Visualisierungen helfen, Fehlerquellen schnell zu identifizieren.          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Geloggte Metriken:                                                         â”‚
â”‚  - train_loss, val_loss                                                     â”‚
â”‚  - train_z_loss, val_z_loss                                                 â”‚
â”‚  - train_z_visual_loss, val_z_visual_loss                                   â”‚
â”‚  - train_z_proprio_loss, val_z_proprio_loss                                 â”‚
â”‚  - train_decoder_recon_loss, val_decoder_recon_loss                         â”‚
â”‚  - z_visual_err_rollout, z_proprio_err_rollout                              â”‚
â”‚  - Image Quality Metrics (PSNR, SSIM, etc.)                                 â”‚
â”‚                                                                             â”‚
â”‚  Visualisierungen:                                                          â”‚
â”‚  - Rekonstruierte Bilder vs. Ground Truth                                   â”‚
â”‚  - Vorhersage-Sequenzen                                                     â”‚
â”‚  - Rollout-Plots                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 20: Checkpoint speichern                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Zweck: Modell-Zustand sichern fÃ¼r spÃ¤teres Laden oder Inferenz.            â”‚
â”‚  Speichert alle Weights + Optimizer-States fÃ¼r nahtlose Fortsetzung.        â”‚
â”‚  model_latest.pth wird bei jedem Save Ã¼berschrieben, model_N.pth bleibt.    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  if epoch % save_every_x_epoch == 0:                                        â”‚
â”‚      torch.save({                                                           â”‚
â”‚          'epoch': epoch,                                                    â”‚
â”‚          'encoder': encoder.state_dict(),         # DINO Weights           â”‚
â”‚          'predictor': predictor.state_dict(),     # ViT Weights            â”‚
â”‚          'decoder': decoder.state_dict(),         # VQ-VAE Weights         â”‚
â”‚          'action_encoder': action_encoder.state_dict(),                     â”‚
â”‚          'proprio_encoder': proprio_encoder.state_dict(),                   â”‚
â”‚          'encoder_optimizer': ...,                                          â”‚
â”‚          'predictor_optimizer': ...,                                        â”‚
â”‚          'decoder_optimizer': ...,                                          â”‚
â”‚      }, f"checkpoints/model_{epoch}.pth")                                   â”‚
â”‚                                                                             â”‚
â”‚  Gespeichert in: outputs/DATUM/ZEIT/checkpoints/                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Modell-Architektur

### 5.1 DINO v2 Encoder

**Was ist DINO?**
DINO (Self-**DI**stillation with **NO** labels) ist ein selbstÃ¼berwachtes Vision-Modell von Meta/Facebook, das ohne Labels trainiert wurde.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DINO v2 ViT-Small/14                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: RGB Bild (3, 224, 224)                                              â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Patch Embedding                                                     â”‚   â”‚
â”‚  â”‚  - Bild in 14Ã—14 Patches aufteilen                                  â”‚   â”‚
â”‚  â”‚  - 224/14 = 16 Patches pro Seite                                    â”‚   â”‚
â”‚  â”‚  - 16 Ã— 16 = 256 Patches total                                      â”‚   â”‚
â”‚  â”‚  - Jeder Patch: 14Ã—14Ã—3 = 588 Pixel â†’ Linear â†’ 384 dim              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Transformer Encoder (12 BlÃ¶cke)                                     â”‚   â”‚
â”‚  â”‚  - Self-Attention Ã¼ber alle 256 Patches                             â”‚   â”‚
â”‚  â”‚  - Lernt rÃ¤umliche Beziehungen                                      â”‚   â”‚
â”‚  â”‚  - Vortrainiert auf ImageNet (ohne Labels!)                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: Patch Tokens (256, 384)                                            â”‚
â”‚          [num_patches, emb_dim]                                             â”‚
â”‚                                                                             â”‚
â”‚  Eigenschaften:                                                             â”‚
â”‚  - Vortrainiert: Parameter werden NICHT verÃ¤ndert                          â”‚
â”‚  - Semantic Features: Lernt bedeutungsvolle visuelle ReprÃ¤sentationen      â”‚
â”‚  - Patch-basiert: ErhÃ¤lt rÃ¤umliche Information                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Warum DINO?**
- Starke, generalisierbare visuelle Features
- Funktioniert gut auf Robotik-DomÃ¤ne (obwohl auf natÃ¼rliche Bilder trainiert)
- Keine zusÃ¤tzlichen Labels nÃ¶tig

### 5.2 Action & Proprio Encoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Proprioceptive Embedding (MLP)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Action Encoder:                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  Input: (B, T, 45)     # 45 = 9 actions Ã— 5 frameskip                      â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Conv1d(45 â†’ 10, kernel=1, stride=1)                                       â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: (B, T, 10)    # Komprimierte Action-ReprÃ¤sentation                â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  Proprio Encoder:                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  Input: (B, T, 3)      # EE-Position (x, y, z)                             â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Conv1d(3 â†’ 10, kernel=1, stride=1)                                        â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: (B, T, 10)    # Komprimierte Proprio-ReprÃ¤sentation               â”‚
â”‚                                                                             â”‚
â”‚  Zweck:                                                                     â”‚
â”‚  - Dimensionsreduktion fÃ¼r effiziente Verarbeitung                         â”‚
â”‚  - Lernt relevante Features aus rohen Sensor-Daten                         â”‚
â”‚  - Wird MIT trainiert (im Gegensatz zu DINO)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 ViT Predictor

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Vision Transformer Predictor                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: z (B, 3, 256, 404)  # 3 Frames Ã— 256 Patches Ã— 404 dim             â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (B, 768, 404)     # 3Ã—256 = 768 Tokens                           â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Positional Embedding                                                â”‚   â”‚
â”‚  â”‚  - Lernbares Embedding: (1, 768, 404)                               â”‚   â”‚
â”‚  â”‚  - Addiert zu Input                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  6Ã— Transformer Block                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Multi-Head Attention (16 Heads)                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - KAUSAL: Kann nur vergangene Tokens sehen                   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Query, Key, Value: Linear(404 â†’ 64Ã—16)                     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Output: Linear(64Ã—16 â†’ 404)                                â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚            â”‚ + Residual                                              â”‚   â”‚
â”‚  â”‚            â–¼                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Feed-Forward Network                                         â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Linear(404 â†’ 2048) â†’ GELU â†’ Linear(2048 â†’ 404)              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚            â”‚ + Residual                                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  LayerNorm                                                                  â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (B, 768, 404) â†’ (B, 3, 256, 404)                                 â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: z_pred (B, 3, 256, 404)                                            â”‚
â”‚                                                                             â”‚
â”‚  Kausale Maske (Visualisierung fÃ¼r 3 Frames Ã— 2 Patches):                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚        P0_F0  P1_F0  P0_F1  P1_F1  P0_F2  P1_F2    â”‚                   â”‚
â”‚  â”‚ P0_F0    1      1      0      0      0      0      â”‚                   â”‚
â”‚  â”‚ P1_F0    1      1      0      0      0      0      â”‚                   â”‚
â”‚  â”‚ P0_F1    1      1      1      1      0      0      â”‚                   â”‚
â”‚  â”‚ P1_F1    1      1      1      1      0      0      â”‚                   â”‚
â”‚  â”‚ P0_F2    1      1      1      1      1      1      â”‚                   â”‚
â”‚  â”‚ P1_F2    1      1      1      1      1      1      â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  (1 = kann sehen, 0 = maskiert)                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 VQ-VAE Decoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          VQ-VAE Decoder                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: z_visual (B, T, 256, 384)   # Nur visuelle Features                â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (BÃ—T, 16, 16, 384)        # âˆš256 = 16                            â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Quantize (DEAKTIVIERT: quantize=False)                             â”‚   â”‚
â”‚  â”‚  - Bei aktiviert: Diskretisierung in Codebook                       â”‚   â”‚
â”‚  â”‚  - Hier: Kontinuierlicher Latent-Space                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Permute: (BÃ—T, 384, 16, 16)        # Channel-first fÃ¼r Conv              â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Upsample Block (4Ã—)                                                 â”‚   â”‚
â”‚  â”‚  Conv2d(384, 384) + 4Ã— ResBlock + ConvTranspose2d(stride=2) Ã—2      â”‚   â”‚
â”‚  â”‚  (16, 16) â†’ (32, 32) â†’ (64, 64)                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  (BÃ—T, 384, 64, 64)                                                         â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Decode Block (4Ã—)                                                   â”‚   â”‚
â”‚  â”‚  Conv2d(384, 384) + 4Ã— ResBlock + ConvTranspose2d(stride=2) Ã—2      â”‚   â”‚
â”‚  â”‚  (64, 64) â†’ (128, 128) â†’ (256, 256) â†’ Resize â†’ (224, 224)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: (BÃ—T, 3, 224, 224)         # RGB Bilder                           â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (B, T, 3, 224, 224)                                               â”‚
â”‚                                                                             â”‚
â”‚  ResBlock Architektur:                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚   â”‚
â”‚  â”‚    â”‚                                        â”‚                       â”‚   â”‚
â”‚  â”‚    â–¼                                        â”‚                       â”‚   â”‚
â”‚  â”‚  ReLU â†’ Conv3Ã—3 â†’ ReLU â†’ Conv1Ã—1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(+)â”€â”€â†’ Output           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.5 Gesamtarchitektur: VWorldModel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Visual World Model                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                           INPUT                                     â”‚    â”‚
â”‚  â”‚  obs['visual']: (B, 4, 3, 224, 224)                                â”‚    â”‚
â”‚  â”‚  obs['proprio']: (B, 4, 3)                                         â”‚    â”‚
â”‚  â”‚  act: (B, 4, 45)                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â”‚              â”‚              â”‚                          â”‚
â”‚                    â–¼              â–¼              â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   DINO Encoder   â”‚  â”‚  Proprio Encoder â”‚  â”‚  Action Encoder  â”‚          â”‚
â”‚  â”‚   (FROZEN)       â”‚  â”‚  (TRAINABLE)     â”‚  â”‚  (TRAINABLE)     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚          â”‚                     â”‚                      â”‚                     â”‚
â”‚          â–¼                     â–¼                      â–¼                     â”‚
â”‚    (B, 4, 256, 384)      (B, 4, 10)            (B, 4, 10)                   â”‚
â”‚          â”‚                     â”‚                      â”‚                     â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                    â”‚     CONCATENATE      â”‚                                â”‚
â”‚                    â”‚   (concat_dim=1)     â”‚                                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                         (B, 4, 256, 404)                                    â”‚
â”‚                                â”‚                                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â–¼                                   â–¼                          â”‚
â”‚       z_src[:, :3]                        z_tgt[:, 1:]                      â”‚
â”‚     (Historie)                           (Target)                           â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â–¼                                   â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚                          â”‚
â”‚  â”‚    ViT Predictor     â”‚                       â”‚                          â”‚
â”‚  â”‚    (TRAINABLE)       â”‚                       â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚                          â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â–¼                                   â”‚                          â”‚
â”‚        z_pred                                    â”‚                          â”‚
â”‚     (B, 3, 256, 404)                            â”‚                          â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                    â”‚   LATENT LOSS    â”‚                                    â”‚
â”‚                    â”‚  MSE(z_pred,     â”‚                                    â”‚
â”‚                    â”‚      z_tgt)      â”‚                                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                              â”‚                                              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â–¼                               â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   VQ-VAE Decoder     â”‚        â”‚   VQ-VAE Decoder     â”‚                  â”‚
â”‚  â”‚   (TRAINABLE)        â”‚        â”‚   (TRAINABLE)        â”‚                  â”‚
â”‚  â”‚   auf z_pred         â”‚        â”‚   auf z (alle)       â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â–¼                               â–¼                              â”‚
â”‚      visual_pred                     visual_recon                          â”‚
â”‚    (B, 3, 3, 224, 224)            (B, 4, 3, 224, 224)                      â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â”‚                               â–¼                              â”‚
â”‚              â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚                    â”‚  DECODER LOSS    â”‚                      â”‚
â”‚              â”‚                    â”‚  MSE(recon, gt)  â”‚                      â”‚
â”‚              â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                    â”‚    TOTAL LOSS    â”‚                                    â”‚
â”‚                    â”‚ z_loss + decoder â”‚                                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Loss-Funktionen

### 6.1 Ãœbersicht aller Losses

| Loss Name | Formel | Gewichtung | Zweck |
|-----------|--------|------------|-------|
| `z_loss` | MSE(z_pred, z_tgt) | 1.0 | Hauptloss fÃ¼r Predictor |
| `z_visual_loss` | MSE(z_pred_visual, z_tgt_visual) | (geloggt) | Nur visuelle Features |
| `z_proprio_loss` | MSE(z_pred_proprio, z_tgt_proprio) | (geloggt) | Nur Proprio-Features |
| `decoder_recon_loss` | MSE(visual_recon, obs_visual) | 1.0 | RekonstruktionsqualitÃ¤t |
| `decoder_vq_loss` | Commitment Loss | 0.25 | VQ Regularisierung (=0 wenn quantize=False) |
| `decoder_loss` | recon + 0.25Ã—vq | 1.0 | Decoder-Training |

### 6.2 Warum diese Kombination?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ZWEI-STUFEN TRAINING-STRATEGIE                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. LATENT LOSS (z_loss):                                                   â”‚
â”‚     - Trainiert den Predictor im kompakten Latent-Space                    â”‚
â”‚     - Weniger anfÃ¤llig fÃ¼r Pixel-Level Noise                               â”‚
â”‚     - Fokussiert auf semantische Vorhersage                                â”‚
â”‚                                                                             â”‚
â”‚  2. DECODER LOSS (decoder_loss):                                            â”‚
â”‚     - Trainiert den Decoder zur Bildrekonstruktion                         â”‚
â”‚     - Stellt sicher, dass Latent-Space interpretierbar bleibt             â”‚
â”‚     - ErmÃ¶glicht Visualisierung der Vorhersagen                            â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: Decoder wird auf z.detach() trainiert                            â”‚
â”‚           â†’ Decoder-Gradients flieÃŸen NICHT zum Predictor                  â”‚
â”‚           â†’ Verhindert, dass Decoder den Predictor "betrÃ¼gt"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Training starten

### 7.1 Basis-Kommando

```bash
cd /media/tsp_jw/fc8bca1b-cab8-4522-81d0-06172d2beae8/dino_wm2

# Standard-Training
python train.py env=franka_cube_stack

# Mit expliziten Parametern
python train.py env=franka_cube_stack \
    frameskip=5 \
    num_hist=3 \
    training.epochs=100 \
    training.batch_size=8
```

### 7.2 Empfohlene Parameter fÃ¼r deinen Datensatz

Da du nur 10 Episoden hast, hier optimierte Einstellungen:

```bash
python train.py env=franka_cube_stack \
    frameskip=3 \                    # Feinere Dynamik
    num_hist=3 \                     # Standard Kontext
    training.epochs=200 \            # Mehr Epochen (kleiner Datensatz)
    training.batch_size=8 \          # Kleinere Batch-Size
    training.predictor_lr=3e-4 \     # Etwas niedriger
    training.decoder_lr=2e-4 \       # Etwas niedriger
    debug=True                       # Wandb Debug-Projekt
```

### 7.3 Erwartete Ausgabe

```
outputs/
â””â”€â”€ 2026-01-13/
    â””â”€â”€ 15-30-45/                    # Zeitstempel
        â”œâ”€â”€ checkpoints/
        â”‚   â”œâ”€â”€ model_latest.pth
        â”‚   â”œâ”€â”€ model_1.pth
        â”‚   â”œâ”€â”€ model_2.pth
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ train/
        â”‚   â””â”€â”€ train_e00001_b0.png  # Visualisierungen
        â”œâ”€â”€ valid/
        â”‚   â””â”€â”€ valid_e00001_b0.png
        â”œâ”€â”€ rollout_plots/
        â”‚   â””â”€â”€ e1_rollout/
        â””â”€â”€ hydra.yaml               # Gespeicherte Konfiguration
```

### 7.4 Monitoring mit Weights & Biases

Training wird automatisch zu W&B geloggt:
- Projekt: `dino_wm_debug` (wenn `debug=True`) oder `dino_wm`
- Metriken: Loss-Kurven, Image Metrics, Visualisierungen

---

## 8. Glossar

| Begriff | ErklÃ¤rung |
|---------|-----------|
| **DINO** | Self-Distillation with No Labels - vortrainiertes Vision-Modell |
| **ViT** | Vision Transformer - Transformer-Architektur fÃ¼r Bilder |
| **VQ-VAE** | Vector Quantized Variational Autoencoder - generatives Modell |
| **Patch** | Bildausschnitt (14Ã—14 Pixel bei DINO) |
| **Embedding** | Kompakte VektorreprÃ¤sentation |
| **Latent Space** | Komprimierter ReprÃ¤sentationsraum |
| **Frameskip** | Temporales Subsampling - jeder n-te Frame wird verwendet |
| **num_hist** | Anzahl Kontext-Frames als Input |
| **num_pred** | Anzahl vorherzusagender zukÃ¼nftiger Frames |
| **Causal Mask** | Verhindert, dass Modell zukÃ¼nftige Frames sieht |
| **MSE** | Mean Squared Error - quadratischer Fehler |
| **Proprio** | Proprioceptive Daten - Roboter-Eigenwahrnehmung (z.B. Gelenkwinkel) |
| **Accelerator** | HuggingFace Tool fÃ¼r verteiltes Training |

---

## 9. Troubleshooting

### 9.1 Training Freeze / Deadlock (kein Temperatur-Problem)

**Symptome:**
- Training stoppt ohne Fehlermeldung
- GPU-Temperatur ist normal (~48Â°C)
- `ps aux | grep train.py` zeigt Zombie-Prozess `[python] <defunct>`
- Mehrere DataLoader-Worker-Prozesse hÃ¤ngen

**Ursache:** PyTorch DataLoader Multiprocessing Deadlock
- `num_workers > 0` kann bei `torch.load()` in Subprozessen deadlocken
- Bekanntes PyTorch-Issue bei groÃŸen Tensoren

**LÃ¶sung 1: num_workers auf 0 setzen**
```yaml
# conf/env/franka_cube_stack.yaml
num_workers: 0  # Deaktiviert Multiprocessing - langsamer aber stabil
```

**LÃ¶sung 2: preload_images aktivieren (bereits Standard)**
```python
# In FrankaCubeStackDataset - lÃ¤dt alle Bilder beim Init in RAM
preload_images=True  # Verhindert torch.load() in Worker-Prozessen
```

**LÃ¶sung 3: Debugging aktivieren**
```bash
CUDA_LAUNCH_BLOCKING=1 python train.py env=franka_cube_stack
```

### 9.2 GPU Out of Memory (OOM)

**Symptome:**
- `RuntimeError: CUDA out of memory`
- Training crasht beim ersten Batch

**LÃ¶sung:**
```bash
# Batch-Size reduzieren
python train.py env=franka_cube_stack training.batch_size=8

# Oder num_hist reduzieren
python train.py env=franka_cube_stack num_hist=2
```

### 9.3 GPU Thermal Throttling

**Symptome:**
- Training wird langsamer Ã¼ber Zeit
- GPU-Temperatur >80Â°C
- `nvidia-smi` zeigt reduzierte Taktrate

**LÃ¶sung:**
```bash
# Power Limit reduzieren
sudo nvidia-smi -pl 100

# LÃ¼ftersteuerung mit GreenWithEnvy
flatpak run com.leinardi.gwe
```

---

## Anhang: Datensatz-Statistiken

FÃ¼r deinen Datensatz `2026_01_13_1152_fcs_dset`:

| Metrik | Wert |
|--------|------|
| Episoden | 10 |
| Frames pro Episode | 932 |
| Gesamtframes | 9.320 |
| State-Dimension | 22 |
| Action-Dimension | 9 |
| BildgrÃ¶ÃŸe | 256Ã—256 â†’ resize auf 224Ã—224 |
| Training-Samples (frameskip=5, num_hist=3) | ~8.217 |
| Validation-Samples | ~913 |
| Speicherbedarf (Bilder) | ~10 Ã— 932 Ã— 256 Ã— 256 Ã— 3 â‰ˆ 1.8 GB |

---

*Dokumentation erstellt am 13.01.2026*


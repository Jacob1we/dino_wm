# ğŸ§  DINO World Model - VollstÃ¤ndige Training-Dokumentation

> Eine detaillierte, chronologische Dokumentation des gesamten Trainingsprozesses fÃ¼r das DINO World Model mit dem Franka Cube Stacking Datensatz.

---

## ğŸ“‘ Inhaltsverzeichnis

1. [Ãœberblick und Konzept](#1-Ã¼berblick-und-konzept)
2. [Datensatz-Struktur](#2-datensatz-struktur)
3. [Konfiguration und Parameter](#3-konfiguration-und-parameter)
4. [Training-Pipeline (Chronologisch)](#4-training-pipeline-chronologisch)
5. [Modell-Architektur](#5-modell-architektur)
6. [Proprioceptive Encoder â€” VollstÃ¤ndiger Trainingsablauf](#6-proprioceptive-encoder--vollstÃ¤ndiger-trainingsablauf)
7. [Loss-Funktionen](#7-loss-funktionen)
8. [W&B Metriken und Monitoring](#8-wb-metriken-und-monitoring)
9. [Training starten](#9-training-starten)
10. [Glossar](#10-glossar)
11. [ğŸš¨ KRITISCH: Action-Observation Temporale Alignment-Analyse (20.02.2026)](#-kritisch-action-observation-temporale-alignment-analyse-20022026)

---

## 1. Ãœberblick und Konzept

### Was ist das DINO World Model?

Das **DINO World Model** ist ein visuelles Weltmodell, das lernt, zukÃ¼nftige visuelle ZustÃ¤nde eines Roboters vorherzusagen. Es kombiniert:

- **DINO v2 Encoder**: Vortrainiertes Vision-Modell von Meta zur BildreprÃ¤sentation
- **ViT Predictor**: Vision Transformer zur Vorhersage im Latent-Space
- **VQ-VAE Decoder**: Rekonstruktion von Bildern aus dem Latent-Space

### Konzept der Vorhersage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WORLD MODEL KONZEPT                                   â”‚
â”‚                                                                             â”‚
â”‚    Gegeben:  [Bild_t-2, Bild_t-1, Bild_t] + [Aktionen]                     â”‚
â”‚    Ziel:     Vorhersage von Bild_t+1 im Latent-Space                       â”‚
â”‚                                                                             â”‚
â”‚    Das Modell lernt die DYNAMIK der Welt:                                  â”‚
â”‚    "Wenn ich diese Bilder sehe und diese Aktion ausfÃ¼hre,                  â”‚
â”‚     wie wird die Welt danach aussehen?"                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Datensatz-Struktur

### 2.1 Aktueller Datensatz (Primitiv-basiert)

**Aktuell:** `NEps1000_RobOpac0_NPrim20_NCams4_NCube1` (985 Episoden, 20 Primitive/Timesteps)

```
NEps1000_RobOpac0_NPrim20_NCams4_NCube1/
â”œâ”€â”€ states.pth          # WÃ¼rfelpositionen: (985, 20, N_cubes*4)
â”œâ”€â”€ actions.pth         # Aktionen: (985, 20, 8)  â† 8D mit Gripper
â”œâ”€â”€ metadata.pkl        # Metadaten
â”œâ”€â”€ seq_lengths.pkl     # SequenzlÃ¤ngen pro Episode
â”œâ”€â”€ cameras/            # Kamera-Konfiguration
â””â”€â”€ 000000/ ... 000984/ # 985 Episoden
    â”œâ”€â”€ obses.pth       # RGB-Bilder: (20, 256, 256, 3) uint8
    â”œâ”€â”€ 00.h5 ... 19.h5 # Pro Primitiv eine H5-Datei
    â”‚   â”œâ”€â”€ action        # (8,) 8D-Action [start_pos, g_start, end_pos, g_end]
    â”‚   â”œâ”€â”€ eef_states    # (1, 1, 14) â†’ 14D EEF-Zustand (Start+End)
    â”‚   â”œâ”€â”€ positions     # (1, N_cubes, 4) WÃ¼rfelpositionen (homogen)
    â”‚   â”œâ”€â”€ observations/ # color + depth Bilder
    â”‚   â””â”€â”€ info/         # Metadaten (phase, n_steps, movement_distance, ...)
    â””â”€â”€ property_params.pkl
```

**Wichtig:** Jeder Timestep = 1 Bewegungsprimitiv (nicht 1 Simulations-Frame!)  
Ein Primitiv fasst mehrere Simulations-Schritte zu einer diskreten Bewegungseinheit zusammen.

### 2.2 Datensatz-Dimensionen

| Komponente | Form | Beschreibung |
|------------|------|--------------|
| **Actions** | `(985, 20, 8)` | 985 Episoden, 20 Primitive, **8D** Action (mit Gripper) |
| **EEF States** | `(1, 1, 14)` pro H5 | 14D End-Effector-Zustand (Start + End des Primitivs) |
| **Images** | `(20, 256, 256, 3)` pro Episode | 20 RGB-Bilder (1 pro Primitiv, nach Bewegung) |
| **Proprio** | `(T, 3)` extrahiert | Nur EE-Position (x,y,z) = `eef[:, :3]` |

### 2.3 EEF-States Aufbau (14 Dimensionen) â€” Proprio-Quelle

Die `eef_states` speichern den End-Effector-Zustand am **Ende** (current) und **Anfang** (previous) 
jedes Primitivs. Das Format folgt der Referenz aus `robot_env.py` (Rope/Deformable Datensatz):

```
eef_states (14D) = [pos_end(3), pos_start(3), quat_end(4), quat_start(4)]
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚           â”‚             â”‚            â””â”€â”€ Orientierung am Primitiv-START
                    â”‚           â”‚             â””â”€â”€ Orientierung am Primitiv-ENDE (aktuell)
                    â”‚           â””â”€â”€ EE-Position am Primitiv-START (vorherig)
                    â””â”€â”€ EE-Position am Primitiv-ENDE (aktuell)

Index  Dim           Beschreibung                      Typ
â”€â”€â”€â”€â”€  â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€
0-2    pos_end       EE-Position NACH Bewegung (x,y,z) float64, Meter (lokal)
3-5    pos_start     EE-Position VOR Bewegung (x,y,z)  float64, Meter (lokal)
6-9    quat_end      EE-Quaternion NACH Bewegung        float64, [qx,qy,qz,qw]
10-13  quat_start    EE-Quaternion VOR Bewegung          float64, [qx,qy,qz,qw]
```

**Proprio-Extraktion:** Nur `eef[:, :3]` (= `pos_end`, aktuelle EE-Position) wird als 
Proprioceptive Input fÃ¼r das Modell verwendet â†’ **proprio_dim = 3**.

**Referenz-Vergleich:**
| | Franka (unser Datensatz) | Rope/Deformable (Referenz) |
|---|---|---|
| eef_states Format | `[pos_end, pos_start, quat_end, quat_start]` | `[pos_cur, pos_prev, quat_cur, quat_prev]` |
| Proprio verwendet | `eef[:, :3]` = pos_end (3D) | `np.zeros(1)` = Dummy (1D, nicht genutzt) |
| Semantik [0:3] | Aktuelle EE-Position | Aktuelle Partikelposition |
| Semantik [3:6] | Vorherige EE-Position | Vorherige Partikelposition |

### 2.4 Action-Vektor Aufbau (8 Dimensionen)

Der Action-Vektor beschreibt eine Bewegungsprimitiv als Startâ†’End-Transition des End-Effectors:

```
Action (8D) = [x_start, y_start, z_start, g_start, x_end, y_end, z_end, g_end]
               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  Primitiv-START (vorher)            â”‚  Primitiv-ENDE (nachher)
               â””â”€â”€ Wo war der EE?                    â””â”€â”€ Wohin hat er sich bewegt?

Index  Dim        Beschreibung                          Wertebereich
â”€â”€â”€â”€â”€  â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0      x_start    Start-Position X (vor/zurÃ¼ck)         ~0.2 - 0.7 m
1      y_start    Start-Position Y (links/rechts)       ~-0.3 - 0.3 m
2      z_start    Start-Position Z (HÃ¶he)               ~0.05 - 0.4 m
3      g_start    Gripper-State am Start                 0.0 (zu) / 0.04 (auf)
4      x_end      End-Position X                         ~0.2 - 0.7 m
5      y_end      End-Position Y                         ~-0.3 - 0.3 m
6      z_end      End-Position Z                         ~0.05 - 0.4 m
7      g_end      Gripper-State am Ende                  0.0 (zu) / 0.04 (auf)
```

**Beispiel-Action** (APPROACH-Primitiv):
```
[0.475, -0.018, 0.320, 0.040, 0.475, -0.018, 0.160, 0.040]
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Start: (0.475, -0.018, 0.320), Gripper offen
                                       End: (0.475, -0.018, 0.160), Gripper offen
 â†’ AbwÃ¤rtsbewegung: Î”z = -0.16m (AnnÃ¤herung an WÃ¼rfel)
```

**Ohne Gripper-Tracking** (Ã¤ltere DatensÃ¤tze): Action ist 6D = `[x_start, y_start, z_start, x_end, y_end, z_end]`

**Hinweis zur zeitlichen Ordnung (Action vs. EEF States):**
- Action: `[start â†’ end]` = zeitlich vorwÃ¤rts (Bewegungsbefehl: von wo nach wo)
- EEF States: `[current, previous]` = aktuell zuerst (Zustandsbeschreibung: wo bin ich, wo war ich)
- Diese unterschiedliche Konvention ist **kein Problem**, weil sie verschiedene Zwecke erfÃ¼llen:
  Action = Bewegungsrichtung, EEF States = Zustandsinformation. Das Modell lernt die Semantik.
  Das `proprio` nutzt ohnehin nur `eef[:, :3]` = pos_end = aktuelle Position.

**Kein Frameskip bei Primitiv-DatensÃ¤tzen:**
Da jeder Timestep bereits ein ganzes Bewegungsprimitiv reprÃ¤sentiert (nicht ein einzelner
Simulations-Frame), wird `frameskip=1` verwendet. Frameskip-Konkatenation entfÃ¤llt.
Effektive Action-Dimension = 8 (nicht 8 Ã— frameskip)
---

## 3. Konfiguration und Parameter

### 3.1 Haupt-Konfigurationsdatei: `conf/train.yaml`

```yaml
# KRITISCHE PARAMETER
frameskip: 5       # Temporales Subsampling
num_hist: 3        # Anzahl Kontext-Frames
num_pred: 1        # Anzahl Vorhersage-Frames (nur 1 unterstÃ¼tzt)
img_size: 224      # BildgrÃ¶ÃŸe fÃ¼r Encoder

### Temporales Subsampling (frameskip)
Temporales Subsampling bedeutet, dass nur jeder n-te Frame aus der Originalsequenz verwendet wird, anstatt alle Frames.

# TRAINING
training:
  epochs: 100
  batch_size: 12
  seed: 0
  save_every_x_epoch: 1
  encoder_lr: 1e-6      # DINO Encoder (eingefroren)
  decoder_lr: 3e-4      # VQ-VAE Decoder
  predictor_lr: 5e-4    # ViT Predictor
  action_encoder_lr: 5e-4

# EMBEDDING DIMENSIONEN
action_emb_dim: 10      # Action Embedding Dimension
proprio_emb_dim: 10     # Proprio Embedding Dimension
concat_dim: 1           # Wie Embeddings kombiniert werden (0 oder 1)

# MODELL-KOMPONENTEN
model:
  train_encoder: False   # DINO wird NICHT trainiert (vortrainiert)
  train_predictor: True  # Predictor wird trainiert
  train_decoder: True    # Decoder wird trainiert
```

### 3.2 Parameter-ErklÃ¤rung: `frameskip`

**Frameskip** definiert das temporale Subsampling der Daten:

Temporales Subsampling bedeutet, dass nur jeder n-te Frame aus der Originalsequenz verwendet wird, anstatt alle Frames.

Original-Aufnahme (30 FPS, 932 Frames):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚ 9 â”‚10 â”‚11 â”‚12 â”‚13 â”‚14 â”‚15 â”‚16 â”‚17 â”‚18 â”‚19 â”‚...
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Mit frameskip=5 (jeder 5. Frame):
â”Œâ”€â”€â”€â”           â”Œâ”€â”€â”€â”           â”Œâ”€â”€â”€â”           â”Œâ”€â”€â”€â”
â”‚ 0 â”‚           â”‚ 5 â”‚           â”‚10 â”‚           â”‚15 â”‚  ...
â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜
  â†“               â†“               â†“               â†“
Frame 0 â”€â”€â”€â”€â”€â”€â–º Frame 1 â”€â”€â”€â”€â”€â”€â–º Frame 2 â”€â”€â”€â”€â”€â”€â–º Frame 3  (fÃ¼r das Modell verwendet)

# Warum Subsampling?
Vorteil	                ErklÃ¤rung
GrÃ¶ÃŸere Bewegung	      Zwischen Frame 0 und Frame 5 passiert mehr als zwischen Frame 0 und Frame 1 â†’ leichter zu lernen
Weniger Redundanz	      Aufeinanderfolgende Frames sind oft fast identisch
Effektivere Aktionen	  5 Aktionen werden zu einer kombiniert â†’ reichhaltigere Information
LÃ¤ngere Zeitspannen	    Mit gleicher Anzahl Frames kann mehr Zeit abgedeckt werden

**Auswirkungen:**
- **GrÃ¶ÃŸere visuelle Differenzen** zwischen Frames â†’ einfacher zu lernen
- **Mehr Bewegung pro Schritt** â†’ Modell muss grÃ¶ÃŸere Dynamik erfassen
- **Aktionen werden konkateniert**: 5 Aktionen â†’ 1 kombinierte Aktion
  - `action_dim_effektiv = action_dim Ã— frameskip = 9 Ã— 5 = 45`

### 3.3 Parameter-ErklÃ¤rung: `num_hist`
Kontext-Frames (num_hist)
Kontext-Frames sind die Anzahl der vergangenen Bilder, die dem Modell als Input gegeben werden, um die Zukunft vorherzusagen.

Beispiel mit num_hist=3, num_pred=1, frameskip=5:
Input:      [Frame0, Frame5, Frame10] + [Actions 0-14]
                    â†“ Predictor
Output:     Vorhersage fÃ¼r Frame15

Zeitlinie:      t=0     t=5     t=10    t=15
                 â”‚       â”‚        â”‚       â”‚
                 â–¼       â–¼        â–¼       â–¼
              â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
              â”‚Bild â”‚ â”‚Bild â”‚ â”‚Bild â”‚ â”‚Bild â”‚
              â”‚  0  â”‚ â”‚  1  â”‚ â”‚  2  â”‚ â”‚  3  â”‚
              â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
                 â”‚       â”‚        â”‚       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                         â”‚                â”‚
                    KONTEXT (3)      VORHERSAGE (1)
                    (num_hist)        (num_pred)
                         â”‚                â”‚
                         â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ INPUT fÃ¼r Modell â”‚  â”‚ ZIEL/TARGET  â”‚
              â”‚ [Bild0,Bild1,    â”‚  â”‚   [Bild3]    â”‚
              â”‚  Bild2,Actions]  â”‚  â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Warum mehrere Kontext-Frames?
Grund	              ErklÃ¤rung
Geschwindigkeit	    Aus 2+ Frames kann Bewegungsrichtung inferiert werden
Beschleunigung	    Aus 3+ Frames kann Beschleunigung erkannt werden
Verdeckungen	      Objekte, die in einem Frame verdeckt sind, kÃ¶nnen in anderen sichtbar sein
AmbiguitÃ¤t	        Ein einzelnes Bild kann mehrdeutig sein (steht still? bewegt sich?)

**Warum wichtig:**
- Mehr Historie = besseres VerstÃ¤ndnis der Dynamik
- Geschwindigkeit/Beschleunigung kÃ¶nnen inferiert werden
- Trade-off: Mehr Speicher, aber bessere Vorhersagen


### Zusammenspiel beider Parameter frameskip und num_hist
Gesamte SequenzlÃ¤nge pro Sample = (num_hist + num_pred) Ã— frameskip
                                = (3 + 1) Ã— 5 = 20 Original-Frames

Aus deinen 932 Frames pro Episode:
â”œâ”€â”€ Sample 1: Frames [0, 5, 10, 15]  â†’ Input: [0,5,10], Target: [15]
â”œâ”€â”€ Sample 2: Frames [1, 6, 11, 16]  â†’ Input: [1,6,11], Target: [16]
â”œâ”€â”€ Sample 3: Frames [2, 7, 12, 17]  â†’ Input: [2,7,12], Target: [17]
...
â””â”€â”€ Sample 913: Frames [912, 917, 922, 927]

= 913 Trainingssamples pro Episode

### 3.4 Hyperparameter-AbhÃ¤ngigkeiten: Grenzen und Formeln

Die Parameter `frameskip`, `num_hist`, `num_pred`, `batch_size` und die EpisodenlÃ¤nge `T` stehen in direktem Zusammenhang. Falsche Kombinationen fÃ¼hren zu **0 Training-Samples** oder einem **Freeze bei der Validation**.

#### Zentrale Formeln

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORMEL 1: BenÃ¶tigte Frames pro Sample                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                                             â”‚
â”‚  benÃ¶tigte_frames = (num_hist + num_pred) Ã— frameskip                       â”‚
â”‚                                                                             â”‚
â”‚  Beispiel: (6 + 1) Ã— 2 = 14                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORMEL 2: Training funktioniert (Slices > 0)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚                                                                             â”‚
â”‚  (num_hist + num_pred) Ã— frameskip  â‰¤  T                                    â”‚
â”‚                                                                             â”‚
â”‚  âŸ¹  num_hist  â‰¤  âŒŠT / frameskipâŒ‹ - num_pred                               â”‚
â”‚  âŸ¹  num_hist  â‰¤  âŒŠT / frameskipâŒ‹ - 1                                      â”‚
â”‚                                                                             â”‚
â”‚  Wenn diese Bedingung NICHT erfÃ¼llt ist:                                    â”‚
â”‚  â†’ 0 Slices, kein Training mÃ¶glich                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORMEL 3: Slices pro Episode (= Trainingssamples pro Episode)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                                             â”‚
â”‚  slices = T - (num_hist + num_pred) Ã— frameskip + 1                         â”‚
â”‚                                                                             â”‚
â”‚  Beispiel (T=22, num_hist=6, frameskip=2):                                  â”‚
â”‚  slices = 22 - (6+1)Ã—2 + 1 = 22 - 14 + 1 = 9                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORMEL 4: Steps pro Epoch (= tqdm-Balken LÃ¤nge)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚                                                                             â”‚
â”‚  train_samples = Î£ max(0, T_i - (num_hist+num_pred) Ã— frameskip + 1)       â”‚
â”‚                  Ã¼ber alle Train-Episoden                                   â”‚
â”‚                                                                             â”‚
â”‚  â‰ˆ nutzbare_train_episoden Ã— slices_pro_episode                             â”‚
â”‚                                                                             â”‚
â”‚  steps_pro_epoch = âŒˆ train_samples / batch_size âŒ‰                          â”‚
â”‚                                                                             â”‚
â”‚  Hinweis: num_workers hat KEINEN Einfluss auf die Anzahl der Steps.         â”‚
â”‚  Workers beschleunigen nur das Laden der Daten.                             â”‚
â”‚                                                                             â”‚
â”‚  Beispiel (499 Ep., T=22, num_hist=6, frameskip=2, batch_size=4):           â”‚
â”‚  train_episoden â‰ˆ 419 (von 449, da ~30 Ep. zu kurz)                        â”‚
â”‚  train_samples  = 419 Ã— 9 = 3771                                           â”‚
â”‚  steps          = âŒˆ3771 / 4âŒ‰ = 943                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORMEL 5: Validation-Rollout friert NICHT ein                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚                                                                             â”‚
â”‚  Die openloop_rollout-Funktion setzt:                                       â”‚
â”‚      min_horizon = 2 + num_hist                                             â”‚
â”‚                                                                             â”‚
â”‚  und prÃ¼ft (strikt grÃ¶ÃŸer!):                                                â”‚
â”‚      max_horizon = âŒŠ(T - 1) / frameskipâŒ‹  >  min_horizon                   â”‚
â”‚                                                                             â”‚
â”‚  âŸ¹  âŒŠ(T-1) / frameskipâŒ‹  >  2 + num_hist                                  â”‚
â”‚  âŸ¹  num_hist  <  âŒŠ(T-1) / frameskipâŒ‹ - 2                                  â”‚
â”‚                                                                             â”‚
â”‚  Wenn diese Bedingung NICHT erfÃ¼llt ist:                                    â”‚
â”‚  â†’ Endlos-Schleife! Training hÃ¤ngt nach dem Train-Balken.                   â”‚
â”‚                                                                             â”‚
â”‚  ACHTUNG: Diese Grenze ist STRENGER als die Training-Grenze!                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Ãœbersichtstabelle: Maximales num_hist nach T und frameskip

**T = 25** (z.B. `primLogger_NEps1000_ActInt2`)

| frameskip | Max num_hist (Training) | Max num_hist (Rollout ohne Freeze) | Slices bei max Rollout |
|-----------|------------------------|------------------------------------|------------------------|
| 1 | 24 | 21 | 4 |
| 2 | 11 | 9 | 3 |
| **3** | **7** | **5** | **8** |
| 4 | 5 | 3 | 6 |
| 5 | 4 | 2 | 6 |

**T = 22** (z.B. `NEps500_RobOpac10`)

| frameskip | Max num_hist (Training) | Max num_hist (Rollout ohne Freeze) | Slices bei max Rollout |
|-----------|------------------------|------------------------------------|------------------------|
| 1 | 21 | 18 | 4 |
| **2** | **10** | **7** | **8** |
| 3 | 6 | 4 | 7 |
| 4 | 4 | 2 | 5 |
| 5 | 3 | 1 | 3 |

**T = 21** (z.B. `NEps500_RobOpac10` Ã¤ltere Version)

| frameskip | Max num_hist (Training) | Max num_hist (Rollout ohne Freeze) | Slices bei max Rollout |
|-----------|------------------------|------------------------------------|------------------------|
| 1 | 20 | 17 | 4 |
| **2** | **9** | **6** | **9** |
| 3 | 6 | 3 | 7 |
| 4 | 4 | 2 | 2 |
| 5 | 3 | 1 | 2 |

#### Empfohlene Konfigurationen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ziel: Maximale Historie bei stabilem Training + Validation            â”‚
â”‚                                                                        â”‚
â”‚  T=25, frameskip=3:  num_hist=5  â†’ 8 Slices/Ep   âœ… Empfohlen         â”‚
â”‚  T=25, frameskip=2:  num_hist=6  â†’ 12 Slices/Ep  âœ… Empfohlen         â”‚
â”‚  T=22, frameskip=2:  num_hist=6  â†’ 9 Slices/Ep   âœ… Empfohlen         â”‚
â”‚  T=22, frameskip=3:  num_hist=4  â†’ 7 Slices/Ep   âœ… OK                â”‚
â”‚                                                                        â”‚
â”‚  âš ï¸  Nicht verwenden (Rollout-Freeze):                                 â”‚
â”‚  T=25, frameskip=3, num_hist=6  â†’ Training OK, Rollout hÃ¤ngt!         â”‚
â”‚  T=22, frameskip=3, num_hist=5  â†’ Training OK, Rollout hÃ¤ngt!         â”‚
â”‚  T=25, frameskip=4, num_hist=6  â†’ Training scheitert (0 Slices)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Diagnostik: Warum friert mein Training ein?

```
Symptom: tqdm-Balken "Epoch X Train: 100%" fertig, danach keine Ausgabe mehr

Ursache: openloop_rollout() in val() sucht endlos nach einer
         Trajektorie die lang genug ist â†’ while-Schleife terminiert nie

PrÃ¼fung:
  1. Berechne: min_horizon = 2 + num_hist
  2. Berechne: max_horizon = âŒŠ(T - 1) / frameskipâŒ‹
  3. Wenn max_horizon â‰¤ min_horizon â†’ FREEZE!

LÃ¶sung:
  â†’ num_hist reduzieren, oder
  â†’ frameskip reduzieren, oder
  â†’ openloop_rollout in train.py absichern (max_attempts + Fallback)
```

### 3.5 Action & Proprio Embedding Prozess

Die `action_emb_dim: 10` und `proprio_emb_dim: 10` entsprechen **nicht** den Rohdimensionen der Daten (Action: 8, Proprio: 3). Stattdessen werden die Rohdaten durch einen **lernbaren Encoder** in diese Embedding-Dimensionen transformiert.

#### Schritt 1: Kein Frameskip bei Primitiv-DatensÃ¤tzen

Bei Primitiv-basierten DatensÃ¤tzen reprÃ¤sentiert jeder Timestep bereits ein ganzes Bewegungsprimitiv 
(mehrere Simulations-Schritte zusammengefasst). Daher wird `frameskip=1` verwendet:

```
Primitiv-basiert (frameskip=1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Action pro Primitiv: 8 Dimensionen  â”‚
â”‚  [x_s, y_s, z_s, g_s, x_e, y_e, z_e, g_e]  â”‚
â”‚  Keine Konkatenation nÃ¶tig!          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Effektive Action-Dimension = 8 Ã— 1 = 8 (nicht vergrÃ¶ÃŸert durch frameskip)
```

#### Schritt 2: Embedding durch Conv1d

Der `ProprioceptiveEmbedding`-Encoder transformiert die Rohdaten in kompakte Embeddings:

```
ACTION ENCODER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  (Batch, Time, 8)    â† 8D: [start_pos(3), g_start(1), end_pos(3), g_end(1)]
              â”‚
              â–¼
        Conv1d(8 â†’ 10)      â† Lernbare Projektion (kernel_size=1)
              â”‚
              â–¼
Output: (Batch, Time, 10)   â† action_emb_dim


PROPRIO ENCODER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  (Batch, Time, 3)    â† 3D: EE-Position [x, y, z] (= eef[:, :3] = pos_end)
              â”‚
              â–¼
        Conv1d(3 â†’ 10)      â† Lernbare Projektion (kernel_size=1)
              â”‚
              â–¼
Output: (Batch, Time, 10)   â† proprio_emb_dim
```

**Hinweis:** `Conv1d(kernel_size=1, stride=1)` ist Ã¤quivalent zu einer punktweisen linearen 
Transformation (Fully-Connected Layer pro Zeitschritt). Es werden KEINE temporalen Faltungen 
Ã¼ber benachbarte Zeitschritte durchgefÃ¼hrt.

#### Warum diese Transformation?

| Aspekt | ErklÃ¤rung |
|--------|-----------|
| **Dimensionsanpassung** | 8D Action / 3D Proprio â†’ einheitlich 10D Embedding |
| **Lernbare Features** | Netzwerk lernt, welche Action-Kombinationen wichtig sind |
| **KompatibilitÃ¤t** | Kleinere Embedding-Dimension passt besser zu DINO (384 dim) |
| **Regularisierung** | Verhindert Overfitting auf hochdimensionale Inputs |

#### Finale Embedding-Zusammensetzung (concat_dim=1)

Pro Patch im Latent-Space werden alle Embeddings konkateniert:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DINO Visual (384) â”‚ Proprio Emb (10) â”‚ Action Emb (10) â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚       384         â”‚        10        â”‚       10        â”‚= 404â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Code-Referenz

```python
# Aus models/proprio.py - ProprioceptiveEmbedding:
self.patch_embed = nn.Conv1d(
    in_chans,      # 8 fÃ¼r Actions (8D Primitiv), 3 fÃ¼r Proprio
    emb_dim,       # 10 (action_emb_dim / proprio_emb_dim)
    kernel_size=1,
    stride=1
)

# Aus train.py - Dynamische Dimensionen aus Datensatz:
proprio_encoder = ProprioceptiveEmbedding(
    in_chans=datasets["train"].proprio_dim,  # 3 (auto-detektiert)
    emb_dim=cfg.proprio_emb_dim              # 10 (aus Config)
)
action_encoder = ProprioceptiveEmbedding(
    in_chans=datasets["train"].action_dim,   # 8 (auto-detektiert aus H5)
    emb_dim=cfg.action_emb_dim               # 10 (aus Config)
)
```

**Zusammenfassung des Datenflusses:**
```
Actions:  (B, T, 8)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º (B, T, 8)  â”€â”€Conv1dâ”€â”€â–º (B, T, 10)
                      (kein frameskip)
Proprio:  (B, T, 3)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º (B, T, 3)  â”€â”€Conv1dâ”€â”€â–º (B, T, 10)
                      eef[:, :3]
Proprio:  (B, T, 3) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Conv1dâ”€â”€â–º (B, T, 10)
```

### 3.6 Umgebungs-Konfiguration: `conf/env/franka_cube_stack.yaml`

```yaml
name: franka_cube_stack
dataset:
  _target_: "datasets.franka_cube_stack_dset.load_franka_cube_stack_slice_train_val"
  data_path: /pfad/zu/deinem/datensatz
  n_rollout: null        # null = alle Rollouts laden
  normalize_action: true # Aktionen werden z-normalisiert
  split_ratio: 0.9       # 90% Train, 10% Validation
  transform:
    _target_: "datasets.img_transforms.default_transform"
    img_size: 224        # Resize auf 224x224

num_workers: 4           # Dataloader Workers
decoder_path: null       # Optional: Vortrainierter Decoder
```

---

### 3.7 VRAM-Analyse und Validierungs-Lastspitze

Die GPU-Speicherbelegung ist die **harte Grenze** fÃ¼r die Hyperparameter-Wahl. Auf der NVIDIA A5000 (24 564 MiB) bestimmt der VRAM maÃŸgeblich, wie hoch `num_hist` bei gegebenem `batch_size` und `frameskip` gewÃ¤hlt werden kann.

#### 3.7.1 VRAM-Modell: Drei Kostenklassen

Der VRAM-Verbrauch zerfÃ¤llt in drei Kategorien:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VRAM-Zerlegung                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                            â”‚
â”‚                                                                             â”‚
â”‚  1. FESTE KOSTEN (~559 MiB, konfigurationsunabhÃ¤ngig)                       â”‚
â”‚     â”œâ”€ Frozen Encoder (DINOv2 ViT-S/14):  21M Ã— 4 Bytes   â‰ˆ  80 MiB       â”‚
â”‚     â”œâ”€ Trainable Weights (fp16):          31M Ã— 2 Bytes   â‰ˆ  59 MiB       â”‚
â”‚     â”œâ”€ AdamW Optimizer States:            31M Ã— 12 Bytes  â‰ˆ 355 MiB       â”‚
â”‚     â”‚  (fp32 master copy + momentum + variance)                            â”‚
â”‚     â””â”€ Gradients (fp16):                  31M Ã— 2 Bytes   â‰ˆ  59 MiB       â”‚
â”‚                                                                             â”‚
â”‚  2. AKTIVIERUNGEN (~13 908 MiB bei bs=4, nh=6, fs=2) â† HAUPTTREIBER       â”‚
â”‚     â”œâ”€ DINOv2 Encoder:      linear in batch_size Ã— (num_hist + 1)          â”‚
â”‚     â”‚  (12 Layers Ã— Attention + FF pro Frame)                              â”‚
â”‚     â”œâ”€ ViT Predictor:       QUADRATISCH in num_hist Ã— 196                  â”‚
â”‚     â”‚  Attention-Matrix: O((num_hist Ã— 196)Â²) â† KRITISCH                  â”‚
â”‚     â”‚  (6 Layers, 16 Heads, seq_len = num_hist Ã— 196)                     â”‚
â”‚     â”œâ”€ VQVAE Decoder:       linear in batch_size Ã— (num_hist + 1) Ã— 2     â”‚
â”‚     â”‚  (2Ã— Forward: Prediction + Reconstruction)                          â”‚
â”‚     â””â”€ Misc: Loss-Buffers, einops-TemporÃ¤rtensoren, Tiling                â”‚
â”‚                                                                             â”‚
â”‚  3. CUDA OVERHEAD (~2000 MiB, Basis-Kosten)                                â”‚
â”‚     â”œâ”€ PyTorch CUDA Context                                                â”‚
â”‚     â”œâ”€ cuDNN Workspace                                                     â”‚
â”‚     â””â”€ CUDA Memory Allocator Reservierung                                  â”‚
â”‚                                                                             â”‚
â”‚  GESAMT = Feste Kosten + Aktivierungen + CUDA Overhead                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Warum Attention quadratisch skaliert:**

$$\text{VRAM}_{\text{Attention}} \propto B \times H_{\text{heads}} \times (\text{num\_hist} \times 196)^2 \times D \times 2$$

| num_hist | seq_len (Ã—196) | Attention-Speicher (relativ) |
|----------|---------------|------------------------------|
| 1        | 196           | 1Ã—                           |
| 3        | 588           | 9Ã—                           |
| 6        | 1176          | 36Ã—                          |
| 10       | 1960          | 100Ã—                         |

#### 3.7.2 Empirische Kalibrierung

Theoretische VRAM-Formeln unterschÃ¤tzen systematisch, da sie folgende Faktoren nicht erfassen:
- PyTorch Autograd-Graph (speichert Computation Graph fÃ¼r Backward)
- CUDA Memory Allocator Fragmentierung
- TemporÃ¤re Tensoren bei `einops.rearrange`, `torch.cat`, `repeat`
- cuDNN Workspace fÃ¼r optimierte Convolution-Kernel

**Kalibrierung an realen Messdaten:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REFERENZ-MESSPUNKT (A5000, Epoch 1)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  Konfiguration: batch_size=4, num_hist=6, frameskip=2                       â”‚
â”‚  Gemessen:      16 467 MiB  (67.0% von 24 564 MiB)                         â”‚
â”‚                                                                             â”‚
â”‚  Zerlegung:                                                                 â”‚
â”‚    Feste Kosten:     559 MiB                                                â”‚
â”‚    CUDA Overhead:   2 000 MiB                                               â”‚
â”‚    Activations:    13 908 MiB  (= 16467 - 559 - 2000)                       â”‚
â”‚                                                                             â”‚
â”‚  Theoretische Activations: ~2 524 MiB (viel zu niedrig!)                    â”‚
â”‚  â†’ Kalibrierungsfaktor: 13 908 / 2 524 = 5.51Ã—                             â”‚
â”‚                                                                             â”‚
â”‚  Kreuzvalidierung:                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ batch_sizeâ”‚num_histâ”‚ GeschÃ¤tzt (MiB)      â”‚ Gemessen â”‚ Quelle       â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚     4     â”‚   6    â”‚ 16 467 (= Referenz)  â”‚ 16 467   â”‚ Epoch 1 Log  â”‚     â”‚
â”‚  â”‚     8     â”‚   3-4  â”‚ ~14 598 (59.4%)      â”‚ ~60%     â”‚ train.yaml   â”‚     â”‚
â”‚  â”‚    16     â”‚   3-4  â”‚ >24 564 (OOM)        â”‚ OOM!     â”‚ train.yaml   â”‚     â”‚
â”‚  â”‚    32     â”‚   3-4  â”‚ >24 564 (OOM)        â”‚ OOM!     â”‚ train.yaml   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â”‚  âœ… SchÃ¤tzung "59.4%" passt zum Kommentar "~60%" in train.yaml              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.7.3 Validierungs-Lastspitze (Val Peak)

**Kritischer Befund:** Die Validierungsphase verbraucht **mehr VRAM** als das Training!

Drei Ursachen im Code (`train.py`, Methode `val()`):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROBLEM 1: openloop_rollout() vor dem Val-Loop                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  â†’ model.rollout() baut z-Tensor iterativ auf via torch.cat                â”‚
â”‚  â†’ decode_obs() erzeugt Bilder auf der GPU                                 â”‚
â”‚  â†’ CUDA-Allocator hÃ¤lt freigegebene BlÃ¶cke als fragmentierten Cache        â”‚
â”‚                                                                             â”‚
â”‚  PROBLEM 2: Val Forward Pass OHNE torch.no_grad()                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  for batch in valid_dataloader:                                             â”‚
â”‚      model(obs, act)      â† baut vollen Computation Graph!                â”‚
â”‚      encode_obs(obs)      â† ZUSÃ„TZLICHER Encoder-Pass (fÃ¼r Plots)          â”‚
â”‚                                                                             â”‚
â”‚  â†’ Identische Activation-Kosten wie Training Forward Pass                  â”‚
â”‚  â†’ Autograd-Graph wird gebaut, obwohl backward() nie aufgerufen wird       â”‚
â”‚  â†’ Verschwendet VRAM durch gespeicherte Zwischenergebnisse                 â”‚
â”‚                                                                             â”‚
â”‚  PROBLEM 3: Kein torch.cuda.empty_cache() zwischen Rollout und Val-Loop    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  â†’ Fragmentierte BlÃ¶cke vom Rollout + neue Val-Allokationen                â”‚
â”‚  â†’ CUDA-Allocator findet keine zusammenhÃ¤ngenden BlÃ¶cke                    â”‚
â”‚  â†’ ~12% zusÃ¤tzlicher Overhead durch Fragmentierung                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Formel: Validierungs-Lastspitze**

$$\text{VRAM}_{\text{Val Peak}} = \bigl(\underbrace{F}_{\text{Feste Kosten}} + \underbrace{C}_{\text{CUDA}} + \underbrace{A_{\text{val}}}_{\substack{\text{Val Activations}\\\text{(= Train Fwd)}}} + \underbrace{R}_{\text{Rollout-Residuen}} + \underbrace{P}_{\text{Extra Plot}}\bigr) \times \underbrace{1.12}_{\text{Fragmentierung}}$$

**Beispielrechnung fÃ¼r aktuelle Konfiguration (bs=4, nh=6, fs=2, T=22):**

| Komponente | MiB | ErklÃ¤rung |
|------------|-----|-----------|
| Training VRAM | 16 467 | Gemessener Wert |
| + Rollout-Residuen | ~73 | z-Tensor + Decode-Bilder + Fragmentierung |
| + Extra Plot-Decode | ~151 | encode_obs + eval_images beim 1. Batch |
| Ã— Fragmentierung (1.12) | | CUDA-Allocator Overhead |
| **= Val Peak** | **~19 239** | **78.3% von 24 564 MiB** |
| Overhead vs. Training | **+16.8%** | |

#### 3.7.4 Maximales num_hist nach VRAM (inkl. Val Peak)

**Szenario: T=22, frameskip=2, batch_size=4 (aktuelle Konfiguration)**

| num_hist | Train VRAM | Train % | Val Peak | Val Peak % | Status |
|----------|-----------|---------|----------|------------|--------|
| 3 | 6 654 MiB | 27.1% | 7 783 MiB | 31.7% | âœ… Sicher |
| 4 | 8 699 MiB | 35.4% | 10 182 MiB | 41.5% | âœ… Sicher |
| 5 | 12 101 MiB | 49.3% | 14 157 MiB | 57.6% | âœ… Sicher |
| 6 | 16 467 MiB | 67.0% | 19 239 MiB | 78.3% | âœ… Aktuell |
| **7** | **21 797 MiB** | **88.7%** | **25 479 MiB** | **103.7%** | **âš ï¸ Val OOM!** |
| 8 | 28 092 MiB | 114.4% | â€“ | â€“ | âŒ Train OOM |

> **Ergebnis:** Bei `batch_size=4, frameskip=2` ist `num_hist=6` das Maximum,
> das sowohl Training als auch Validierung ohne OOM Ã¼bersteht.
> `num_hist=7` wÃ¼rde im Training noch passen (88.7%), aber die
> Validierung sprengt den VRAM (103.7%)!

#### 3.7.5 Optimale Konfigurationen (Solver-Ergebnisse)

Der Optimierungssolver (`hyperparameter_analysis.py`) maximiert `num_hist` unter
der harten Grenze `Val Peak â‰¤ 90% Ã— 24 564 MiB`:

**Szenarien (T=22, E=500):**

| Rang | Config (bs/nh/fs) | Train VRAM | Val Peak | Val Peak % | Slices/Ep | Score |
|------|-------------------|-----------|----------|------------|-----------|-------|
| 1 | bs=4, nh=7, fs=2 | 21 797 | ~25 479 | ~103.7% | 8 | âš ï¸ Val OOM |
| **2** | **bs=4, nh=6, fs=2** | **16 467** | **19 239** | **78.3%** | **9** | **GewÃ¤hlt âœ…** |
| 3 | bs=2, nh=7, fs=2 | ~11 266 | ~13 182 | ~53.7% | 8 | Machbar |
| 4 | bs=1, nh=8, fs=2 | ~7 621 | ~8 917 | ~36.3% | 7 | Machbar |

> **BegrÃ¼ndung fÃ¼r bs=4, nh=6, fs=2:**
> - Maximales `num_hist` bei `batch_size â‰¥ 4` (stabile GradientenschÃ¤tzung)
> - 9 Slices/Episode â†’ gute Dateneffizienz
> - Val Peak bei 78.3% â†’ ausreichend Headroom
> - Korrespondiert mit Paper-Empfehlung: Zhou et al. nutzen `batch_size=32`
>   auf A6000 (48 GB), skaliert linear: 32 Ã— (24564/49152) â‰ˆ 16 â†’ unser bs=4
>   mit hÃ¶herem nh kompensiert durch kleineren bs

#### 3.7.6 Vergleich mit Paper (Zhou et al. 2025)

| Parameter | Paper (PushT/PointMaze) | Unsere Konfig. (Franka) | BegrÃ¼ndung |
|-----------|------------------------|------------------------|------------|
| GPU | A6000 (48 GB) | A5000 (24.5 GB) | ~50% VRAM |
| batch_size | 32 | 4 | VRAM-limitiert |
| num_hist | 1â€“3 | 6 | Maximiert (PrioritÃ¤t 1) |
| frameskip | 1â€“5 | 2 | Franka: langsame Dynamik |
| Epochen | 100 | 100 | Identisch |
| num_pred | 1 | 1 | Identisch |

#### 3.7.7 Generierte Analyse-Plots

Alle Plots befinden sich in `hyperparameter_analysis/` und wurden mit
`hyperparameter_analysis.py` erzeugt (PDF + PNG):

| # | Datei | Inhalt |
|---|-------|--------|
| 01 | `01_feasibility_heatmap_T{22,25}.pdf` | Machbarkeitskarte: num_hist Ã— batch_size (grÃ¼n/gelb/rot) |
| 02 | `02_vram_vs_batch_numhist_T22.pdf` | VRAM-Kurven: Train + Val Peak vs. batch_size pro num_hist |
| 03 | `03_samples_efficiency_T{22,25}.pdf` | Slices/Ep + Steps/Ep Ã¼ber num_hist Ã— frameskip |
| 04 | `04_optimal_frontier_T{22,25}.pdf` | Pareto-Front: Score vs. Val Peak pro Konfiguration |
| 05 | `05_vram_breakdown_T22.pdf` | Gestapeltes Balkendiagramm: Weights, Optimizer, Activations, Val-Overhead |
| 06 | `06_attention_scaling.pdf` | Quadratische Attention-Skalierung Ã¼ber seq_len |
| 07 | `07_paper_comparison.pdf` | Unsere Konfig vs. Paper-Referenz (skaliert auf A5000) |
| 08 | `08_sweep_table_T{22,25}.pdf` | VollstÃ¤ndige Sweep-Tabelle mit Status-Codes |
| 09 | `09_validation_peak_T{22,25}.pdf` | Validierungs-Lastspitze: Training vs. Val Peak + Zerlegung |

#### 3.7.8 Potentielle Code-Verbesserungen (train.py)

Die folgenden Ã„nderungen wÃ¼rden die Validierungs-Lastspitze um ~12â€“15% senken:

```python
# FIX 1: torch.no_grad() um den Validation Loop
# Aktuell (train.py, val()):
for i, batch in enumerate(valid_dataloader):
    out = self.model(obs, act)         # â† baut Computation Graph!

# Verbesserung:
with torch.no_grad():                  # â† spart ~gleiche Activations wie Forward
    for i, batch in enumerate(valid_dataloader):
        out = self.model(obs, act)     # â† kein Graph, nur Inference

# FIX 2: torch.cuda.empty_cache() zwischen Rollout und Val Loop
# Nach openloop_rollout und vor dem Val Loop einfÃ¼gen:
torch.cuda.empty_cache()              # â† rÃ¤umt CUDA-Allocator auf
```

> **Hinweis:** Diese Fixes wurden NICHT angewendet, um die Reproduzierbarkeit
> gegenÃ¼ber dem Originalcode (Zhou et al. 2025) zu wahren. Die VRAM-Analyse
> berÃ¼cksichtigt diese Overhead-Quellen in der Parameterwahl.

---

## 4. Training-Pipeline (Chronologisch)

### Phase 1: Initialisierung

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 1: Konfiguration laden                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  python train.py env=franka_cube_stack                                      â”‚
â”‚                                                                             â”‚
â”‚  â†’ Hydra lÃ¤dt: conf/train.yaml + conf/env/franka_cube_stack.yaml           â”‚
â”‚  â†’ Parameter werden zusammengefÃ¼hrt                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 2: Trainer-Objekt erstellen                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  class Trainer:                                                             â”‚
â”‚      def __init__(self, cfg):                                               â”‚
â”‚          self.cfg = cfg                                                     â”‚
â”‚          self.accelerator = Accelerator(log_with="wandb")                   â”‚
â”‚          self.device = self.accelerator.device  # GPU                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Datensatz laden und vorbereiten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 3: FrankaCubeStackDataset laden                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  class FrankaCubeStackDataset:                                              â”‚
â”‚      def __init__(self, data_path, ...):                                    â”‚
â”‚          # 1. States und Actions laden                                      â”‚
â”‚          self.states = torch.load("states.pth")    # (10, 932, 22)         â”‚
â”‚          self.actions = torch.load("actions.pth")  # (10, 932, 9)          â”‚
â”‚                                                                             â”‚
â”‚          # 2. Episoden-LÃ¤ngen aus metadata.pkl                              â”‚
â”‚          self.seq_lengths = [932, 932, ..., 932]   # 10 Ã— 932              â”‚
â”‚                                                                             â”‚
â”‚          # 3. Proprio extrahieren (EE-Position)                             â”‚
â”‚          self.proprios = self.states[..., :3]      # (10, 932, 3)          â”‚
â”‚                                                                             â”‚
â”‚          # 4. Z-Normalisierung (wenn normalize_action=True)                 â”‚
â”‚          self.actions = (self.actions - mean) / std                         â”‚
â”‚          self.proprios = (self.proprios - mean) / std                       â”‚
â”‚                                                                             â”‚
â”‚          # 5. Alle Bilder in RAM laden (preload_images=True)                â”‚
â”‚          self.images_cache = [                                              â”‚
â”‚              torch.load("000000/obses.pth"),  # (932, 256, 256, 3)         â”‚
â”‚              torch.load("000001/obses.pth"),                                â”‚
â”‚              ...                                                            â”‚
â”‚          ]                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 4: Train/Validation Split                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Mit split_ratio=0.9 und 10 Episoden:                                       â”‚
â”‚  - Training: 9 Episoden (zufÃ¤llig ausgewÃ¤hlt)                               â”‚
â”‚  - Validation: 1 Episode                                                    â”‚
â”‚                                                                             â”‚
â”‚  split_traj_datasets(dataset, train_fraction=0.9)                           â”‚
â”‚  â†’ train_set = TrajSubset(dataset, [0,1,2,3,4,5,6,7,8])  # Beispiel        â”‚
â”‚  â†’ val_set = TrajSubset(dataset, [9])                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 5: TrajSlicerDataset erstellen                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Parameter:                                                                 â”‚
â”‚  - num_frames = num_hist + num_pred = 3 + 1 = 4                            â”‚
â”‚  - frameskip = 5                                                            â”‚
â”‚                                                                             â”‚
â”‚  FÃ¼r jede Episode (T=932 Frames):                                           â”‚
â”‚  - BenÃ¶tigte Frames pro Sample: 4 Ã— 5 = 20                                 â”‚
â”‚  - MÃ¶gliche Start-Positionen: 932 - 20 + 1 = 913                           â”‚
â”‚                                                                             â”‚
â”‚  Slices pro Episode:                                                        â”‚
â”‚  [                                                                          â”‚
â”‚    (episode_idx, 0, 20),    # Frames 0,5,10,15                              â”‚
â”‚    (episode_idx, 1, 21),    # Frames 1,6,11,16                              â”‚
â”‚    (episode_idx, 2, 22),    # Frames 2,7,12,17                              â”‚
â”‚    ...                                                                      â”‚
â”‚    (episode_idx, 912, 932), # Frames 912,917,922,927                        â”‚
â”‚  ]                                                                          â”‚
â”‚                                                                             â”‚
â”‚  GESAMT: 9 Episoden Ã— 913 Slices = ~8.217 Training-Samples                 â”‚
â”‚          1 Episode Ã— 913 Slices = ~913 Validation-Samples                   â”‚
â”‚                                                                             â”‚
â”‚  â†’ Slices werden zufÃ¤llig gemischt (shuffle)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 3: Modelle initialisieren

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 6: DINO v2 Encoder laden                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  self.encoder = DinoV2Encoder(                                              â”‚
â”‚      name="dinov2_vits14",          # ViT-Small, Patch 14                   â”‚
â”‚      feature_key="x_norm_patchtokens"                                       â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  # LÃ¤dt vortrainiertes Modell von Facebook                                  â”‚
â”‚  torch.hub.load("facebookresearch/dinov2", "dinov2_vits14")                 â”‚
â”‚                                                                             â”‚
â”‚  Eigenschaften:                                                             â”‚
â”‚  - emb_dim: 384 (Feature-Dimension)                                         â”‚
â”‚  - patch_size: 14 (jeder 14Ã—14 Pixel-Block = 1 Token)                      â”‚
â”‚  - latent_ndim: 2 (Patches sind 2D angeordnet)                              â”‚
â”‚                                                                             â”‚
â”‚  FÃ¼r 224Ã—224 Bilder:                                                        â”‚
â”‚  - num_patches = (224/14)Â² = 16Â² = 256 Patches                              â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: train_encoder=False â†’ Parameter sind eingefroren!                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 7: Action & Proprio Encoder laden                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  # Action Encoder                                                           â”‚
â”‚  self.action_encoder = ProprioceptiveEmbedding(                             â”‚
â”‚      in_chans=45,      # action_dim Ã— frameskip = 9 Ã— 5                    â”‚
â”‚      emb_dim=10        # action_emb_dim aus config                          â”‚
â”‚  )                                                                          â”‚
â”‚  # Verwendet 1D Convolution: Conv1d(45 â†’ 10)                                â”‚
â”‚                                                                             â”‚
â”‚  # Proprio Encoder                                                          â”‚
â”‚  self.proprio_encoder = ProprioceptiveEmbedding(                            â”‚
â”‚      in_chans=3,       # proprio_dim (EE-Position x,y,z)                    â”‚
â”‚      emb_dim=10        # proprio_emb_dim aus config                         â”‚
â”‚  )                                                                          â”‚
â”‚  # Verwendet 1D Convolution: Conv1d(3 â†’ 10)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 8: ViT Predictor laden                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  self.predictor = ViTPredictor(                                             â”‚
â”‚      num_patches=198,  # 196 visual + 2 (proprio + action bei concat_dim=0) â”‚
â”‚                        # oder 196 bei concat_dim=1                          â”‚
â”‚      num_frames=3,     # num_hist                                           â”‚
â”‚      dim=404,          # 384 (DINO) + 10 (action) + 10 (proprio)           â”‚
â”‚      depth=6,          # 6 Transformer-BlÃ¶cke                               â”‚
â”‚      heads=16,         # 16 Attention-Heads                                 â”‚
â”‚      mlp_dim=2048,     # Feed-Forward Dimension                             â”‚
â”‚      dropout=0.1                                                            â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  # Verwendet KAUSALE ATTENTION MASK                                         â”‚
â”‚  # â†’ Kann nur vergangene Frames sehen, nicht zukÃ¼nftige                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 9: VQ-VAE Decoder laden                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  self.decoder = VQVAE(                                                      â”‚
â”‚      channel=384,       # Entspricht DINO emb_dim                           â”‚
â”‚      n_embed=2048,      # Codebook-GrÃ¶ÃŸe (nicht verwendet wenn quantize=F)  â”‚
â”‚      n_res_block=4,     # Residual Blocks                                   â”‚
â”‚      n_res_channel=128,                                                     â”‚
â”‚      quantize=False     # KEINE Quantisierung (kontinuierlicher Latent)    â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  # Architektur:                                                             â”‚
â”‚  # Latent (14Ã—14Ã—384) â†’ Upsample (4Ã—) â†’ 56Ã—56 â†’ Upsample (4Ã—) â†’ 224Ã—224Ã—3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 10: VWorldModel zusammensetzen                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  self.model = VWorldModel(                                                  â”‚
â”‚      encoder=self.encoder,                                                  â”‚
â”‚      proprio_encoder=self.proprio_encoder,                                  â”‚
â”‚      action_encoder=self.action_encoder,                                    â”‚
â”‚      predictor=self.predictor,                                              â”‚
â”‚      decoder=self.decoder,                                                  â”‚
â”‚      num_hist=3,                                                            â”‚
â”‚      num_pred=1,                                                            â”‚
â”‚      concat_dim=1  # Embeddings werden entlang Feature-Dimension konkateniertâ”‚
â”‚  )                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 4: Training-Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 11: Optimizer initialisieren                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  self.encoder_optimizer = Adam(encoder.parameters(), lr=1e-6)               â”‚
â”‚  self.predictor_optimizer = AdamW(predictor.parameters(), lr=5e-4)          â”‚
â”‚  self.decoder_optimizer = Adam(decoder.parameters(), lr=3e-4)               â”‚
â”‚  self.action_encoder_optimizer = AdamW(                                     â”‚
â”‚      [action_encoder.params, proprio_encoder.params], lr=5e-4               â”‚
â”‚  )                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 12: Training-Epoch                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  for epoch in range(1, 101):  # 100 Epochen                                â”‚
â”‚      for batch in dataloader:                                               â”‚
â”‚          obs, act, state = batch                                            â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â”‚  obs['visual']: (B, 4, 3, 224, 224) - 4 Bilder                  â”‚
â”‚          â”‚  obs['proprio']: (B, 4, 3) - 4 EE-Positionen                    â”‚
â”‚          â”‚  act: (B, 4, 45) - 4 Ã— (9Ã—5) konkatenierte Aktionen             â”‚
â”‚          â”‚  state: (B, 4, 22) - 4 vollstÃ¤ndige States                      â”‚
â”‚          â–¼                                                                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚      â”‚  FORWARD PASS (siehe nÃ¤chstes Diagramm)                         â”‚   â”‚
â”‚      â”‚  z_pred, visual_pred, visual_recon, loss = model(obs, act)      â”‚   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â–¼                                                                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚      â”‚  BACKWARD PASS                                                   â”‚   â”‚
â”‚      â”‚  1. encoder_optimizer.zero_grad()                               â”‚   â”‚
â”‚      â”‚  2. predictor_optimizer.zero_grad()                             â”‚   â”‚
â”‚      â”‚  3. decoder_optimizer.zero_grad()                               â”‚   â”‚
â”‚      â”‚  4. action_encoder_optimizer.zero_grad()                        â”‚   â”‚
â”‚      â”‚                                                                  â”‚   â”‚
â”‚      â”‚  accelerator.backward(loss)  # Gradient berechnen               â”‚   â”‚
â”‚      â”‚                                                                  â”‚   â”‚
â”‚      â”‚  # NUR trainierbare Komponenten updaten:                        â”‚   â”‚
â”‚      â”‚  predictor_optimizer.step()      # âœ“ train_predictor=True       â”‚   â”‚
â”‚      â”‚  decoder_optimizer.step()         # âœ“ train_decoder=True        â”‚   â”‚
â”‚      â”‚  action_encoder_optimizer.step()  # âœ“ immer                     â”‚   â”‚
â”‚      â”‚  # encoder_optimizer.step()      # âœ— train_encoder=False       â”‚   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 5: Forward Pass im Detail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 13: Forward Pass - Encoding                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  INPUT:                                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                                    â”‚
â”‚  obs['visual']: (B, 4, 3, 224, 224)   # 4 RGB Bilder                       â”‚
â”‚  obs['proprio']: (B, 4, 3)            # 4 EE-Positionen (x,y,z)            â”‚
â”‚  act: (B, 4, 45)                      # 4 konkatenierte Aktionen           â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  VISUAL ENCODING (DINO v2):                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  1. Bilder reshapen: (B, 4, 3, 224, 224) â†’ (BÃ—4, 3, 224, 224)              â”‚
â”‚  2. Optional resize auf encoder_image_size (fÃ¼r DINO patch alignment)       â”‚
â”‚  3. DINO forward: (BÃ—4, 3, 224, 224) â†’ (BÃ—4, 256, 384)                     â”‚
â”‚                   [batchÃ—time, num_patches, emb_dim]                        â”‚
â”‚  4. Reshape zurÃ¼ck: (BÃ—4, 256, 384) â†’ (B, 4, 256, 384)                     â”‚
â”‚                                                                             â”‚
â”‚  z_visual: (B, 4, 256, 384)                                                 â”‚
â”‚            â†‘    â†‘    â†‘                                                      â”‚
â”‚         batch time patches                                                  â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  PROPRIO ENCODING:                                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  proprio_encoder(obs['proprio'])                                            â”‚
â”‚  (B, 4, 3) â†’ Conv1d â†’ (B, 4, 10)                                           â”‚
â”‚                                                                             â”‚
â”‚  z_proprio: (B, 4, 10)                                                      â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  ACTION ENCODING:                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  action_encoder(act)                                                        â”‚
â”‚  (B, 4, 45) â†’ Conv1d â†’ (B, 4, 10)                                          â”‚
â”‚                                                                             â”‚
â”‚  z_action: (B, 4, 10)                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 14: Forward Pass - Concatenation (concat_dim=1)                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Mit concat_dim=1: Embeddings werden entlang Feature-Dimension konkateniert â”‚
â”‚                                                                             â”‚
â”‚  z_visual:  (B, 4, 256, 384)  # 256 Patches Ã— 384 dim                      â”‚
â”‚  z_proprio: (B, 4, 10)        # â†’ tile auf (B, 4, 256, 10)                 â”‚
â”‚  z_action:  (B, 4, 10)        # â†’ tile auf (B, 4, 256, 10)                 â”‚
â”‚                                                                             â”‚
â”‚  Konkatenation:                                                             â”‚
â”‚  z = concat([z_visual, z_proprio_tiled, z_action_tiled], dim=-1)           â”‚
â”‚                                                                             â”‚
â”‚  z: (B, 4, 256, 384+10+10) = (B, 4, 256, 404)                              â”‚
â”‚                                                                             â”‚
â”‚  Visualisierung eines Patches:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ DINO Features (384) â”‚ Proprio (10) â”‚ Action (10) â”‚ = 404 dim â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 15: Forward Pass - Prediction                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Source (Input fÃ¼r Predictor):                                              â”‚
â”‚  z_src = z[:, :num_hist]   = z[:, :3]   # Erste 3 Zeitschritte             â”‚
â”‚  z_src: (B, 3, 256, 404)                                                    â”‚
â”‚                                                                             â”‚
â”‚  Target (Ground Truth):                                                     â”‚
â”‚  z_tgt = z[:, num_pred:]   = z[:, 1:]   # Letzte 3 Zeitschritte            â”‚
â”‚  z_tgt: (B, 3, 256, 404)   # Zeitlich um 1 verschoben                      â”‚
â”‚                                                                             â”‚
â”‚  ViT Predictor:                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚  1. Reshape: (B, 3, 256, 404) â†’ (B, 768, 404)  # 3Ã—256 = 768 Tokens        â”‚
â”‚  2. Position Embedding addieren                                             â”‚
â”‚  3. 6Ã— Transformer Blocks mit KAUSALER MASKE                               â”‚
â”‚  4. Output: (B, 768, 404) â†’ (B, 3, 256, 404)                               â”‚
â”‚                                                                             â”‚
â”‚  z_pred: (B, 3, 256, 404)                                                   â”‚
â”‚                                                                             â”‚
â”‚  Kausale Maske Visualisierung:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚     Frame 0   Frame 1   Frame 2         â”‚                               â”‚
â”‚  â”‚  F0   âœ“         âœ—         âœ—             â”‚  â† Kann nur sich selbst sehen â”‚
â”‚  â”‚  F1   âœ“         âœ“         âœ—             â”‚  â† Kann F0 und sich sehen     â”‚
â”‚  â”‚  F2   âœ“         âœ“         âœ“             â”‚  â† Kann alle sehen            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 16: Forward Pass - Decoding                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Separate Embeddings:                                                       â”‚
â”‚  z_pred: (B, 3, 256, 404)                                                   â”‚
â”‚      â†“                                                                      â”‚
â”‚  z_visual_pred: (B, 3, 256, 384)  # Nur DINO Features                      â”‚
â”‚  z_proprio_pred: (B, 3, 10)        # Proprio (nicht decodiert)             â”‚
â”‚  z_action_pred: (B, 3, 10)         # Action (nicht decodiert)              â”‚
â”‚                                                                             â”‚
â”‚  VQ-VAE Decoder:                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  1. Reshape: (B, 3, 256, 384) â†’ (BÃ—3, 16, 16, 384)  # âˆš256 = 16           â”‚
â”‚  2. Permute: (BÃ—3, 16, 16, 384) â†’ (BÃ—3, 384, 16, 16)                       â”‚
â”‚  3. Upsample 4Ã—: (BÃ—3, 384, 16, 16) â†’ (BÃ—3, 384, 64, 64)                   â”‚
â”‚  4. Decode: (BÃ—3, 384, 64, 64) â†’ (BÃ—3, 3, 224, 224)                        â”‚
â”‚  5. Reshape: (BÃ—3, 3, 224, 224) â†’ (B, 3, 3, 224, 224)                      â”‚
â”‚                                                                             â”‚
â”‚  visual_pred: (B, 3, 3, 224, 224)  # Vorhersagte Bilder                    â”‚
â”‚                                                                             â”‚
â”‚  ZusÃ¤tzlich: Rekonstruktion der Originalen                                  â”‚
â”‚  visual_recon: (B, 4, 3, 224, 224)  # Alle 4 Frames rekonstruiert          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 6: Loss-Berechnung

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 17: Loss-Berechnung                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LATENT SPACE LOSS (z_loss)                                          â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚   â”‚
â”‚  â”‚  z_pred: (B, 3, 256, 404) - Vorhersage                              â”‚   â”‚
â”‚  â”‚  z_tgt:  (B, 3, 256, 404) - Ground Truth (detached)                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  z_visual_loss = MSE(z_pred[..., :384], z_tgt[..., :384])           â”‚   â”‚
â”‚  â”‚  z_proprio_loss = MSE(z_pred[..., 384:394], z_tgt[..., 384:394])    â”‚   â”‚
â”‚  â”‚  z_loss = MSE(z_pred[..., :394], z_tgt[..., :394])  # ohne Action   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Gewichtung: 1.0                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    +                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DECODER LOSS (Reconstruction)                                       â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚   â”‚
â”‚  â”‚  visual_recon: (B, 4, 3, 224, 224) - Rekonstruierte Bilder          â”‚   â”‚
â”‚  â”‚  obs['visual']: (B, 4, 3, 224, 224) - Original Bilder               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  recon_loss = MSE(visual_recon, obs['visual'])                      â”‚   â”‚
â”‚  â”‚  vq_loss = 0 (da quantize=False)                                    â”‚   â”‚
â”‚  â”‚  decoder_loss = recon_loss + 0.25 Ã— vq_loss                         â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Gewichtung: 1.0                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    +                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DECODER PREDICTION LOSS (Optional)                                  â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚   â”‚
â”‚  â”‚  visual_pred: (B, 3, 3, 224, 224) - Vorhersagte Bilder              â”‚   â”‚
â”‚  â”‚  obs['visual'][:, 1:]: (B, 3, 3, 224, 224) - Ground Truth           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  pred_recon_loss = MSE(visual_pred, obs['visual'][:, 1:])           â”‚   â”‚
â”‚  â”‚  (Dieser Loss wird geloggt aber nicht zum Training verwendet)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    =                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TOTAL LOSS                                                          â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚   â”‚
â”‚  â”‚  loss = z_loss + decoder_loss                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 7: Validation und Logging

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 18: Validation                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  model.eval()  # Keine Gradientenberechnung                                â”‚
â”‚                                                                             â”‚
â”‚  1. Standard Validation (wie Training, aber ohne Optimizer-Steps)          â”‚
â”‚  2. Open-Loop Rollout:                                                      â”‚
â”‚     - Nimm erste num_hist Frames                                           â”‚
â”‚     - Sage iterativ zukÃ¼nftige Frames vorher                               â”‚
â”‚     - Vergleiche mit Ground Truth                                          â”‚
â”‚                                                                             â”‚
â”‚  Rollout-Visualisierung:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  t=0   t=1   t=2   t=3   t=4   t=5   ...                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  GT:   [F0]  [F1]  [F2]  [F3]  [F4]  [F5]  ...                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Pred: [F0]  [F1]  [F2]  [P3]  [P4]  [P5]  ...                      â”‚   â”‚
â”‚  â”‚         â†‘     â†‘     â†‘     â†‘                                         â”‚   â”‚
â”‚  â”‚       Input Input Input Vorhersage (autoregressiv)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 19: Logging zu Weights & Biases                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Geloggte Metriken:                                                         â”‚
â”‚  - train_loss, val_loss                                                     â”‚
â”‚  - train_z_loss, val_z_loss                                                 â”‚
â”‚  - train_z_visual_loss, val_z_visual_loss                                   â”‚
â”‚  - train_z_proprio_loss, val_z_proprio_loss                                 â”‚
â”‚  - train_decoder_recon_loss, val_decoder_recon_loss                         â”‚
â”‚  - z_visual_err_rollout, z_proprio_err_rollout                              â”‚
â”‚  - Image Quality Metrics (PSNR, SSIM, etc.)                                 â”‚
â”‚                                                                             â”‚
â”‚  Visualisierungen:                                                          â”‚
â”‚  - Rekonstruierte Bilder vs. Ground Truth                                   â”‚
â”‚  - Vorhersage-Sequenzen                                                     â”‚
â”‚  - Rollout-Plots                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT 20: Checkpoint speichern                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  if epoch % save_every_x_epoch == 0:                                        â”‚
â”‚      torch.save({                                                           â”‚
â”‚          'epoch': epoch,                                                    â”‚
â”‚          'encoder': encoder.state_dict(),         # DINO Weights           â”‚
â”‚          'predictor': predictor.state_dict(),     # ViT Weights            â”‚
â”‚          'decoder': decoder.state_dict(),         # VQ-VAE Weights         â”‚
â”‚          'action_encoder': action_encoder.state_dict(),                     â”‚
â”‚          'proprio_encoder': proprio_encoder.state_dict(),                   â”‚
â”‚          'encoder_optimizer': ...,                                          â”‚
â”‚          'predictor_optimizer': ...,                                        â”‚
â”‚          'decoder_optimizer': ...,                                          â”‚
â”‚      }, f"checkpoints/model_{epoch}.pth")                                   â”‚
â”‚                                                                             â”‚
â”‚  Gespeichert in: outputs/DATUM/ZEIT/checkpoints/                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Modell-Architektur

### 5.1 DINO v2 Encoder

**Was ist DINO?**
DINO (Self-**DI**stillation with **NO** labels) ist ein selbstÃ¼berwachtes Vision-Modell von Meta/Facebook, das ohne Labels trainiert wurde.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DINO v2 ViT-Small/14                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: RGB Bild (3, 224, 224)                                              â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Patch Embedding                                                     â”‚   â”‚
â”‚  â”‚  - Bild in 14Ã—14 Patches aufteilen                                  â”‚   â”‚
â”‚  â”‚  - 224/14 = 16 Patches pro Seite                                    â”‚   â”‚
â”‚  â”‚  - 16 Ã— 16 = 256 Patches total                                      â”‚   â”‚
â”‚  â”‚  - Jeder Patch: 14Ã—14Ã—3 = 588 Pixel â†’ Linear â†’ 384 dim              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Transformer Encoder (12 BlÃ¶cke)                                     â”‚   â”‚
â”‚  â”‚  - Self-Attention Ã¼ber alle 256 Patches                             â”‚   â”‚
â”‚  â”‚  - Lernt rÃ¤umliche Beziehungen                                      â”‚   â”‚
â”‚  â”‚  - Vortrainiert auf ImageNet (ohne Labels!)                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: Patch Tokens (256, 384)                                            â”‚
â”‚          [num_patches, emb_dim]                                             â”‚
â”‚                                                                             â”‚
â”‚  Eigenschaften:                                                             â”‚
â”‚  - Vortrainiert: Parameter werden NICHT verÃ¤ndert                          â”‚
â”‚  - Semantic Features: Lernt bedeutungsvolle visuelle ReprÃ¤sentationen      â”‚
â”‚  - Patch-basiert: ErhÃ¤lt rÃ¤umliche Information                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Warum DINO?**
- Starke, generalisierbare visuelle Features
- Funktioniert gut auf Robotik-DomÃ¤ne (obwohl auf natÃ¼rliche Bilder trainiert)
- Keine zusÃ¤tzlichen Labels nÃ¶tig

### 5.2 Action & Proprio Encoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Proprioceptive Embedding (MLP)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Action Encoder:                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  Input: (B, T, 45)     # 45 = 9 actions Ã— 5 frameskip                      â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Conv1d(45 â†’ 10, kernel=1, stride=1)                                       â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: (B, T, 10)    # Komprimierte Action-ReprÃ¤sentation                â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  Proprio Encoder:                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  Input: (B, T, 3)      # EE-Position (x, y, z)                             â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Conv1d(3 â†’ 10, kernel=1, stride=1)                                        â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: (B, T, 10)    # Komprimierte Proprio-ReprÃ¤sentation               â”‚
â”‚                                                                             â”‚
â”‚  Zweck:                                                                     â”‚
â”‚  - Dimensionsreduktion fÃ¼r effiziente Verarbeitung                         â”‚
â”‚  - Lernt relevante Features aus rohen Sensor-Daten                         â”‚
â”‚  - Wird MIT trainiert (im Gegensatz zu DINO)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 ViT Predictor

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Vision Transformer Predictor                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: z (B, 3, 256, 404)  # 3 Frames Ã— 256 Patches Ã— 404 dim             â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (B, 768, 404)     # 3Ã—256 = 768 Tokens                           â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Positional Embedding                                                â”‚   â”‚
â”‚  â”‚  - Lernbares Embedding: (1, 768, 404)                               â”‚   â”‚
â”‚  â”‚  - Addiert zu Input                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  6Ã— Transformer Block                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Multi-Head Attention (16 Heads)                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - KAUSAL: Kann nur vergangene Tokens sehen                   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Query, Key, Value: Linear(404 â†’ 64Ã—16)                     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Output: Linear(64Ã—16 â†’ 404)                                â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚            â”‚ + Residual                                              â”‚   â”‚
â”‚  â”‚            â–¼                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Feed-Forward Network                                         â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Linear(404 â†’ 2048) â†’ GELU â†’ Linear(2048 â†’ 404)              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚            â”‚ + Residual                                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  LayerNorm                                                                  â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (B, 768, 404) â†’ (B, 3, 256, 404)                                 â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: z_pred (B, 3, 256, 404)                                            â”‚
â”‚                                                                             â”‚
â”‚  Kausale Maske (Visualisierung fÃ¼r 3 Frames Ã— 2 Patches):                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚        P0_F0  P1_F0  P0_F1  P1_F1  P0_F2  P1_F2    â”‚                   â”‚
â”‚  â”‚ P0_F0    1      1      0      0      0      0      â”‚                   â”‚
â”‚  â”‚ P1_F0    1      1      0      0      0      0      â”‚                   â”‚
â”‚  â”‚ P0_F1    1      1      1      1      0      0      â”‚                   â”‚
â”‚  â”‚ P1_F1    1      1      1      1      0      0      â”‚                   â”‚
â”‚  â”‚ P0_F2    1      1      1      1      1      1      â”‚                   â”‚
â”‚  â”‚ P1_F2    1      1      1      1      1      1      â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  (1 = kann sehen, 0 = maskiert)                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 VQ-VAE Decoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          VQ-VAE Decoder                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: z_visual (B, T, 256, 384)   # Nur visuelle Features                â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (BÃ—T, 16, 16, 384)        # âˆš256 = 16                            â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Quantize (DEAKTIVIERT: quantize=False)                             â”‚   â”‚
â”‚  â”‚  - Bei aktiviert: Diskretisierung in Codebook                       â”‚   â”‚
â”‚  â”‚  - Hier: Kontinuierlicher Latent-Space                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Permute: (BÃ—T, 384, 16, 16)        # Channel-first fÃ¼r Conv              â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Upsample Block (4Ã—)                                                 â”‚   â”‚
â”‚  â”‚  Conv2d(384, 384) + 4Ã— ResBlock + ConvTranspose2d(stride=2) Ã—2      â”‚   â”‚
â”‚  â”‚  (16, 16) â†’ (32, 32) â†’ (64, 64)                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  (BÃ—T, 384, 64, 64)                                                         â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Decode Block (4Ã—)                                                   â”‚   â”‚
â”‚  â”‚  Conv2d(384, 384) + 4Ã— ResBlock + ConvTranspose2d(stride=2) Ã—2      â”‚   â”‚
â”‚  â”‚  (64, 64) â†’ (128, 128) â†’ (256, 256) â†’ Resize â†’ (224, 224)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Output: (BÃ—T, 3, 224, 224)         # RGB Bilder                           â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  Reshape: (B, T, 3, 224, 224)                                               â”‚
â”‚                                                                             â”‚
â”‚  ResBlock Architektur:                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚   â”‚
â”‚  â”‚    â”‚                                        â”‚                       â”‚   â”‚
â”‚  â”‚    â–¼                                        â”‚                       â”‚   â”‚
â”‚  â”‚  ReLU â†’ Conv3Ã—3 â†’ ReLU â†’ Conv1Ã—1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(+)â”€â”€â†’ Output           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.5 Gesamtarchitektur: VWorldModel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Visual World Model                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                           INPUT                                     â”‚    â”‚
â”‚  â”‚  obs['visual']: (B, 4, 3, 224, 224)                                â”‚    â”‚
â”‚  â”‚  obs['proprio']: (B, 4, 3)                                         â”‚    â”‚
â”‚  â”‚  act: (B, 4, 45)                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â”‚              â”‚              â”‚                          â”‚
â”‚                    â–¼              â–¼              â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   DINO Encoder   â”‚  â”‚  Proprio Encoder â”‚  â”‚  Action Encoder  â”‚          â”‚
â”‚  â”‚   (FROZEN)       â”‚  â”‚  (TRAINABLE)     â”‚  â”‚  (TRAINABLE)     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚          â”‚                     â”‚                      â”‚                     â”‚
â”‚          â–¼                     â–¼                      â–¼                     â”‚
â”‚    (B, 4, 256, 384)      (B, 4, 10)            (B, 4, 10)                   â”‚
â”‚          â”‚                     â”‚                      â”‚                     â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                    â”‚     CONCATENATE      â”‚                                â”‚
â”‚                    â”‚   (concat_dim=1)     â”‚                                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                         (B, 4, 256, 404)                                    â”‚
â”‚                                â”‚                                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â–¼                                   â–¼                          â”‚
â”‚       z_src[:, :3]                        z_tgt[:, 1:]                      â”‚
â”‚     (Historie)                           (Target)                           â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â–¼                                   â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚                          â”‚
â”‚  â”‚    ViT Predictor     â”‚                       â”‚                          â”‚
â”‚  â”‚    (TRAINABLE)       â”‚                       â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚                          â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â–¼                                   â”‚                          â”‚
â”‚        z_pred                                    â”‚                          â”‚
â”‚     (B, 3, 256, 404)                            â”‚                          â”‚
â”‚              â”‚                                   â”‚                          â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                    â”‚   LATENT LOSS    â”‚                                    â”‚
â”‚                    â”‚  MSE(z_pred,     â”‚                                    â”‚
â”‚                    â”‚      z_tgt)      â”‚                                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                              â”‚                                              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â–¼                               â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   VQ-VAE Decoder     â”‚        â”‚   VQ-VAE Decoder     â”‚                  â”‚
â”‚  â”‚   (TRAINABLE)        â”‚        â”‚   (TRAINABLE)        â”‚                  â”‚
â”‚  â”‚   auf z_pred         â”‚        â”‚   auf z (alle)       â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â–¼                               â–¼                              â”‚
â”‚      visual_pred                     visual_recon                          â”‚
â”‚    (B, 3, 3, 224, 224)            (B, 4, 3, 224, 224)                      â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â”‚                               â–¼                              â”‚
â”‚              â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚                    â”‚  DECODER LOSS    â”‚                      â”‚
â”‚              â”‚                    â”‚  MSE(recon, gt)  â”‚                      â”‚
â”‚              â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                    â”‚    TOTAL LOSS    â”‚                                    â”‚
â”‚                    â”‚ z_loss + decoder â”‚                                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Proprioceptive Encoder â€” VollstÃ¤ndiger Trainingsablauf

> **Verifiziert am 14.02.2026** â€” Alle Code-Pfade, Variablennamen und Tensor-Dimensionen wurden anhand des
> Quellcodes nachvollzogen. Referenzmodell: `outputs/2026-02-09/17-59-59` (500 Episoden, frameskip=2, num_hist=4).

### 6.1 Ãœberblick: Was wird trainiert und warum?

Der **Proprioceptive Encoder** (`proprio_encoder`) ist eine lernbare Projektion, die die rohe
EE-Position (End-Effector Position, 3D) in einen kompakten Embedding-Vektor (10D) transformiert.
Er wird **gemeinsam** mit dem Action Encoder, dem ViT Predictor und dem VQ-VAE Decoder trainiert.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TRAINIERBARE vs. EINGEFRORENE KOMPONENTEN                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Komponente              Trainiert?    Optimizer                   LR       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  DINO v2 Encoder         âœ— FROZEN     (encoder_optimizer)        1e-6 (*)  â”‚
â”‚  Proprio Encoder         âœ“ TRAINIERT  action_encoder_optimizer   5e-4      â”‚
â”‚  Action Encoder          âœ“ TRAINIERT  action_encoder_optimizer   5e-4      â”‚
â”‚  ViT Predictor           âœ“ TRAINIERT  predictor_optimizer        2e-4      â”‚
â”‚  VQ-VAE Decoder          âœ“ TRAINIERT  decoder_optimizer          1e-4      â”‚
â”‚                                                                             â”‚
â”‚  (*) encoder_optimizer existiert, aber .step() wird NIE aufgerufen          â”‚
â”‚      weil train_encoder=False â†’ alle Parameter haben requires_grad=False   â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: Proprio Encoder und Action Encoder teilen sich denselben          â”‚
â”‚           Optimizer (action_encoder_optimizer) und dieselbe Learning Rate!  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Datensatz â†’ Proprio-Extraktion (Schritt fÃ¼r Schritt)

#### 6.2.1 Rohdaten im Datensatz

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATENSATZ-QUELLE (Primitiv-basiert)                                        â”‚
â”‚  Pfad: fcs_datasets/NEps1000_RobOpac0_NPrim20_NCams4_NCube1/              â”‚
â”‚                                                                             â”‚
â”‚  Pro Episode (z.B. 000042/):                                                â”‚
â”‚  â”œâ”€â”€ obses.pth         # (T, H, W, C) = (20, 256, 256, 3) BGR uint8       â”‚
â”‚  â”œâ”€â”€ 00.h5             # Primitiv 0 (= Timestep 0)                        â”‚
â”‚  â”‚   â”œâ”€â”€ action        # (8,) 8D-Action                                   â”‚
â”‚  â”‚   â”‚                   [x_s, y_s, z_s, g_s, x_e, y_e, z_e, g_e]         â”‚
â”‚  â”‚   â”œâ”€â”€ eef_states    # (1, 1, 14) â†’ 14D EEF-Zustand                     â”‚
â”‚  â”‚   â”‚                   [pos_end(3), pos_start(3), quat_end(4),           â”‚
â”‚  â”‚   â”‚                    quat_start(4)]                                    â”‚
â”‚  â”‚   â”œâ”€â”€ positions     # (1, N_cubes, 4) WÃ¼rfelpositionen (homogen)        â”‚
â”‚  â”‚   â””â”€â”€ info/         # n_steps, movement_distance, phase,                â”‚
â”‚  â”‚                       primitive_name, primitive_type                      â”‚
â”‚  â”œâ”€â”€ 01.h5 ... 19.h5                                                       â”‚
â”‚  â””â”€â”€ property_params.pkl                                                    â”‚
â”‚                                                                             â”‚
â”‚  985 Episoden Ã— 20 Primitive = 19.700 Datenpunkte                          â”‚
â”‚                                                                             â”‚
â”‚  DETAIL: eef_states[0, 0, :] = 14D Vektor:                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ [0:3]  pos_end   = EE-Position NACH Bewegung (aktuell)          â”‚       â”‚
â”‚  â”‚ [3:6]  pos_start = EE-Position VOR Bewegung (vorherig)          â”‚       â”‚
â”‚  â”‚ [6:10] quat_end  = EE-Quaternion NACH Bewegung                  â”‚       â”‚
â”‚  â”‚ [10:14]quat_start= EE-Quaternion VOR Bewegung                   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚  DETAIL: action[:] = 8D Vektor:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ [0:3]  start_pos = EE-Position am Primitiv-Start                â”‚       â”‚
â”‚  â”‚ [3]    g_start   = Gripper-State am Start (0.0/0.04)            â”‚       â”‚
â”‚  â”‚ [4:7]  end_pos   = EE-Position am Primitiv-Ende                 â”‚       â”‚
â”‚  â”‚ [7]    g_end     = Gripper-State am Ende (0.0/0.04)             â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.2.2 Laden in `FrankaCubeStackDataset.__init__()` (franka_cube_stack_dset.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT A: H5-Dateien lesen (pro Episode, pro Primitiv/Timestep)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (franka_cube_stack_dset.py, __init__):                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  for t in range(episode_length):        # t = 0..19 (20 Primitive)         â”‚
â”‚      with h5py.File(f"{t:02d}.h5") as f:                                   â”‚
â”‚          action = f["action"][:]        # â†’ numpy (8,) 8D-Action           â”‚
â”‚          eef = f["eef_states"][:]       # â†’ numpy (1, 1, 14)               â”‚
â”‚          eef_states.append(eef.flatten())  # â†’ numpy (14,)                 â”‚
â”‚                                                                             â”‚
â”‚  Variablen nach dem Loop:                                                   â”‚
â”‚  self.all_actions[i]    : numpy (20, 8)   # 20 Primitive Ã— 8D Actions     â”‚
â”‚  self.all_eef_states[i] : numpy (20, 14)  # 20 Primitive Ã— 14D EEF        â”‚
â”‚                                                                             â”‚
â”‚  Konvertierung zu Tensoren:                                                 â”‚
â”‚  self.actions_tensors[i] = torch.from_numpy(actions).float()  # (20, 8)    â”‚
â”‚  self.eef_tensors[i]    = torch.from_numpy(eef).float()       # (20, 14)   â”‚
â”‚                                                                             â”‚
â”‚  Automatische Dimensions-Erkennung:                                         â”‚
â”‚  self.action_dim = actions.shape[-1]  # â†’ 8 (auto-detektiert aus H5)       â”‚
â”‚  self.proprio_dim = 3                 # â†’ fest: nur eef[:, :3] = pos_end   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT B: Z-Score-Normalisierungs-Statistiken berechnen                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (franka_cube_stack_dset.py, __init__, normalize_action=True):         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚                                                                             â”‚
â”‚  # Alle EEF-Daten aller Episoden zusammenfassen:                            â”‚
â”‚  all_eef_flat = torch.cat(self.eef_tensors, dim=0)  # (19700, 14)         â”‚
â”‚                                                                             â”‚
â”‚  # Proprio-Statistiken: NUR erste 3 Dimensionen (= pos_end = aktuelle Pos) â”‚
â”‚  self.proprio_mean = all_eef_flat[:, :3].mean(dim=0)  # (3,)              â”‚
â”‚  self.proprio_std  = all_eef_flat[:, :3].std(dim=0) + 1e-6  # (3,)        â”‚
â”‚                                                                             â”‚
â”‚  Typische Werte (985 Episoden):                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  proprio_mean â‰ˆ [0.476, 0.017, 0.161]   (Meter, Weltkoord.)    â”‚        â”‚
â”‚  â”‚  proprio_std  â‰ˆ [0.124, 0.161, 0.072]   (Streuung in Meter)    â”‚        â”‚
â”‚  â”‚                                                                  â”‚        â”‚
â”‚  â”‚  Interpretation:                                                 â”‚        â”‚
â”‚  â”‚  - x â‰ˆ 0.476m Â± 0.124m (vor/zurÃ¼ck)                             â”‚        â”‚
â”‚  â”‚  - y â‰ˆ 0.017m Â± 0.161m (links/rechts, zentriert)                â”‚        â”‚
â”‚  â”‚  - z â‰ˆ 0.161m Â± 0.072m (HÃ¶he Ã¼ber Tisch)                        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: Diese Statistiken werden bei Inferenz/Planning benÃ¶tigt!         â”‚
â”‚  Der Planner muss die gleiche Normalisierung verwenden.                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.2.3 Proprio-Extraktion in `get_frames()` (franka_cube_stack_dset.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT C: Proprio fÃ¼r einen Batch-Eintrag extrahieren                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (franka_cube_stack_dset.py, get_frames):                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  def get_frames(self, idx, frames):                                         â”‚
â”‚      eef = self.eef_tensors[idx][frames]        # (T_slice, 14)            â”‚
â”‚      proprio = (eef[:, :3] - self.proprio_mean) / self.proprio_std          â”‚
â”‚      #           â†‘                                                          â”‚
â”‚      #  Nur erste 3 Dims: pos_end = EE-Position NACH Bewegung [x, y, z]    â”‚
â”‚      #  = Aktuelle Position des End-Effectors                               â”‚
â”‚      #                                                                      â”‚
â”‚      obs = {"visual": image, "proprio": proprio}                            â”‚
â”‚      return obs, act, state, {}                                             â”‚
â”‚                                                                             â”‚
â”‚  Tensor-Dimensionen (Beispiel: frameskip=1, num_hist=4, num_pred=1):        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  Input frames (nach TrajSlicerDataset):                                     â”‚
â”‚    frames = [start, start+1, start+2, start+3, start+4]  # 5 Frames       â”‚
â”‚              â†‘ frameskip=1 bei Primitiv-DatensÃ¤tzen (jeder Schritt)         â”‚
â”‚                                                                             â”‚
â”‚  eef:     (5, 14)   â† 5 selektierte Zeitschritte, 14D EEF                 â”‚
â”‚  eef[:, :3]: (5, 3) â† NUR pos_end [x, y, z] (aktuelle EE-Position)        â”‚
â”‚  proprio: (5, 3)    â† z-normalisiert                                       â”‚
â”‚                                                                             â”‚
â”‚  Normalisierung (Element-weise):                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  proprio[t] = (eef[t, :3] - proprio_mean) / proprio_std        â”‚        â”‚
â”‚  â”‚                                                                  â”‚        â”‚
â”‚  â”‚  eef[t, :3] = pos_end = EE-Position am Ende des Primitivs t     â”‚        â”‚
â”‚  â”‚  Beispiel: eef = [0.45, 0.02, 0.16]                              â”‚        â”‚
â”‚  â”‚  proprio  = ([0.45, 0.02, 0.16] - [0.476, 0.017, 0.161])        â”‚        â”‚
â”‚  â”‚             / [0.124, 0.161, 0.072]                              â”‚        â”‚
â”‚  â”‚           = [-0.21, 0.019, -0.014]   â† ~N(0,1) verteilt         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.2.4 Frameskip-Anwendung in `TrajSlicerDataset` (traj_dset.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT D: Frameskip und Slicing                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG fÃ¼r Primitiv-DatensÃ¤tze: frameskip = 1                             â”‚
â”‚  â†’ Jeder Timestep ist bereits ein Bewegungsprimitiv!                        â”‚
â”‚  â†’ Kein temporales Subsampling nÃ¶tig.                                       â”‚
â”‚                                                                             â”‚
â”‚  Code (traj_dset.py, TrajSlicerDataset.__getitem__):                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  def __getitem__(self, idx):                                                â”‚
â”‚      i, start, end = self.slices[idx]    # z.B. (42, 3, 8)                â”‚
â”‚      obs, act, state, _ = self.dataset[i]  # Volle Episode laden          â”‚
â”‚      for k, v in obs.items():                                               â”‚
â”‚          obs[k] = v[start:end:self.frameskip]  # frameskip=1: alle         â”‚
â”‚      state = state[start:end:self.frameskip]                                â”‚
â”‚      act = act[start:end]                                                   â”‚
â”‚      act = rearrange(act, "(n f) d -> n (f d)", n=self.num_frames)         â”‚
â”‚      return obs, act, state                                                 â”‚
â”‚                                                                             â”‚
â”‚  Beispiel (frameskip=1, num_frames=5, start=3, end=8):                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚                                                                             â”‚
â”‚  Primitiv-Sequenz der Episode:                                              â”‚
â”‚  Index:  0  1  2 [3] [4] [5] [6] [7]  8  9 ...                            â”‚
â”‚                  â†‘    â†‘    â†‘    â†‘    â†‘                                     â”‚
â”‚  Frames:        F0   F1   F2   F3   F4     (alle, frameskip=1)             â”‚
â”‚                                                                             â”‚
â”‚  obs['proprio']: v[3:8:1] = v[[3, 4, 5, 6, 7]]  â†’ (5, 3)                 â”‚
â”‚  obs['visual']:  v[3:8:1] = v[[3, 4, 5, 6, 7]]  â†’ (5, 3, 224, 224)       â”‚
â”‚                                                                             â”‚
â”‚  act: v[3:8] = 5 Actions â†’ rearrange zu (5, 8)                             â”‚
â”‚       â†‘ n=num_frames=5, f=frameskip=1, d=action_dim=8                      â”‚
â”‚       â†‘ (5Ã—1, 8) â†’ (5, 1Ã—8=8)  â† Keine Konkatenation!                    â”‚
â”‚                                                                             â”‚
â”‚  KRITISCH: Proprio wird mit demselben Frameskip subsampled wie Visual!      â”‚
â”‚  â†’ Proprio und Visual sind zeitlich perfekt synchron.                       â”‚
â”‚  â†’ Bei frameskip=1: Actions werden 1:1 durchgereicht (8D bleibt 8D)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Batch-Zusammenstellung â€” Tensoren beim Dataloader-Output

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATALOADER OUTPUT (1 Batch)                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Konfiguration: B=8, num_hist=4, num_pred=1, frameskip=1, action_dim=8     â”‚
â”‚                 proprio_dim=3, img_size=224                                 â”‚
â”‚                                                                             â”‚
â”‚  obs, act, state = next(dataloader)                                         â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Variable           â”‚ Shape                  â”‚ Beschreibung         â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ obs['visual']       â”‚ (8, 5, 3, 224, 224)    â”‚ 5 RGB Bilder         â”‚   â”‚
â”‚  â”‚ obs['proprio']      â”‚ (8, 5, 3)              â”‚ 5 EE-Positionen      â”‚   â”‚
â”‚  â”‚ act                 â”‚ (8, 5, 8)              â”‚ 5 Ã— 8D Actions       â”‚   â”‚
â”‚  â”‚ state               â”‚ (8, 5, 14)             â”‚ 5 EEF-ZustÃ¤nde       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Wobei:                                                                     â”‚
â”‚  - 5 = num_hist + num_pred = 4 + 1 = 5 Zeitschritte                       â”‚
â”‚  - 8 = action_dim (8D Primitiv-Action, kein frameskip)                     â”‚
â”‚  - 3 = proprio_dim (EE x, y, z = eef[:, :3] = pos_end)                    â”‚
â”‚  - 14 = eef_dim (voller EEF-Zustand mit Start+End)                         â”‚
â”‚                                                                             â”‚
â”‚  obs['proprio'] Beispiel-Werte (z-normalisiert):                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Batch 0, Frame 0: [-0.21,  0.02, -0.01]  â† ~N(0,1)              â”‚   â”‚
â”‚  â”‚  Batch 0, Frame 1: [-0.18,  0.05,  0.12]                          â”‚   â”‚
â”‚  â”‚  Batch 0, Frame 2: [-0.15,  0.09,  0.25]  â† Roboter bewegt sich  â”‚   â”‚
â”‚  â”‚  Batch 0, Frame 3: [-0.10,  0.11,  0.38]                          â”‚   â”‚
â”‚  â”‚  Batch 0, Frame 4: [-0.05,  0.13,  0.50]  â† Target-Frame          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.4 Proprio Encoder â€” Architektur und Forward Pass

#### 6.4.1 Instanziierung in `train.py` (init_models)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT E: Proprio Encoder Instanziierung                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (train.py, init_models):                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  self.proprio_encoder = hydra.utils.instantiate(                            â”‚
â”‚      self.cfg.proprio_encoder,    # â†’ ProprioceptiveEmbedding              â”‚
â”‚      in_chans=self.datasets["train"].proprio_dim,  # = 3                   â”‚
â”‚      emb_dim=self.cfg.proprio_emb_dim,             # = 10                  â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  Hydra-Konfiguration (conf/proprio_encoder/proprio.yaml):                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  _target_: models.proprio.ProprioceptiveEmbedding                           â”‚
â”‚  num_frames: 2          # â† Nicht relevant (nur fÃ¼r pos_embed, unused)     â”‚
â”‚  tubelet_size: 1        # â† kernel_size = stride = 1                       â”‚
â”‚  use_3d_pos: False      # â† Kein 3D Positional Embedding                  â”‚
â”‚                                                                             â”‚
â”‚  Resultierende Instanz:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ProprioceptiveEmbedding(                                           â”‚   â”‚
â”‚  â”‚    in_chans = 3          # Input: EE-Position (x, y, z)            â”‚   â”‚
â”‚  â”‚    emb_dim  = 10         # Output: Proprio-Embedding               â”‚   â”‚
â”‚  â”‚    (patch_embed): Conv1d(                                           â”‚   â”‚
â”‚  â”‚      in_channels  = 3,   # 3 â†’ Proprio-Dimensionen                 â”‚   â”‚
â”‚  â”‚      out_channels = 10,  # 10 â†’ proprio_emb_dim                    â”‚   â”‚
â”‚  â”‚      kernel_size  = 1,   # Punkt-weise Projektion                   â”‚   â”‚
â”‚  â”‚      stride       = 1    # Kein Downsampling                        â”‚   â”‚
â”‚  â”‚    )                                                                â”‚   â”‚
â”‚  â”‚  )                                                                  â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Trainierbare Parameter:                                            â”‚   â”‚
â”‚  â”‚  - patch_embed.weight: (10, 3, 1) = 30 Parameter                   â”‚   â”‚
â”‚  â”‚  - patch_embed.bias:   (10,)      = 10 Parameter                   â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚   â”‚
â”‚  â”‚  GESAMT: 40 trainierbare Parameter                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.4.2 Forward Pass des Proprio Encoders (models/proprio.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT F: ProprioceptiveEmbedding.forward(x)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (models/proprio.py):                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  def forward(self, x):                                                      â”‚
â”‚      # x: (B, T, D) = (8, 5, 3)                                           â”‚
â”‚      x = x.permute(0, 2, 1)   # â†’ (B, D, T) = (8, 3, 5)                  â”‚
â”‚      x = self.patch_embed(x)  # Conv1d: (8, 3, 5) â†’ (8, 10, 5)           â”‚
â”‚      x = x.permute(0, 2, 1)   # â†’ (B, T, emb_dim) = (8, 5, 10)           â”‚
â”‚      return x                                                               â”‚
â”‚                                                                             â”‚
â”‚  Tensor-Fluss im Detail:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  INPUT x:          (B=8, T=5, D=3)                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚   â”‚
â”‚  â”‚  â”‚  Batch 0: [[xâ‚€,yâ‚€,zâ‚€],              â”‚  â† 5 EE-Positionen       â”‚   â”‚
â”‚  â”‚  â”‚            [xâ‚,yâ‚,zâ‚],              â”‚     (z-normalisiert)     â”‚   â”‚
â”‚  â”‚  â”‚            [xâ‚‚,yâ‚‚,zâ‚‚],              â”‚                          â”‚   â”‚
â”‚  â”‚  â”‚            [xâ‚ƒ,yâ‚ƒ,zâ‚ƒ],              â”‚                          â”‚   â”‚
â”‚  â”‚  â”‚            [xâ‚„,yâ‚„,zâ‚„]]              â”‚                          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚   â”‚
â”‚  â”‚                     â”‚                                                â”‚   â”‚
â”‚  â”‚                     â–¼ permute(0, 2, 1)                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  PERMUTED:          (B=8, D=3, T=5)                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚   â”‚
â”‚  â”‚  â”‚  Batch 0: [[xâ‚€,xâ‚,xâ‚‚,xâ‚ƒ,xâ‚„],      â”‚  â† Channels-first       â”‚   â”‚
â”‚  â”‚  â”‚            [yâ‚€,yâ‚,yâ‚‚,yâ‚ƒ,yâ‚„],      â”‚     fÃ¼r Conv1d            â”‚   â”‚
â”‚  â”‚  â”‚            [zâ‚€,zâ‚,zâ‚‚,zâ‚ƒ,zâ‚„]]      â”‚                          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚   â”‚
â”‚  â”‚                     â”‚                                                â”‚   â”‚
â”‚  â”‚                     â–¼ Conv1d(3â†’10, k=1, s=1)                        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  CONV OUTPUT:       (B=8, emb=10, T=5)                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚   â”‚
â”‚  â”‚  â”‚  Pro Zeitschritt t:                  â”‚                           â”‚   â”‚
â”‚  â”‚  â”‚  emb_t = W Ã— [x_t, y_t, z_t] + b   â”‚  â† Lineare Projektion    â”‚   â”‚
â”‚  â”‚  â”‚          â†‘                           â”‚     W: (10, 3)           â”‚   â”‚
â”‚  â”‚  â”‚    10Ã—3 Matrix                       â”‚     b: (10,)             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚   â”‚
â”‚  â”‚                     â”‚                                                â”‚   â”‚
â”‚  â”‚                     â–¼ permute(0, 2, 1)                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  OUTPUT:            (B=8, T=5, emb_dim=10)                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚   â”‚
â”‚  â”‚  â”‚  z_proprio: 10D Embedding pro Frame  â”‚  â† Verwendbar fÃ¼r        â”‚   â”‚
â”‚  â”‚  â”‚  [eâ‚€, eâ‚, eâ‚‚, ..., eâ‚‰]              â”‚     Concat mit DINO      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Mathematisch: Conv1d(k=1, s=1) â‰¡ nn.Linear(3, 10)                        â”‚
â”‚  â†’ Punkt-weise lineare Transformation, identisch fÃ¼r jeden Zeitschritt     â”‚
â”‚  â†’ Keine Aktivierungsfunktion (rein linear!)                               â”‚
â”‚  â†’ Jeder Zeitschritt wird unabhÃ¤ngig transformiert                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.5 Embedding-Fusion: Proprio + Visual + Action (encode-Methode)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT G: VWorldModel.encode(obs, act) â€” Fusion aller ModalitÃ¤ten        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (visual_world_model.py, encode):                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  def encode(self, obs, act):                                                â”‚
â”‚      z_dct = self.encode_obs(obs)    # â†’ {"visual": ..., "proprio": ...}   â”‚
â”‚      act_emb = self.encode_act(act)  # â†’ (B, T, action_emb_dim)           â”‚
â”‚      # concat_dim=1 â†’ Fusion entlang Feature-Dimension                    â”‚
â”‚                                                                             â”‚
â”‚  Aufrufe im Detail:                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                             â”‚
â”‚  1) encode_obs(obs):                                                        â”‚
â”‚     â”œâ”€â”€ DINO Encoder:                                                       â”‚
â”‚     â”‚   obs['visual']: (8, 5, 3, 224, 224)                                 â”‚
â”‚     â”‚     â†’ rearrange: (40, 3, 224, 224)                                   â”‚
â”‚     â”‚     â†’ DINO forward: (40, 256, 384)                                   â”‚
â”‚     â”‚     â†’ rearrange: (8, 5, 256, 384)                                    â”‚
â”‚     â”‚   visual_embs: (B=8, T=5, P=256, D=384)                              â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â””â”€â”€ Proprio Encoder:                                                    â”‚
â”‚         obs['proprio']: (8, 5, 3)                                           â”‚
â”‚           â†’ proprio_encoder.forward: (8, 5, 10)                            â”‚
â”‚         proprio_emb: (B=8, T=5, emb_dim=10)                                â”‚
â”‚                                                                             â”‚
â”‚  2) encode_act(act):                                                        â”‚
â”‚     act: (8, 5, 12)                                                         â”‚
â”‚       â†’ action_encoder.forward: (8, 5, 10)                                 â”‚
â”‚     act_emb: (B=8, T=5, emb_dim=10)                                        â”‚
â”‚                                                                             â”‚
â”‚  3) Fusion (concat_dim=1):                                                  â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚     # Proprio tiling: (B,T,10) â†’ unsqueeze â†’ (B,T,1,10) â†’ tile â†’          â”‚
â”‚     #                  (B,T,256,10) â†’ repeat(num_proprio_repeat=1) â†’       â”‚
â”‚     #                  (B,T,256,10)                                         â”‚
â”‚     proprio_tiled = repeat(proprio_emb.unsqueeze(2),                        â”‚
â”‚                            "b t 1 a -> b t f a", f=256)                    â”‚
â”‚     proprio_repeated = proprio_tiled.repeat(1, 1, 1, 1)   # Ã—1            â”‚
â”‚                                                                             â”‚
â”‚     # Action tiling: identisch                                              â”‚
â”‚     act_tiled = repeat(act_emb.unsqueeze(2),                                â”‚
â”‚                        "b t 1 a -> b t f a", f=256)                        â”‚
â”‚     act_repeated = act_tiled.repeat(1, 1, 1, 1)   # Ã—1                    â”‚
â”‚                                                                             â”‚
â”‚     # Concatenation entlang letzer Dimension:                               â”‚
â”‚     z = torch.cat([visual_embs, proprio_repeated, act_repeated], dim=3)    â”‚
â”‚                                                                             â”‚
â”‚  Resultat:                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  z: (B=8, T=5, P=256, D=404)                                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Aufbau der 404 Dimensionen pro Patch:                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚
â”‚  â”‚  â”‚  DINO Visual     â”‚  Proprio Emb â”‚  Action Emb  â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  z[..., :384]    â”‚ z[...,384:394]â”‚ z[...,394:404]â”‚                â”‚   â”‚
â”‚  â”‚  â”‚  384 dim         â”‚  10 dim      â”‚  10 dim      â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  (FROZEN)        â”‚ (TRAINIERT)  â”‚ (TRAINIERT)  â”‚                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  JEDER der 256 Patches enthÃ¤lt dieselben Proprio/Action-Werte       â”‚   â”‚
â”‚  â”‚  (getiled Ã¼ber alle Patches)                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.6 Prediction und Loss-Berechnung fÃ¼r Proprio

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT H: Forward Pass mit Source/Target-Split und Loss                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (visual_world_model.py, forward):                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚                                                                             â”‚
â”‚  z = self.encode(obs, act)                                                  â”‚
â”‚  z: (B=8, T=5, P=256, D=404)                                               â”‚
â”‚                                                                             â”‚
â”‚  # Source/Target Aufteilung:                                                â”‚
â”‚  z_src = z[:, :num_hist]     = z[:, :4]     # (8, 4, 256, 404)            â”‚
â”‚  z_tgt = z[:, num_pred:]     = z[:, 1:]     # (8, 4, 256, 404)            â”‚
â”‚                                                                             â”‚
â”‚  Zeitliche Zuordnung (num_hist=4, num_pred=1):                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Zeitschritt:    t=0    t=1    t=2    t=3    t=4                   â”‚     â”‚
â”‚  â”‚                                                                    â”‚     â”‚
â”‚  â”‚  z_src:         [F0]   [F1]   [F2]   [F3]                         â”‚     â”‚
â”‚  â”‚                                         â†“ Predictor                â”‚     â”‚
â”‚  â”‚  z_pred:        [P1]   [P2]   [P3]   [P4]                         â”‚     â”‚
â”‚  â”‚                                                                    â”‚     â”‚
â”‚  â”‚  z_tgt:         [F1]   [F2]   [F3]   [F4]   â† Ground Truth       â”‚     â”‚
â”‚  â”‚                                                                    â”‚     â”‚
â”‚  â”‚  Vergleich: z_pred[i] soll z_tgt[i] vorhersagen                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â”‚  ViT Predictor:                                                             â”‚
â”‚  z_pred = self.predict(z_src)   # (8, 4, 256, 404)                         â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  â•‘  LOSS-BERECHNUNG (concat_dim=1)                                     â•‘    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚                                                                             â”‚
â”‚  Variablen-Mapping (self.proprio_dim=10, self.action_dim=10):              â”‚
â”‚                                                                             â”‚
â”‚  z-Vektor Layout pro Patch (404 dim):                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚    Visual (384)  â”‚ Proprio (10)â”‚ Action (10) â”‚                           â”‚
â”‚  â”‚  Indices: [0:384]â”‚ [384:394]   â”‚ [394:404]   â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                             â”‚
â”‚  Code (visual_world_model.py, forward, concat_dim=1):                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚                                                                             â”‚
â”‚  # 1) z_visual_loss: NUR visuelle Features (384 dim)                       â”‚
â”‚  z_visual_loss = MSE(                                                       â”‚
â”‚      z_pred[:, :, :, :-(10+10)],       # z_pred[..., :384]                 â”‚
â”‚      z_tgt[:, :, :, :-(10+10)].detach()                                    â”‚
â”‚  )                                                                          â”‚
â”‚  # Shape: MSE Ã¼ber (8, 4, 256, 384) vs (8, 4, 256, 384)                   â”‚
â”‚  # â†’ Skalar                                                                â”‚
â”‚                                                                             â”‚
â”‚  # 2) z_proprio_loss: NUR Proprio-Embedding (10 dim)                       â”‚
â”‚  z_proprio_loss = MSE(                                                      â”‚
â”‚      z_pred[:, :, :, -(10+10):-10],    # z_pred[..., 384:394]             â”‚
â”‚      z_tgt[:, :, :, -(10+10):-10].detach()                                 â”‚
â”‚  )                                                                          â”‚
â”‚  # Shape: MSE Ã¼ber (8, 4, 256, 10) vs (8, 4, 256, 10)                     â”‚
â”‚  # â†’ Skalar                                                                â”‚
â”‚  # HINWEIS: Alle 256 Patches haben identische Proprio-Werte (getiled)      â”‚
â”‚  # â†’ MSE wird Ã¼ber alle Patches gemittelt, aber da identisch = kein Fehler â”‚
â”‚                                                                             â”‚
â”‚  # 3) z_loss: Visual + Proprio ZUSAMMEN (394 dim, OHNE Action)             â”‚
â”‚  z_loss = MSE(                                                              â”‚
â”‚      z_pred[:, :, :, :-10],            # z_pred[..., :394]                 â”‚
â”‚      z_tgt[:, :, :, :-10].detach()                                         â”‚
â”‚  )                                                                          â”‚
â”‚  # Shape: MSE Ã¼ber (8, 4, 256, 394) vs (8, 4, 256, 394)                   â”‚
â”‚  # â†’ Skalar                                                                â”‚
â”‚  # â†‘ DAS IST DER HAUPTLOSS, der zum Training-Loss addiert wird!           â”‚
â”‚                                                                             â”‚
â”‚  loss = loss + z_loss   â† Proprio ist TEIL des Haupt-Losses!               â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG:                                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                  â”‚
â”‚  â€¢ z_loss enthÃ¤lt IMPLIZIT den Proprio-Loss (da 394 = 384 + 10)            â”‚
â”‚  â€¢ z_visual_loss und z_proprio_loss werden NUR geloggt, nicht addiert      â”‚
â”‚  â€¢ Action-Embedding (letzte 10 dim) wird NICHT in den Loss einbezogen      â”‚
â”‚  â€¢ z_tgt wird mit .detach() abgetrennt â†’ DINO-Encoder bekommt keinen       â”‚
â”‚    Gradient (ist ohnehin eingefroren, aber detach ist zusÃ¤tzliche Sicherh.)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.7 Gradient-Fluss und Optimizer-Update

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT I: Backward Pass und Gradient-Fluss zum Proprio Encoder            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (train.py, train):                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚  # 1. Zero-Grad fÃ¼r alle Optimizer                                         â”‚
â”‚  self.encoder_optimizer.zero_grad()                                         â”‚
â”‚  self.decoder_optimizer.zero_grad()                                         â”‚
â”‚  self.predictor_optimizer.zero_grad()                                       â”‚
â”‚  self.action_encoder_optimizer.zero_grad()  â† Setzt Gradienten auf 0      â”‚
â”‚  #  â†‘ Dieser Optimizer enthÃ¤lt BEIDE: action_encoder UND proprio_encoder   â”‚
â”‚                                                                             â”‚
â”‚  # 2. Backward Pass                                                        â”‚
â”‚  self.accelerator.backward(loss)                                            â”‚
â”‚  #  loss = z_loss + decoder_loss                                           â”‚
â”‚  #       = MSE(z_pred[..., :394], z_tgt[..., :394])    â† enthÃ¤lt Proprio  â”‚
â”‚  #       + MSE(visual_recon, obs_visual) + 0.25 Ã— vq_loss                  â”‚
â”‚                                                                             â”‚
â”‚  # 3. Optimizer-Steps (NUR trainierbare Komponenten)                       â”‚
â”‚  # self.encoder_optimizer.step()  â† NICHT aufgerufen (train_encoder=False) â”‚
â”‚  self.decoder_optimizer.step()             # âœ“ train_decoder=True          â”‚
â”‚  self.predictor_optimizer.step()           # âœ“ train_predictor=True        â”‚
â”‚  self.action_encoder_optimizer.step()      # âœ“ IMMER aufgerufen           â”‚
â”‚  # â†‘ Updated sowohl action_encoder.parameters() ALS AUCH                   â”‚
â”‚  #   proprio_encoder.parameters()!                                          â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  â•‘  GRADIENT-FLUSS ZUM PROPRIO ENCODER (RÃ¼ckwÃ¤rtspfad)                 â•‘    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚                                                                             â”‚
â”‚  z_loss = MSE(z_pred[..., :394], z_tgt[..., :394].detach())                â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”‚  âˆ‚z_loss / âˆ‚z_pred                                                   â”‚
â”‚     â–¼                                                                       â”‚
â”‚  z_pred = self.predict(z_src)                                               â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”‚  âˆ‚z_pred / âˆ‚z_src  (durch ViT Predictor)                             â”‚
â”‚     â–¼                                                                       â”‚
â”‚  z_src = z[:, :4]                                                           â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”‚  âˆ‚z_src / âˆ‚z  (Identity, nur Slicing)                                â”‚
â”‚     â–¼                                                                       â”‚
â”‚  z = cat([visual_embs, proprio_tiled, act_tiled], dim=-1)                   â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”‚  âˆ‚z / âˆ‚proprio_tiled  (Identity, nur Concat-RÃ¼ckpropagation)         â”‚
â”‚     â–¼                                                                       â”‚
â”‚  proprio_tiled = repeat(proprio_emb.unsqueeze(2), ..., f=256)              â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”‚  âˆ‚proprio_tiled / âˆ‚proprio_emb  (Summierung Ã¼ber 256 Patches)        â”‚
â”‚     â–¼                                                                       â”‚
â”‚  proprio_emb = self.proprio_encoder(obs['proprio'])                        â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”‚  âˆ‚proprio_emb / âˆ‚W_proprio  (Conv1d Gradient)                        â”‚
â”‚     â–¼                                                                       â”‚
â”‚  W_proprio = self.proprio_encoder.patch_embed.weight  # (10, 3, 1)        â”‚
â”‚  b_proprio = self.proprio_encoder.patch_embed.bias    # (10,)             â”‚
â”‚                                                                             â”‚
â”‚  â†’ action_encoder_optimizer.step() aktualisiert W und b!                   â”‚
â”‚                                                                             â”‚
â”‚  GRADIENT-VERSTÃ„RKUNG DURCH TILING:                                        â”‚
â”‚  Da proprio_emb auf 256 Patches getiled wird, wird der Gradient             â”‚
â”‚  Ã¼ber alle 256 Patches summiert:                                            â”‚
â”‚  âˆ‚L/âˆ‚proprio_emb = Î£(p=0..255) âˆ‚L/âˆ‚proprio_tiled[p]                       â”‚
â”‚  â†’ Faktor ~256Ã— stÃ¤rkerer Gradient als ohne Tiling                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.7.1 Optimizer-Konfiguration (train.py, init_optimizers)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPTIMIZER FÃœR PROPRIO ENCODER                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (train.py, init_optimizers):                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  self.action_encoder_optimizer = torch.optim.AdamW(                         â”‚
â”‚      itertools.chain(                                                       â”‚
â”‚          self.action_encoder.parameters(),   # Conv1d(12â†’10): 130 Params   â”‚
â”‚          self.proprio_encoder.parameters()   # Conv1d(3â†’10):  40 Params    â”‚
â”‚      ),                                                                     â”‚
â”‚      lr=self.cfg.training.action_encoder_lr  # = 5e-4 = 0.0005            â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  Parameter-Ãœbersicht:                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Modell-Komponente     â”‚ Parameter          â”‚ Shape      â”‚ Anzahl  â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  action_encoder       â”‚ patch_embed.weight â”‚ (10, 12, 1)â”‚    120  â”‚   â”‚
â”‚  â”‚  action_encoder       â”‚ patch_embed.bias   â”‚ (10,)      â”‚     10  â”‚   â”‚
â”‚  â”‚  proprio_encoder      â”‚ patch_embed.weight â”‚ (10,  3, 1)â”‚     30  â”‚   â”‚
â”‚  â”‚  proprio_encoder      â”‚ patch_embed.bias   â”‚ (10,)      â”‚     10  â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  GESAMT               â”‚                    â”‚            â”‚    170  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  AdamW-Eigenschaften:                                                       â”‚
â”‚  - Learning Rate: 5e-4                                                      â”‚
â”‚  - Weight Decay: Standard (0.01)                                            â”‚
â”‚  - Betas: Standard (0.9, 0.999)                                            â”‚
â”‚  - Eps: Standard (1e-8)                                                     â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: Beide Encoder teilen NICHT die Gewichte, nur den Optimizer!      â”‚
â”‚  â†’ Jeder hat eigene W und b, aber dieselbe Learning Rate.                  â”‚
â”‚  â†’ AdamW verwaltet separate Momentum- und Varianz-Statistiken              â”‚
â”‚     fÃ¼r jeden Parameter.                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.8 Separate-Embedding: Proprio aus z extrahieren (separate_emb)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT J: Proprio aus dem kombinierten z-Tensor extrahieren               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (visual_world_model.py, separate_emb, concat_dim=1):                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  def separate_emb(self, z):                                                 â”‚
â”‚      # z: (B, T, P=256, D=404)                                             â”‚
â”‚      # self.proprio_dim = 10 (proprio_emb_dim Ã— num_proprio_repeat = 10Ã—1) â”‚
â”‚      # self.action_dim  = 10 (action_emb_dim Ã— num_action_repeat = 10Ã—1)  â”‚
â”‚                                                                             â”‚
â”‚      z_visual  = z[..., :-(10+10)]              # z[..., :384]   â†’ (B,T,256,384) â”‚
â”‚      z_proprio = z[..., -(10+10):-10]           # z[..., 384:394]â†’ (B,T,256,10)  â”‚
â”‚      z_act     = z[..., -10:]                   # z[..., 394:404]â†’ (B,T,256,10)  â”‚
â”‚                                                                             â”‚
â”‚      # RÃ¼ckgÃ¤ngigmachung des Tilings:                                      â”‚
â”‚      z_proprio = z_proprio[:, :, 0, :10 // 1]  # â†’ (B, T, 10)             â”‚
â”‚      z_act     = z_act[:, :, 0, :10 // 1]      # â†’ (B, T, 10)             â”‚
â”‚      # â†‘ Nimmt nur Patch 0, da alle 256 Patches identisch sind            â”‚
â”‚      # â†‘ :10//1 = :10 (Division durch num_proprio_repeat=1)               â”‚
â”‚                                                                             â”‚
â”‚      z_obs = {"visual": z_visual, "proprio": z_proprio}                    â”‚
â”‚      return z_obs, z_act                                                    â”‚
â”‚                                                                             â”‚
â”‚  Output-Dimensionen:                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚ z_obs["visual"]   â”‚ (B, T, 256, 384)       â”‚                            â”‚
â”‚  â”‚ z_obs["proprio"]  â”‚ (B, T, 10)             â”‚ â† Proprio Embedding       â”‚
â”‚  â”‚ z_act             â”‚ (B, T, 10)             â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                             â”‚
â”‚  Verwendung:                                                                â”‚
â”‚  - decode_obs() nutzt z_obs["visual"] fÃ¼r VQ-VAE Decoder                   â”‚
â”‚  - z_obs["proprio"] wird NICHT decodiert (kein Proprio-Decoder!)            â”‚
â”‚  - z_obs["proprio"] wird bei Planning fÃ¼r Rollout-Auswertung genutzt       â”‚
â”‚    (Proprio-Anteil der Objective Function)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.9 Rollout: Proprio im autoregressiven Vorhersage-Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT K: Autoregressive Vorhersage mit Proprio (rollout-Methode)         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (visual_world_model.py, rollout):                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚  def rollout(self, obs_0, act):                                             â”‚
â”‚      # obs_0['visual']:  (1, num_hist, 3, 224, 224)  = (1, 4, 3, 224, 224)â”‚
â”‚      # obs_0['proprio']: (1, num_hist, 3)             = (1, 4, 3)          â”‚
â”‚      # act:              (1, num_hist+H, action_dim)  = (1, 4+H, 12)      â”‚
â”‚                                                                             â”‚
â”‚      num_obs_init = obs_0['visual'].shape[1]   # = 4                       â”‚
â”‚      act_0 = act[:, :4]      # Initiale Actions: (1, 4, 12)               â”‚
â”‚      action = act[:, 4:]     # ZukÃ¼nftige Actions: (1, H, 12)             â”‚
â”‚                                                                             â”‚
â”‚      z = self.encode(obs_0, act_0)   # (1, 4, 256, 404)                   â”‚
â”‚      # â†‘ EnthÃ¤lt Proprio der initialen 4 Frames (aus obs_0)                â”‚
â”‚                                                                             â”‚
â”‚      # Autoregressive Schleife:                                             â”‚
â”‚      t = 0                                                                  â”‚
â”‚      while t < H:                                                           â”‚
â”‚          z_pred = self.predict(z[:, -4:])  # Letzte 4 Frames              â”‚
â”‚          z_new = z_pred[:, -1:]            # Nur letzter pred Frame        â”‚
â”‚          z_new = self.replace_actions_from_z(z_new, action[:, t:t+1])      â”‚
â”‚          z = torch.cat([z, z_new], dim=1)  # AnhÃ¤ngen                      â”‚
â”‚          t += 1                                                             â”‚
â”‚                                                                             â”‚
â”‚  Was passiert mit Proprio im Rollout?                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                                             â”‚
â”‚  1. INITIAL (t=0): z enthÃ¤lt echte Proprio-Embeddings aus obs_0             â”‚
â”‚     z[..., 384:394] = proprio_encoder(obs_0['proprio'])                    â”‚
â”‚                                                                             â”‚
â”‚  2. VORHERSAGE (t>0): z_pred enthÃ¤lt VORHERGESAGTE Proprio-Embeddings      â”‚
â”‚     z_pred = predict(z_src)                                                 â”‚
â”‚     z_pred[..., 384:394] = ViT-Vorhersage fÃ¼r Proprio-Embedding           â”‚
â”‚     â†‘ Der ViT Predictor sagt ALLE 404 Dimensionen vorher,                 â”‚
â”‚       einschlieÃŸlich der 10 Proprio-Dimensionen!                            â”‚
â”‚                                                                             â”‚
â”‚  3. ACTION REPLACEMENT: replace_actions_from_z() ersetzt NUR die            â”‚
â”‚     Action-Dimensionen (394:404), NICHT die Proprio-Dimensionen!           â”‚
â”‚     z_new[..., 384:394] = vorhergesagtes Proprio (unverÃ¤ndert)             â”‚
â”‚     z_new[..., 394:404] = neues Action-Embedding (ersetzt)                 â”‚
â”‚                                                                             â”‚
â”‚  Zeitlicher Verlauf von Proprio im z-Tensor:                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Frame:  F0    F1    F2    F3    F4    F5    F6    ...            â”‚     â”‚
â”‚  â”‚                                                                    â”‚     â”‚
â”‚  â”‚  Proprio: REAL  REAL  REAL  REAL  PRED  PRED  PRED  ...          â”‚     â”‚
â”‚  â”‚          â””â”€â”€â”€â”€ aus obs_0 â”€â”€â”€â”€â”˜  â””â”€â”€â”€ vom Predictor â”€â”€â”€â”€â”€â”€â”˜        â”‚     â”‚
â”‚  â”‚                                                                    â”‚     â”‚
â”‚  â”‚  Visual:  REAL  REAL  REAL  REAL  PRED  PRED  PRED  ...          â”‚     â”‚
â”‚  â”‚  Action:  REAL  REAL  REAL  REAL  NEW   NEW   NEW   ...          â”‚     â”‚
â”‚  â”‚                                  â”” replace_actions_from_z() â”˜     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â”‚  Am Ende: z_obses, z = self.separate_emb(z)                                â”‚
â”‚  z_obses["proprio"]: (1, 4+H+1, 10) â† Alle Proprio-Embeddings            â”‚
â”‚  z_obses["visual"]:  (1, 4+H+1, 256, 384) â† Alle Visual-Embeddings       â”‚
â”‚                                                                             â”‚
â”‚  BEDEUTUNG: Das Training des Proprio Encoders beeinflusst direkt           â”‚
â”‚  die QualitÃ¤t der Proprio-Vorhersage im Rollout!                           â”‚
â”‚  â†’ Schlecht trainierter Proprio Encoder = schlechte Proprio-Vorhersage     â”‚
â”‚  â†’ Guter Proprio Encoder = Predictor kann EE-Trajektorie korrekt          â”‚
â”‚    vorhersagen                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.10 Checkpoint: Proprio Encoder speichern und laden

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHRITT L: Checkpoint-Speicherung                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                             â”‚
â”‚  Code (train.py, __init__):                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  self._keys_to_save = ["epoch"]                                             â”‚
â”‚  # ... encoder, predictor, decoder (bedingt) ...                            â”‚
â”‚  self._keys_to_save += ["action_encoder", "proprio_encoder"]                â”‚
â”‚  # â†‘ IMMER gespeichert, unabhÃ¤ngig von train_encoder/train_predictor!      â”‚
â”‚                                                                             â”‚
â”‚  Code (train.py, save_ckpt):                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚  ckpt = {}                                                                  â”‚
â”‚  for k in self._keys_to_save:                                               â”‚
â”‚      ckpt[k] = self.accelerator.unwrap_model(self.__dict__[k])              â”‚
â”‚                                                                             â”‚
â”‚  Checkpoint-Inhalt (model_50.pth):                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Key                  â”‚ Typ                        â”‚ Inhalt         â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  "epoch"              â”‚ int                        â”‚ 50             â”‚   â”‚
â”‚  â”‚  "encoder"            â”‚ DinoV2Encoder              â”‚ DINO Weights   â”‚   â”‚
â”‚  â”‚  "encoder_optimizer"  â”‚ Adam state_dict            â”‚ Opt. States    â”‚   â”‚
â”‚  â”‚  "predictor"          â”‚ ViTPredictor               â”‚ ViT Weights    â”‚   â”‚
â”‚  â”‚  "predictor_optimizer"â”‚ AdamW state_dict           â”‚ Opt. States    â”‚   â”‚
â”‚  â”‚  "decoder"            â”‚ VQVAE                      â”‚ Decoder Wts    â”‚   â”‚
â”‚  â”‚  "decoder_optimizer"  â”‚ Adam state_dict            â”‚ Opt. States    â”‚   â”‚
â”‚  â”‚  "action_encoder"     â”‚ ProprioceptiveEmbedding    â”‚ Conv1d(12â†’10)  â”‚   â”‚
â”‚  â”‚  "proprio_encoder"    â”‚ ProprioceptiveEmbedding    â”‚ Conv1d(3â†’10)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  HINWEIS: action_encoder_optimizer wird NICHT als separater Key             â”‚
â”‚  gespeichert, da er in _keys_to_save nicht enthalten ist!                  â”‚
â”‚  â†’ Bei Checkpoint-Resumption wird der Optimizer NEU initialisiert.          â”‚
â”‚  â†’ Proprio/Action Encoder GEWICHTE werden geladen, aber Optimizer-State    â”‚
â”‚    (Momentum, Varianz) geht verloren.                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.11 Gesamtflowchart: Proprio Encoder Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PROPRIO ENCODER â€” VOLLSTÃ„NDIGER TRAININGS-FLOWCHART       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. DATENSATZ LADEN                                                  â”‚  â”‚
â”‚  â”‚  H5: eef_states (1,1,14) â†’ flatten â†’ (14,) â†’ [:3] = EE pos (3D)    â”‚  â”‚
â”‚  â”‚  500 Ep Ã— 25 Frames â†’ all_eef_flat: (12500, 14)                     â”‚  â”‚
â”‚  â”‚  proprio_mean = all_eef_flat[:, :3].mean(0) â†’ (3,) â‰ˆ [0.48,0.02,0.16] â”‚
â”‚  â”‚  proprio_std  = all_eef_flat[:, :3].std(0)  â†’ (3,) â‰ˆ [0.12,0.16,0.07] â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. BATCH-VORBEREITUNG (pro Training-Iteration)                      â”‚  â”‚
â”‚  â”‚  TrajSlicerDataset: frameskip=2, num_frames=5                        â”‚  â”‚
â”‚  â”‚  get_frames() â†’ proprio = (eef[:, :3] - mean) / std â†’ (5, 3)       â”‚  â”‚
â”‚  â”‚  Dataloader collate â†’ obs['proprio']: (B=8, T=5, D=3)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. PROPRIO ENCODING                                                 â”‚  â”‚
â”‚  â”‚  proprio_encoder.forward(obs['proprio'])                             â”‚  â”‚
â”‚  â”‚  (B=8, T=5, D=3)                                                    â”‚  â”‚
â”‚  â”‚    â†’ permute(0,2,1) â†’ (8, 3, 5)                                    â”‚  â”‚
â”‚  â”‚    â†’ Conv1d(3â†’10, k=1) â†’ (8, 10, 5)                                â”‚  â”‚
â”‚  â”‚    â†’ permute(0,2,1) â†’ (8, 5, 10)                                   â”‚  â”‚
â”‚  â”‚  proprio_emb: (B=8, T=5, emb=10)                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. PARALLEL: VISUAL + ACTION ENCODING                               â”‚  â”‚
â”‚  â”‚  visual_embs = DINO(obs['visual']) â†’ (B=8, T=5, P=256, D=384)      â”‚  â”‚
â”‚  â”‚  act_emb = action_encoder(act)     â†’ (B=8, T=5, emb=10)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  5. FUSION (concat_dim=1)                                            â”‚  â”‚
â”‚  â”‚  proprio_tiled: (8,5,10) â†’ tile auf 256 Patches â†’ (8,5,256,10)     â”‚  â”‚
â”‚  â”‚  act_tiled:     (8,5,10) â†’ tile auf 256 Patches â†’ (8,5,256,10)     â”‚  â”‚
â”‚  â”‚  z = cat([visual_embs, proprio_tiled, act_tiled], dim=-1)            â”‚  â”‚
â”‚  â”‚  z: (B=8, T=5, P=256, D=404)                                        â”‚  â”‚
â”‚  â”‚       â””â”€â”€ 384 visual â”€â”€ 10 proprio â”€â”€ 10 action â”€â”€â”˜                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  6. SRC/TGT SPLIT                                                    â”‚  â”‚
â”‚  â”‚  z_src = z[:, :4]   â†’ (8, 4, 256, 404)  â† Input fÃ¼r Predictor      â”‚  â”‚
â”‚  â”‚  z_tgt = z[:, 1:]   â†’ (8, 4, 256, 404)  â† Ground Truth             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  7. PREDICTION (ViT Predictor)                                       â”‚  â”‚
â”‚  â”‚  z_src: (8, 4, 256, 404) â†’ reshape â†’ (8, 1024, 404)                â”‚  â”‚
â”‚  â”‚    â†’ 6Ã— Transformer Blocks (kausale Maske, 16 Heads)                â”‚  â”‚
â”‚  â”‚    â†’ reshape â†’ z_pred: (8, 4, 256, 404)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  8. LOSS-BERECHNUNG                                                  â”‚  â”‚
â”‚  â”‚  z_visual_loss  = MSE(z_pred[...,:384],   z_tgt[...,:384].detach()) â”‚  â”‚
â”‚  â”‚  z_proprio_loss = MSE(z_pred[...,384:394],z_tgt[...,384:394].detach())â”‚ â”‚
â”‚  â”‚  z_loss         = MSE(z_pred[...,:394],   z_tgt[...,:394].detach()) â”‚  â”‚
â”‚  â”‚                   â†‘ Visual + Proprio, OHNE Action                    â”‚  â”‚
â”‚  â”‚  total_loss = z_loss + decoder_loss                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  9. BACKWARD + OPTIMIZER STEP                                        â”‚  â”‚
â”‚  â”‚  accelerator.backward(total_loss)                                    â”‚  â”‚
â”‚  â”‚  Gradient flieÃŸt: loss â†’ z_pred â†’ ViT â†’ z_src â†’ z â†’ proprio_tiled  â”‚  â”‚
â”‚  â”‚    â†’ proprio_emb â†’ Conv1d.weight/bias (âˆ‚L/âˆ‚W, âˆ‚L/âˆ‚b)               â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  action_encoder_optimizer.step()                                     â”‚  â”‚
â”‚  â”‚    â†’ AdamW-Update fÃ¼r:                                               â”‚  â”‚
â”‚  â”‚      â€¢ action_encoder.patch_embed.weight  (10, 12, 1) â†’ 120 Params  â”‚  â”‚
â”‚  â”‚      â€¢ action_encoder.patch_embed.bias    (10,)       â†’  10 Params  â”‚  â”‚
â”‚  â”‚      â€¢ proprio_encoder.patch_embed.weight (10,  3, 1) â†’  30 Params  â”‚  â”‚
â”‚  â”‚      â€¢ proprio_encoder.patch_embed.bias   (10,)       â†’  10 Params  â”‚  â”‚
â”‚  â”‚    lr = 5e-4, Gesamt: 170 Parameter                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  10. CHECKPOINT (nach jeder Epoch)                                   â”‚  â”‚
â”‚  â”‚  torch.save({"proprio_encoder": ProprioceptiveEmbedding, ...})       â”‚  â”‚
â”‚  â”‚  Gespeichert: Conv1d(3â†’10) Weights + Bias = 40 Parameter            â”‚  â”‚
â”‚  â”‚  Pfad: outputs/DATUM/ZEIT/checkpoints/model_{epoch}.pth              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  W&B Logging:                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  train_z_proprio_loss  â”‚ val_z_proprio_loss  â”‚ z_proprio_err_rollout â”‚  â”‚
â”‚  â”‚  (geloggt pro Epoch)   â”‚ (geloggt pro Epoch) â”‚ (Rollout-Fehler)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.12 Zusammenfassung der Tensor-Dimensionen (Referenzmodell 500 Ep)

| Variable | Shape | Datei | Beschreibung |
|----------|-------|-------|-------------|
| `eef_states` (raw) | `(1, 1, 14)` | H5-Datei | Roher EEF-Zustand pro Timestep |
| `eef.flatten()` | `(14,)` | franka_cube_stack_dset.py | Geflachter EEF |
| `self.eef_tensors[i]` | `(25, 14)` | franka_cube_stack_dset.py | EEF pro Episode |
| `all_eef_flat` | `(12500, 14)` | franka_cube_stack_dset.py | Alle EEF concateniert |
| `self.proprio_mean` | `(3,)` | franka_cube_stack_dset.py | Mean der EE-Position |
| `self.proprio_std` | `(3,)` | franka_cube_stack_dset.py | Std der EE-Position |
| `eef[:, :3]` | `(T, 3)` | get_frames() | EE-Position [x,y,z] |
| `proprio` (normalisiert) | `(T, 3)` | get_frames() | Z-normalisiert ~N(0,1) |
| `obs['proprio']` (Batch) | `(B, T, 3)` = `(8, 5, 3)` | Dataloader | Proprio pro Batch |
| `proprio_emb` | `(B, T, 10)` = `(8, 5, 10)` | encode_obs() | Nach Conv1d |
| `proprio_tiled` | `(B, T, P, 10)` = `(8, 5, 256, 10)` | encode() | Auf Patches getiled |
| `z` (fusioniert) | `(B, T, P, D)` = `(8, 5, 256, 404)` | encode() | Visual+Proprio+Action |
| `z_src` | `(8, 4, 256, 404)` | forward() | Input fÃ¼r Predictor |
| `z_tgt` | `(8, 4, 256, 404)` | forward() | Ground Truth |
| `z_pred` | `(8, 4, 256, 404)` | forward() | Vorhersage |
| `z_pred[..., 384:394]` | `(8, 4, 256, 10)` | forward() | Vorhergesagtes Proprio-Emb |
| `z_tgt[..., 384:394]` | `(8, 4, 256, 10)` | forward() | Ground Truth Proprio-Emb |
| `z_proprio_loss` | Skalar | forward() | MSE(pred, tgt) fÃ¼r Proprio |
| `z_loss` | Skalar | forward() | MSE(pred, tgt) fÃ¼r Visual+Proprio |
| `W_proprio` | `(10, 3, 1)` | proprio_encoder | Conv1d Gewichte (30 Param) |
| `b_proprio` | `(10,)` | proprio_encoder | Conv1d Bias (10 Param) |

### 6.13 Konfigurationsparameter-Referenz

| Parameter | Config-Pfad | Wert (500 Ep) | Bedeutung |
|-----------|-------------|---------------|-----------|
| `proprio_emb_dim` | `conf/train.yaml` | `10` | Output-Dimension des Proprio Encoders |
| `num_proprio_repeat` | `conf/train.yaml` | `1` | Wiederholungsfaktor fÃ¼r Tiling (1 = keine Wiederholung) |
| `proprio_dim` | `conf/env/franka_cube_stack.yaml` | `3` | Input-Dimension (EE x,y,z) |
| `action_encoder_lr` | `conf/train.yaml` | `5e-4` | Learning Rate fÃ¼r Proprio+Action Optimizer |
| `normalize_action` | `conf/train.yaml` | `true` | Z-Normalisierung von Proprio und Actions |
| `concat_dim` | `conf/train.yaml` | `1` | Fusion entlang Feature-Dimension |
| `frameskip` | `conf/train.yaml` | `2` | Temporal Subsampling |
| `num_hist` | `conf/train.yaml` | `4` | Anzahl Kontext-Frames |
| `train_predictor` | `conf/train.yaml â†’ model` | `true` | Aktiviert Predictor + Action/Proprio Optimizer |

---

## 7. Loss-Funktionen

### 7.1 Ãœbersicht aller Losses

| Loss Name | Formel | Gewichtung | Zweck |
|-----------|--------|------------|-------|
| `z_loss` | MSE(z_pred, z_tgt) | 1.0 | Hauptloss fÃ¼r Predictor |
| `z_visual_loss` | MSE(z_pred_visual, z_tgt_visual) | (geloggt) | Nur visuelle Features |
| `z_proprio_loss` | MSE(z_pred_proprio, z_tgt_proprio) | (geloggt) | Nur Proprio-Features |
| `decoder_recon_loss` | MSE(visual_recon, obs_visual) | 1.0 | RekonstruktionsqualitÃ¤t |
| `decoder_vq_loss` | Commitment Loss | 0.25 | VQ Regularisierung (=0 wenn quantize=False) |
| `decoder_loss` | recon + 0.25Ã—vq | 1.0 | Decoder-Training |

### 7.2 Warum diese Kombination?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ZWEI-STUFEN TRAINING-STRATEGIE                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. LATENT LOSS (z_loss):                                                   â”‚
â”‚     - Trainiert den Predictor im kompakten Latent-Space                    â”‚
â”‚     - Weniger anfÃ¤llig fÃ¼r Pixel-Level Noise                               â”‚
â”‚     - Fokussiert auf semantische Vorhersage                                â”‚
â”‚                                                                             â”‚
â”‚  2. DECODER LOSS (decoder_loss):                                            â”‚
â”‚     - Trainiert den Decoder zur Bildrekonstruktion                         â”‚
â”‚     - Stellt sicher, dass Latent-Space interpretierbar bleibt             â”‚
â”‚     - ErmÃ¶glicht Visualisierung der Vorhersagen                            â”‚
â”‚                                                                             â”‚
â”‚  WICHTIG: Decoder wird auf z.detach() trainiert                            â”‚
â”‚           â†’ Decoder-Gradients flieÃŸen NICHT zum Predictor                  â”‚
â”‚           â†’ Verhindert, dass Decoder den Predictor "betrÃ¼gt"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. W&B Metriken und Monitoring

### 8.1 Ãœbersicht aller geplotteten Metriken

Das Training loggt automatisch zahlreiche Metriken zu Weights & Biases. Hier eine vollstÃ¤ndige Ãœbersicht:

#### 8.1.1 Hauptverluste (Loss)

| Metrik | Definition | Ziel |
|--------|------------|------|
| `train_loss` / `val_loss` | Gesamtverlust (kombiniert alle Komponenten) | â†“ niedrig |
| `train_z_loss` / `val_z_loss` | Verlust im latenten Raum (z-Space) - Hauptmetrik fÃ¼r Predictor | â†“ niedrig |
| `train_z_visual_loss` / `val_z_visual_loss` | Visueller Encoder-Verlust im latenten Raum (nur 384 DINO-Features) | â†“ niedrig |
| `train_z_proprio_loss` / `val_z_proprio_loss` | Propriozeptiver Verlust im latenten Raum (10 proprio-dim) | â†“ niedrig |

#### 8.1.2 Decoder-Verluste

| Metrik | Definition | Ziel |
|--------|------------|------|
| `decoder_loss_reconstructed` | Rekonstruktionsverlust (Bild â†’ Encoder â†’ Decoder â†’ Bild) | â†“ niedrig |
| `decoder_loss_pred` | Verlust fÃ¼r vorhergesagte Frames (durch Predictor) | â†“ niedrig |
| `decoder_recon_loss_*` | Reiner Rekonstruktionsverlust ohne VQ-Komponente | â†“ niedrig |
| `decoder_vq_loss_*` | Vector-Quantization Verlust (= 0, wenn `quantize: false`) | â†“ niedrig |

#### 8.1.3 BildqualitÃ¤tsmetriken

Diese Metriken messen die QualitÃ¤t der rekonstruierten/vorhergesagten Bilder:

| Metrik | Definition | Optimal |
|--------|------------|---------|
| `img_mse_reconstructed` / `img_mse_pred` | Mean Squared Error der Pixel | â†“ niedrig (< 0.01 gut) |
| `img_l1_reconstructed` / `img_l1_pred` | L1 Norm (mittlerer absoluter Fehler) | â†“ niedrig |
| `img_l2_reconstructed` / `img_l2_pred` | L2 Norm (euklidischer Abstand) | â†“ niedrig |
| `img_ssim_reconstructed` / `img_ssim_pred` | Structural Similarity Index (Struktur-Ã„hnlichkeit) | â†‘ hoch (max 1.0, > 0.9 gut) |
| `img_psnr_reconstructed` / `img_psnr_pred` | Peak Signal-to-Noise Ratio (dB) | â†‘ hoch (> 30 gut, > 40 exzellent) |
| `img_lpips_reconstructed` / `img_lpips_pred` | Learned Perceptual Image Patch Similarity | â†“ niedrig (< 0.1 gut) |

**Hinweis:** 
- `*_reconstructed`: Decoder rekonstruiert den Input direkt (keine Vorhersage)
- `*_pred`: Decoder rekonstruiert die Vorhersage des Predictors

#### 8.1.4 Rollout-Fehler (Latent Space)

Diese Metriken bewerten die VorhersagequalitÃ¤t Ã¼ber mehrere Zeitschritte:

| Metrik | Definition |
|--------|------------|
| `z_visual_err_pred` | Vorhersagefehler im visuellen latenten Raum (1-Schritt) |
| `z_visual_err_rollout` | Akkumulierter Fehler Ã¼ber mehrere Vorhersage-Schritte |
| `z_visual_err_rollout_1framestart` | Rollout-Fehler, beginnend vom ersten Frame |
| `z_visual_err_full` | Gesamter visueller Rollout-Fehler Ã¼ber alle Frames |
| `z_visual_err_next1` | Fehler fÃ¼r den nÃ¤chsten einzelnen Frame |
| `z_proprio_err_pred` | Vorhersagefehler fÃ¼r Propriozeption (1-Schritt) |
| `z_proprio_err_rollout` | Akkumulierter Proprio-Fehler Ã¼ber mehrere Schritte |
| `z_proprio_err_rollout_1framestart` | Proprio-Rollout-Fehler, beginnend vom ersten Frame |
| `z_proprio_err_full` | Gesamter Propriozeption-Rollout-Fehler |
| `z_proprio_err_next1` | Proprio-Fehler fÃ¼r den nÃ¤chsten Frame |

### 8.2 Interpretation der Metriken

#### Gute Trainingskurven zeigen:
```
train_loss         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
                  â•²                                          Konvergenz
                   â•²___________________________________  â†   (flach)
                                                             
val_loss          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  â•²
                   â•²___________________________________  â†   Ã„hnlich zu train
                                                             
train_img_ssim    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                              _________________________ 
                             â•±                          â†   Anstieg zu ~0.9+
                   _________â•±                                
```

#### Typische Probleme:

**1. Overfitting:**
```
train_loss        val_loss
   â”‚                 â”‚
   â•²                 â•²
    â•²_______          â•²____â•±â€¾â€¾â€¾â€¾â€¾  â† val steigt wieder an!
                                     (train fÃ¤llt weiter)
```

**2. Underfitting:**
```
train_loss = val_loss
     â”‚
     â”‚_______________  â† Beide stagnieren auf hohem Niveau
```

**3. InstabilitÃ¤t:**
```
train_loss
     â”‚â•±â•²  â•±â•²  â•±â•²
     â”‚  â•²â•±  â•²â•±  â•²â•±  â† Starke Schwankungen
```

### 8.3 Overfitting-Diagnose und LÃ¶sungsansÃ¤tze

#### 8.3.1 Typische Overfitting-Indikatoren

Overfitting tritt auf, wenn das Modell die Trainingsdaten "auswendig lernt" statt zu generalisieren:

| Symptom | Betroffene Metriken |
|---------|---------------------|
| Val-Loss steigt nach anfÃ¤nglichem Abfall | `val_loss`, `val_z_loss` |
| Train-Metriken verbessern sich weiter | `train_loss` fÃ¤llt weiter |
| Steigende Image-Fehler auf Validation | `val_img_mse_*`, `val_img_l2_*` steigen |
| Sinkende Image-QualitÃ¤t auf Validation | `val_img_psnr_*`, `val_img_ssim_*` fallen |
| Akkumulierende Rollout-Fehler | `val_z_visual_err_full`, `val_z_proprio_err_full` steigen |

#### 8.3.2 Besonders anfÃ¤llige Metriken

Basierend auf Experimenten mit kleinen DatensÃ¤tzen (20 Episoden):

1. **`val_z_proprio_loss`** - Steigt oft ab Epoch 40-60
2. **`val_z_visual_err_full`** - Akkumulierter Fehler wÃ¤chst kontinuierlich
3. **`val_img_mse_reconstructed`** - Verschlechtert sich nach Epoch 50
4. **`val_decoder_loss_reconstructed`** - Steigt langsam an

#### 8.3.3 LÃ¶sungsansÃ¤tze gegen Overfitting

| Ansatz | Konfiguration | Empfehlung |
|--------|---------------|------------|
| **Learning Rate reduzieren** | `decoder_lr: 1e-4` (von 3e-4)<br>`predictor_lr: 2e-4` (von 5e-4) | âœ“ Erste MaÃŸnahme |
| **Weniger Epochen** | `training.epochs: 50` (von 100) | âœ“ Bei kleinen DatensÃ¤tzen |
| **Mehr Dropout** | `predictor.dropout: 0.2` (von 0.1) | âœ“ Regularisierung |
| **Early Stopping** | Manuell bei Anstieg von val_loss | âœ“ Bester Checkpoint wÃ¤hlen |
| **Learning Rate Scheduler** | CosineAnnealingLR oder ReduceLROnPlateau | âš ï¸ Nicht implementiert |
| **Weight Decay** | In AdamW Optimizer | âš ï¸ Erfordert Code-Ã„nderung |
| **Mehr Trainingsdaten** | ZusÃ¤tzliche Episoden sammeln | âš ï¸ AufwÃ¤ndig |
| **Data Augmentation** | Bild-Transformationen | âš ï¸ Erfordert Code-Ã„nderung |

#### 8.3.4 Dropout erklÃ¤rt

**Was ist Dropout?**

Dropout ist eine Regularisierungstechnik, die wÃ¤hrend des Trainings zufÃ¤llig einen Prozentsatz der Neuronen "ausschaltet" (auf 0 setzt):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DROPOUT MECHANISMUS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  OHNE Dropout (Inferenz):       MIT Dropout (Training, p=0.2):             â”‚
â”‚                                                                             â”‚
â”‚    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—              â—â”€â”€â”€â—‹â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—‹                        â”‚
â”‚    â”‚   â”‚   â”‚   â”‚   â”‚              â”‚       â”‚   â”‚                            â”‚
â”‚    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—              â—â”€â”€â”€â—â”€â”€â”€â—‹â”€â”€â”€â—â”€â”€â”€â—                        â”‚
â”‚    â”‚   â”‚   â”‚   â”‚   â”‚              â”‚   â”‚       â”‚   â”‚                        â”‚
â”‚    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—              â—‹â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—                        â”‚
â”‚                                                                             â”‚
â”‚    Alle Neuronen aktiv           20% zufÃ¤llig deaktiviert (â—‹)              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Warum hilft Dropout gegen Overfitting?**

| Effekt | ErklÃ¤rung |
|--------|-----------|
| **Verhindert Co-Adaptation** | Neuronen kÃ¶nnen sich nicht auf andere Neuronen "verlassen" |
| **Ensemble-Effekt** | Trainiert implizit viele verschiedene Sub-Netzwerke |
| **Robustere Features** | Jedes Neuron muss unabhÃ¤ngig nÃ¼tzlich sein |
| **Noise Injection** | FÃ¼gt Rauschen hinzu, das Generalisierung fÃ¶rdert |

**Dropout im ViT Predictor:**

Im DINO World Model wird Dropout an zwei Stellen im Predictor verwendet:

```yaml
# conf/predictor/vit.yaml
predictor:
  dropout: 0.1      # Dropout nach Attention & Feed-Forward Layers
  emb_dropout: 0    # Dropout nach Embedding Layer (aktuell 0)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ViT Predictor - Dropout Positionen                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input Embedding                                                            â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  [Embedding Dropout] â† emb_dropout (Standard: 0)                           â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚  â”‚  Transformer Block (Ã—6)              â”‚                                  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                  â”‚
â”‚  â”‚  â”‚  Multi-Head Attention          â”‚  â”‚                                  â”‚
â”‚  â”‚  â”‚         â”‚                      â”‚  â”‚                                  â”‚
â”‚  â”‚  â”‚    [Dropout] â† dropout (0.1)   â”‚  â”‚                                  â”‚
â”‚  â”‚  â”‚         â”‚                      â”‚  â”‚                                  â”‚
â”‚  â”‚  â”‚  Feed-Forward Network          â”‚  â”‚                                  â”‚
â”‚  â”‚  â”‚         â”‚                      â”‚  â”‚                                  â”‚
â”‚  â”‚  â”‚    [Dropout] â† dropout (0.1)   â”‚  â”‚                                  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  Output                                                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Empfohlene Dropout-Werte:**

| Datensatz-GrÃ¶ÃŸe | Empfohlenes Dropout | BegrÃ¼ndung |
|-----------------|---------------------|------------|
| < 20 Episoden | 0.3 - 0.4 | Starke Regularisierung nÃ¶tig |
| 20-50 Episoden | 0.2 - 0.3 | Moderate Regularisierung |
| 50-100 Episoden | 0.1 - 0.2 | Leichte Regularisierung |
| > 100 Episoden | 0.0 - 0.1 | Wenig/keine Regularisierung |

**Wichtig:** Dropout ist nur wÃ¤hrend des **Trainings** aktiv. Bei Inferenz (`model.eval()`) werden alle Neuronen verwendet, aber die Gewichte werden skaliert.

#### 8.3.5 Empfohlene Konfiguration fÃ¼r kleine DatensÃ¤tze (< 50 Episoden)

```yaml
# conf/train.yaml Anpassungen
training:
  epochs: 50          # Reduziert von 100
  decoder_lr: 1e-4    # Reduziert von 3e-4
  predictor_lr: 2e-4  # Reduziert von 5e-4

predictor:
  dropout: 0.2        # ErhÃ¶ht von 0.1
```

#### 8.3.6 Optimales Checkpoint-Auswahl

Bei Overfitting **NICHT** das letzte Checkpoint verwenden! Stattdessen:

1. W&B Dashboard Ã¶ffnen
2. Epoch mit niedrigstem `val_loss` identifizieren (oft Epoch 40-60)
3. Entsprechendes Checkpoint laden: `checkpoints/model_XX.pth`

```python
# Beispiel: Bestes Modell laden
best_epoch = 45  # Aus W&B abgelesen
checkpoint_path = f"outputs/DATUM/ZEIT/checkpoints/model_{best_epoch}.pth"
```

---

## 9. Training starten

### 9.1 Basis-Kommando

```bash
cd /path/to/dino_wm

# Standard-Training
python train.py env=franka_cube_stack

# Mit expliziten Parametern
python train.py env=franka_cube_stack \
    frameskip=5 \
    num_hist=3 \
    training.epochs=100 \
    training.batch_size=8
```

### 9.2 Empfohlene Parameter fÃ¼r deinen Datensatz

Da du nur 10 Episoden hast, hier optimierte Einstellungen:

```bash
python train.py env=franka_cube_stack \
    frameskip=3 \                    # Feinere Dynamik
    num_hist=3 \                     # Standard Kontext
    training.epochs=200 \            # Mehr Epochen (kleiner Datensatz)
    training.batch_size=8 \          # Kleinere Batch-Size
    training.predictor_lr=3e-4 \     # Etwas niedriger
    training.decoder_lr=2e-4 \       # Etwas niedriger
    debug=True                       # Wandb Debug-Projekt
```

### 9.3 Erwartete Ausgabe

```
outputs/
â””â”€â”€ 2026-01-13/
    â””â”€â”€ 15-30-45/                    # Zeitstempel
        â”œâ”€â”€ checkpoints/
        â”‚   â”œâ”€â”€ model_latest.pth
        â”‚   â”œâ”€â”€ model_1.pth
        â”‚   â”œâ”€â”€ model_2.pth
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ train/
        â”‚   â””â”€â”€ train_e00001_b0.png  # Visualisierungen
        â”œâ”€â”€ valid/
        â”‚   â””â”€â”€ valid_e00001_b0.png
        â”œâ”€â”€ rollout_plots/
        â”‚   â””â”€â”€ e1_rollout/
        â””â”€â”€ hydra.yaml               # Gespeicherte Konfiguration
```

### 9.4 Monitoring mit Weights & Biases

Training wird automatisch zu W&B geloggt:
- Projekt: `dino_wm_debug` (wenn `debug=True`) oder `dino_wm`
- Metriken: Loss-Kurven, Image Metrics, Visualisierungen

---

## 9.5 Klarstellung: Pixel-Space vs. Meter-Space â€” Kein Problem fÃ¼r das Training

> **Analyse vom 09.02.2026** â€” Die DINO-WM-Architektur ist vollstÃ¤ndig einheitsagnostisch.

### Hintergrund der Fragestellung

Beim Vergleich des Franka Cube Stacking Datensatzes mit den Referenz-DatensÃ¤tzen (Rope, Push-T, Wall) fiel auf, dass diese **unterschiedliche Koordinatensysteme** verwenden. Die BefÃ¼rchtung: Kann das DINO World Model mit Meter-Koordinaten trainiert werden, wenn es mit Pixel-/Sim-Koordinaten entwickelt wurde?

### Analyse der Action-RÃ¤ume aller DatensÃ¤tze

| Datensatz | Action-Dimensionen | Koordinatensystem | Roh-Wertebereich |
|-----------|-------------------|-------------------|------------------|
| **Rope** (Referenz) | 4D: `[x_start, z_start, x_end, z_end]` | FleX-Simulator-Einheiten | ca. Â±4 |
| **Push-T** (Referenz) | 2D: `[dx, dy]` | Pixel-Space (Ã·100) | ca. Â±0.2 |
| **Wall** (Referenz) | 2D: `[a1, a2]` | Eigener Sim-Space | ca. Â±0.5 |
| **Franka** (unserer) | 6D: `[x_s, y_s, z_s, x_e, y_e, z_e]` | Meter (Isaac Sim, lokal) | ca. 0.0â€“0.8 |

**Zentrale Erkenntnis:** Schon die Referenz-DatensÃ¤tze sind untereinander **nicht einheitlich** â€” Rope nutzt Sim-Einheiten (Â±4), Push-T nutzt skalierte Pixel (Â±0.2), Wall nutzt wieder andere Sim-Einheiten (Â±0.5). Die Architektur wurde **bewusst** so designed, dass das Koordinatensystem keine Rolle spielt.

### Warum die Einheit irrelevant ist â€” Der Datenfluss

```
Schritt 1: Z-Score-Normalisierung (im Dataset-Loader)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Rohdaten (beliebige Einheit)  â†’  normalized = (raw - mean) / std  â†’  ~N(0, 1)

  Rope:   [-3.2, 1.1, -2.8, 0.5] â†’ norm. â‰ˆ [-0.8, 0.3, -0.7, 0.1]
  Franka: [0.45, 0.02, 0.35, 0.51, -0.01, 0.38] â†’ norm. â‰ˆ [-0.2, 0.1, 0.9, 0.3, -0.1, 0.6]
  â†’ FÃ¼r das Modell sehen BEIDE wie ~N(0,1)-verteilte Vektoren aus!

Schritt 2: Action Encoder (lernbar)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
normalized_action (action_dim) â†’ nn.Conv1d â†’ action_embedding (10D)
  â†’ Lineare Projektion, lernt beliebige Skalierung
  â†’ Keine hardcodierten Annahmen Ã¼ber Einheiten

Schritt 3: Predictor (ViT)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[visual_patches, proprio_emb, action_emb] â†’ ViT Predictor â†’ predicted_patches
  â†’ Action-Embedding ist nur Conditioning-Signal
  â†’ Loss wird NUR auf visuellen Patches berechnet
  â†’ Action-Skala hat keinen Einfluss auf den Gradienten
```

### Voraussetzungen (beide erfÃ¼llt âœ…)

1. **`action_dim` korrekt konfiguriert:** In `conf/env/franka_cube_stack.yaml` ist `action_dim` passend zum Datensatz-Format gesetzt (6 fÃ¼r `ee_pos`-Format, 4 fÃ¼r `delta_pose`).

2. **`action_mean`/`action_std` korrekt berechnet:** Der `FrankaCubeStackDataset`-Loader berechnet Z-Score-Statistiken on-the-fly aus allen Episoden. Seit dem Grid-Offset-Fix (Commit `a9af071`) enthalten die Daten korrekte lokale Meter-Werte â†’ Mean/Std sind realistisch.

### Was NICHT nÃ¶tig ist

- âŒ Konvertierung Meter â†’ Pixel
- âŒ Anpassung der Action-Skala an Referenz-DatensÃ¤tze
- âŒ Sonderbehandlung im Modell oder Preprocessor
- âŒ Ã„nderung der Loss-Funktion

---

## 10. Glossar

| Begriff | ErklÃ¤rung |
|---------|-----------|
| **DINO** | Self-Distillation with No Labels - vortrainiertes Vision-Modell |
| **ViT** | Vision Transformer - Transformer-Architektur fÃ¼r Bilder |
| **VQ-VAE** | Vector Quantized Variational Autoencoder - generatives Modell |
| **Patch** | Bildausschnitt (14Ã—14 Pixel bei DINO) |
| **Embedding** | Kompakte VektorreprÃ¤sentation |
| **Latent Space** | Komprimierter ReprÃ¤sentationsraum |
| **Frameskip** | Temporales Subsampling - jeder n-te Frame wird verwendet |
| **num_hist** | Anzahl Kontext-Frames als Input |
| **num_pred** | Anzahl vorherzusagender zukÃ¼nftiger Frames |
| **Causal Mask** | Verhindert, dass Modell zukÃ¼nftige Frames sieht |
| **MSE** | Mean Squared Error - quadratischer Fehler |
| **Proprio** | Proprioceptive Daten - Roboter-Eigenwahrnehmung (z.B. Gelenkwinkel) |
| **Accelerator** | HuggingFace Tool fÃ¼r verteiltes Training |

---

## Anhang: Datensatz-Statistiken

FÃ¼r deinen Datensatz `2026_01_13_1152_fcs_dset`:

| Metrik | Wert |
|--------|------|
| Episoden | 10 |
| Frames pro Episode | 932 |
| Gesamtframes | 9.320 |
| State-Dimension | 22 |
| Action-Dimension | 9 |
| BildgrÃ¶ÃŸe | 256Ã—256 â†’ resize auf 224Ã—224 |
| Training-Samples (frameskip=5, num_hist=3) | ~8.217 |
| Validation-Samples | ~913 |
| Speicherbedarf (Bilder) | ~10 Ã— 932 Ã— 256 Ã— 256 Ã— 3 â‰ˆ 1.8 GB |

---

*Dokumentation erstellt am 13.01.2026*

---

## ğŸš¨ KRITISCH: Action-Observation Temporale Alignment-Analyse (20.02.2026)

### Problemstellung

Es wurde eine **fundamentale InkompatibilitÃ¤t** zwischen dem FCS-Datensatz (Franka Cube Stacking) und der Referenz-Konvention des DINO-WM Papers (Rope/Deformable Environments) bei der zeitlichen Zuordnung von Actions und Observations identifiziert.

**Kernfrage:** Beschreibt `action[t]` den Ãœbergang von `obs[t]` zu `obs[t+1]` (vorwÃ¤rtsblickend) oder den Ãœbergang, der zu `obs[t]` fÃ¼hrte (rÃ¼ckwÃ¤rtsblickend)?

### Analyse der Referenz-Konvention (Rope / Deformable Environment)

#### Datenfluss bei der Generierung

In `FlexEnvWrapper.rollout()` ([env/deformable_env/FlexEnvWrapper.py](env/deformable_env/FlexEnvWrapper.py#L156)):
```python
def rollout(self, seed, init_state, actions):
    obs, state_dct = self.prepare(seed, init_state)  # obs_initial (VOR jeder Action)
    obses, rewards, dones, infos = self.step_multiple(actions)  # T Action-Ergebnisse
    for k in obses.keys():
        obses[k] = np.vstack([np.expand_dims(obs[k], 0), obses[k]])  # obs_initial VORANSTELLEN
    # Ergebnis: T+1 Beobachtungen fÃ¼r T Actions
```

- `obs_initial` = Zustand **VOR** der ersten Action
- `step_multiple()` liefert T Bilder â€” jeweils am **ENDE** jeder Action
- `rollout()` stellt `obs_initial` voran â†’ **T+1 Beobachtungen fÃ¼r T Actions**

#### Konvention in den .pth-Dateien

In `DeformDataset.get_frames()` ([datasets/deformable_env_dset.py](datasets/deformable_env_dset.py#L95)):
```python
image = torch.load(obs_dir / "obses.pth")
act = self.actions[idx, frames]
image = image[frames]  # Gleicher Index!
```

In `plan.py` `sample_traj_segment_from_dset()` ([plan.py](plan.py#L263)):
```python
obs = {key: arr[offset : offset + traj_len] for key, arr in obs.items()}
act = act[offset : offset + self.frameskip * self.goal_H]
# traj_len = frameskip * goal_H + 1 â†’ obs hat EINE MEHR EintrÃ¤ge als act!
```

**Beweis:** Das Planning nimmt `traj_len = frameskip * goal_H + 1` Observations aber nur `frameskip * goal_H` Actions. Die eine Extra-Observation ist die **initiale Beobachtung VOR der ersten Action**.

#### Rope-Konvention zusammengefasst

```
Zeitachse:  obs[0]  â†’(act[0])â†’  obs[1]  â†’(act[1])â†’  obs[2]  â†’ ...
              â†‘                    â†‘                    â†‘
          INITIAL              Ergebnis              Ergebnis
        (vor Action)          von act[0]            von act[1]
```

- `obs[t]` = Zustand **VOR** AusfÃ¼hrung von `act[t]`
- `act[t]` beschreibt die Transition `obs[t] â†’ obs[t+1]` (vorwÃ¤rtsblickend)
- `act[t]` wird **VON `obs[t]` aus** ausgefÃ¼hrt

### Analyse der FCS-Konvention (Franka Cube Stacking)

#### Datenfluss im primitive_data_logger.py

In `_save_primitive_h5()` ([isaacsim/00_Franka_Cube_Stack/...primitive_data_logger.py](../isaacsim/00_Franka_Cube_Stack/Franka_Cube_Stacking/primitive_data_logger.py#L722)):
```python
# Bilder am ENDE des Primitivs (nach der Bewegung)
rgb = end_data["rgb_images"]
# EEF-Position am ENDE des Primitivs
ee_pos = end_data["ee_pos"] - env_offset
```

In `end_episode()`:
```python
# obses.pth wird aus ep["imgs_list"] erstellt â€” NUR End-of-Primitive Bilder
obses = torch.stack(ep["imgs_list"]).squeeze(1)
torch.save(obses, obses_path)
```

#### Verifizierung mit echten Daten

```
=== Episode 0 (20 Primitive) ===
  obses.pth: 20 Bilder â€” GLEICHE Anzahl wie Actions
  Timing-Check: action[t].start_pos vs eef_states[t-1][:3]
  t=1: start=[0.4894,0.0899,0.4166] vs eef[0]=[0.4853,0.0852,0.4180] => OK (d<0.01)
  t=2: start=[0.5238,0.1816,0.3655] vs eef[1]=[0.5207,0.1711,0.3736] => OK (dâ‰ˆ0.014)
```

**BestÃ¤tigt:** `action[t].start_pos â‰ˆ eef_states[t-1]` â€” Action t startet dort, wo Action t-1 endete. Also beschreibt `action[t]` den Ãœbergang von `obs[t-1]` nach `obs[t]`.

#### FCS-Konvention zusammengefasst

```
Zeitachse:  ???  â†’(act[0])â†’  obs[0]  â†’(act[1])â†’  obs[1]  â†’(act[2])â†’  obs[2]
              â†‘                â†‘                    â†‘                    â†‘
          INITIAL           Ergebnis             Ergebnis             Ergebnis
       (NICHT GESPEICHERT)  von act[0]           von act[1]           von act[2]
```

- `obs[t]` = Zustand **NACH** AusfÃ¼hrung von `act[t]` (Ergebnis)
- `act[t]` beschreibt die Transition `obs[t-1] â†’ obs[t]` (rÃ¼ckwÃ¤rtsblickend)
- `act[t]` hat `obs[t]` **PRODUZIERT**
- Es gibt **KEIN** initiales Bild vor der ersten Action

### Der Off-by-One Fehler

#### Im TrajSlicerDataset (Training)

Mit `frameskip=2`, `num_frames=7`:
```python
obs_window = [obs[start], obs[start+2], obs[start+4], ..., obs[start+12]]  # 7 Bilder
act_window = [(act[start],act[start+1]), (act[start+2],act[start+3]), ...]  # 7 Gruppen
```

**In Rope-Konvention (korrekt):**
- `act_group[0] = (act[start], act[start+1])`
- `act[start]` transitiert `obs[start] â†’ obs[start+1]`
- `act[start+1]` transitiert `obs[start+1] â†’ obs[start+2]`
- Kombiniert: `obs[start] â†’ obs[start+2]` = `obs_window[0] â†’ obs_window[1]` âœ“

**In FCS-Konvention (FEHLERHAFT):**
- `act_group[0] = (act[start], act[start+1])`
- `act[start]` transitiert `obs[start-1] â†’ obs[start]` â† **RÃœCKWÃ„RTS** (aus dem Window hinaus!)
- `act[start+1]` transitiert `obs[start] â†’ obs[start+1]` â† Nur EIN Schritt vorwÃ¤rts
- Kombiniert: `obs[start-1] â†’ obs[start+1]`, NICHT `obs[start] â†’ obs[start+2]` âŒ
- **Das Modell erhÃ¤lt Actions, die NICHT zur beobachteten Transition passen!**

#### Auswirkung auf das Training

Das VWorldModel lernt in `forward()` ([models/visual_world_model.py](models/visual_world_model.py#L192)):
```python
z_src = z[:, :num_hist]     # Encode(obs[0..5], act[0..5])
z_tgt = z[:, num_pred:]     # Encode(obs[1..6], act[1..6])
z_pred = predict(z_src)     # Vorhersage
loss = criterion(z_pred, z_tgt)  # Soll z_tgt matchen
```

- In **Rope**: `z_src[0] = (obs[0], act[0])` wobei `act[0]` von `obs[0]` wegfÃ¼hrt â†’ Modell lernt: "gegeben Zustand + ausgehende Action â†’ vorhersage nÃ¤chster Zustand"
- In **FCS**: `z_src[0] = (obs[0], act[0])` wobei `act[0]` zu `obs[0]` **hinfÃ¼hrte** â†’ Modell lernt: "gegeben Ergebnis + Action die es produzierte â†’ vorhersage nÃ¤chstes Ergebnis"

Das Modell lernt eine **semantisch verschobene Korrelation**. Die Actions beschreiben nicht die Transition zwischen den beobachteten ZustÃ¤nden, sondern eine um 1 verschobene Transition.

#### Auswirkung auf das Planning (CEM)

Beim Planning (`VWorldModel.rollout()`):
1. CEM schlÃ¤gt Actions vor als "was soll der Roboter **als nÃ¤chstes tun**" (vorwÃ¤rtsblickend)
2. Das Modell erwartet aber Actions als "was hat den **aktuellen Zustand produziert**" (rÃ¼ckwÃ¤rtsblickend)
3. â†’ **Semantischer Mismatch** zwischen CEM und Modell

Dies kÃ¶nnte eine **Hauptursache** fÃ¼r die CEM-Divergenz sein (neben den fehlenden Action Bounds).

### Implementierter Fix: START-Bild statt END-Bild im Data Logger (21.02.2026)

**Entscheidung:** Der Fix wurde im `primitive_data_logger.py` implementiert (nicht im Dataloader), weil:
1. Daten sind an der Quelle korrekt â€” jeder Loader/jedes Tool bekommt die richtige Semantik
2. Kein Datenverlust (weiterhin T obs + T act pro Episode, statt T-1 beim Loader-Shift)
3. Kein Workaround in jedem neuen Loader nÃ¶tig
4. Debugging einfacher â€” Rohdaten auf Disk haben die richtige Semantik

**Konkrete Ã„nderung in `_save_primitive_h5()`:**

```python
# VORHER (falsch): Bild am ENDE des Primitivs
rgb = end_data["rgb_images"]     # obs[t] = Zustand NACH act[t] âŒ

# NACHHER (korrekt): Bild am START des Primitivs
rgb = obs_data["rgb_images"]     # obs[t] = Zustand VOR act[t] âœ“
# obs_data = start_data (Ã¼bergeben von _finalize_primitive_fixed/phase)
```

Alle drei Aufrufstellen (`_finalize_primitive_fixed`, `_finalize_primitive_phase`, `_segment_into_fixed_primitives`) Ã¼bergeben jetzt `start_data` statt `end_data` als Beobachtungsdaten. Die Action bleibt unverÃ¤ndert (`[start_pos â†’ end_pos]`).

**Resultierende Konvention (identisch mit Rope):**
```
obs[0]  â†’(act[0])â†’  obs[1]  â†’(act[1])â†’  obs[2]  â†’ ...
  â†‘                    â†‘                    â†‘
START Prim 0       START Prim 1          START Prim 2
(VOR Bewegung)     (= ENDE Prim 0)      (= ENDE Prim 1)
```

### Gleicher Fix im MinDataLogger (21.02.2026)

Der `min_data_logger.py` hatte den **gleichen backward-looking Bug**: Jede H5-Datei enthielt das Bild vom aktuellen Zustand (`image[t]`) zusammen mit `action = [prev_pos, curr_pos]` â€” d.h. das Bild zeigte den Zustand NACH der Action.

**Fix: Buffer-Ansatz fÃ¼r Forward-Looking Alignment:**
```python
# VORHER (falsch, backward-looking):
# H5(t): image=image[t], action=[pos(t-1), pos(t)] â†’ Bild zeigt Zustand NACH Action âŒ

# NACHHER (korrekt, forward-looking):
# Step 0: buffer {image0, pos0}    â†’ kein H5 (kein Forward-Action bekannt)
# Step 1: save H5 (image0, [pos0â†’pos1]) â†’ buffer {image1, pos1}
# Step 2: save H5 (image1, [pos1â†’pos2]) â†’ buffer {image2, pos2}
# end():  save H5 (image2, [pos2â†’pos2]) â†’ Dummy-Action (letzter Obs) âœ“
```

Neue Hilfsmethoden:
- `_save_step_h5()`: Zentrale H5-Speicherlogik (obs_data + action)
- `_flush_buffer_final()`: Speichert letzten Buffer mit Dummy-Action in `end_episode()`
- `_save_last_frame_if_needed()`: Angepasst auf `buffered_at_frame`-Tracking

### Anpassungen im Anwendungscode (21.02.2026)

**`fcs_main_parallel.py`:** Keine Code-Ã„nderungen nÃ¶tig (log_step()-API unverÃ¤ndert).
Dokumentation aktualisiert:
- `collect_timestep_data()`: Docstring dokumentiert Erfassungsreihenfolge (VOR Action-AusfÃ¼hrung)
- `save_successful_episode()`: Docstring referenziert Rope-Konvention beider Logger
- Hauptschleife: Kommentar betont WICHTIG: Erfassung VOR action-AusfÃ¼hrung

**`planning_client.py`:** Keine Code-Ã„nderungen nÃ¶tig (PlanningLogger = separater, simpler Logger).
Dokumentation aktualisiert:
- `PlanningLogger`: Docstring dokumentiert temporale Konvention
- `log_step_if_active()`: Docstring dokumentiert Aufruf-Semantik (VOR nÃ¤chster Action)

#### Konsequenz

âš ï¸ **Der Datensatz muss NEU GENERIERT werden. Das aktuell trainierte Modell (260218/11-58) wurde mit der falschen Konvention trainiert und muss nach der Neugenerierung NEU TRAINIERT werden.**

### ÃœberprÃ¼fungs-Checkliste

| PrÃ¼fpunkt | Rope (Referenz) | FCS (alt, fehlerhaft) | FCS (nach Fix) |
|-----------|-----------------|----------------------|-----------------|
| `obs[t]` zeigt Zustand... | VOR `act[t]` | NACH `act[t]` âŒ | VOR `act[t]` âœ“ |
| `act[t]` beschreibt... | `obs[t]â†’obs[t+1]` | `obs[t-1]â†’obs[t]` âŒ | `obs[t]â†’obs[t+1]` âœ“ |
| Bild-Zeitpunkt | START (vor Bewegung) | ENDE (nach Bewegung) âŒ | START (vor Bewegung) âœ“ |
| Actions pro Episode | T | T | T |
| Obs pro Episode | T | T | T |
| act_group passt zu obs_window | âœ“ | âŒ (verschoben) | âœ“ |
| Datenverlust | â€” | â€” | Keiner âœ“ |

